{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wlt8za56vXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17aaf6ac-5f66-47e2-d80e-61a2b68d3b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "# just once to install opencv\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxBuW7bh_jBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdaa91d8-dbf1-4bcc-eaf4-6ec31e22e156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# just once to install matplotlib\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tsmOedaCD10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50f53e9-fe6f-4cc8-d568-8014a719cde2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "# just once to install numpy\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGMIWbpq7lPX"
      },
      "outputs": [],
      "source": [
        "# just once to install google.colab\n",
        "#!pip install google-colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM9CBgTjzaM-"
      },
      "outputs": [],
      "source": [
        "#!pip install PyWavelets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY2EJLXij72v"
      },
      "outputs": [],
      "source": [
        "#!pip install image_dehazer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-image"
      ],
      "metadata": {
        "id": "RC3y6ZpI6cAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "179e077e-0864-4c27-c0e8-31bb7ab93b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (0.19.2)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.6.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (21.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->scikit-image) (3.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install fastprogress\n",
        "from fastprogress import master_bar, progress_bar"
      ],
      "metadata": {
        "id": "PcHV0uIoNFHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i-G98GNpyYt",
        "outputId": "59e6e3bf-6c10-4fc9-e4c9-04a2389ef265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5st-OLpSqZ6A",
        "outputId": "41226164-283f-456b-c5c4-dd38782d659b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['normals.txt', 'images', 'groundtruths', 'masks']\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "\n",
        "#first put a shortcut in your drive to the image processing folder\n",
        "\n",
        "DATA_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Image Processing and Analysis 2022',\n",
        "                        'projects',\n",
        "                        'Calcification Detection',\n",
        "                        'dataset')\n",
        "\n",
        "\n",
        "print(os.listdir(DATA_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8nkv1ToTdLO"
      },
      "outputs": [],
      "source": [
        "# os.listdir(DATA_DIR + \"/groundtruths\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc5Lu21CF2sx"
      },
      "outputs": [],
      "source": [
        "# import opencv\n",
        "import cv2\n",
        "# import numpy\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5RUKbMaE50a"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "# here explain what you code\n",
        "def preprocessingWavelet(matrix, mask):\n",
        "  # wavelet high pass, low pass or low pass, high pass, high pass high pass.\n",
        "  import pywt  \n",
        "\n",
        "  # enhancement\n",
        "  # Comparing the Performance of Image Enhancement Methods\n",
        "  # to Detect Microcalcification Clusters in Digital Mammography, Moradmand, Hajar, 2012\n",
        "\n",
        "  # Five-level   discrete   wavelet decomposition  was  employed  by  using  Asymmetric Daubechies  of  order  8; \n",
        "\n",
        "  # normal wavelet from stackoverflow\n",
        "  # convert to grayscale\n",
        "  grayscale = cv2.cvtColor(matrix, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # convert to float\n",
        "  arrayFloat = np.float32(grayscale)\n",
        "  arrayFloat /= 255\n",
        "  \n",
        "  # compute coefficients\n",
        "  coeffs = pywt.wavedec2(arrayFloat,'haar',level=10)\n",
        "\n",
        "  #process coefficients\n",
        "  coeffs_H = list(coeffs)\n",
        "  coeffs_H[0] *= 0\n",
        "\n",
        "  # reconstruction\n",
        "  arrayFloat_H = pywt.waverec2(coeffs_H, 'haar')\n",
        "  arrayFloat_H *= 255\n",
        "  arrayFloat_H = np.uint8(arrayFloat_H)\n",
        "\n",
        "  preprocessed = arrayFloat_H\n",
        "  return preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cJXfvTTks38"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "# here explain what you code\n",
        "def preprocessingDeHazingPy(matrix, mask):\n",
        "  # https://link.springer.com/chapter/10.1007/978-3-319-68548-9_27\n",
        "  # the professor say we can take the grays in the mammogram as haze. so use dehazing\n",
        "\n",
        "  # still no the hazing method that he use this is just one found in a python library\n",
        "  # https://github.com/Utkarsh-Deshmukh/Single-Image-Dehazing-Python\n",
        "  # dehazing\n",
        "\n",
        "  import image_dehazer\t# Load the library\n",
        "\n",
        "  hazeCorrectedImg = image_dehazer.remove_haze(matrix)\t\t# Remove Haze\n",
        "\n",
        "  preprocessed = hazeCorrectedImg\n",
        "  return preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dehaze single image using dark channel prior and guided filter\n",
        "# taken from a repository \n",
        "# https://github.com/He-Zhang/image_dehaze\n",
        "# dehazing method proposed by the professor \n",
        "# simple imaging dehazing using dark channel prior (and guided filter, readme.md of the repo say that)\n",
        "import math\n",
        "\n",
        "# change this methods to do just grayscale will be nice\n",
        "\n",
        "def DarkChannel(im,sz):\n",
        "    b,g,r = cv2.split(im)\n",
        "    dc = cv2.min(cv2.min(r,g),b);\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(sz,sz))\n",
        "    dark = cv2.erode(dc,kernel)\n",
        "    return dark\n",
        "\n",
        "def AtmLight(im,dark):\n",
        "    [h,w] = im.shape[:2]\n",
        "    imsz = h*w\n",
        "    numpx = int(max(math.floor(imsz/1000),1))\n",
        "    darkvec = dark.reshape(imsz);\n",
        "    imvec = im.reshape(imsz,3);\n",
        "\n",
        "    indices = darkvec.argsort();\n",
        "    indices = indices[imsz-numpx::]\n",
        "\n",
        "    atmsum = np.zeros([1,3])\n",
        "    for ind in range(1,numpx):\n",
        "       atmsum = atmsum + imvec[indices[ind]]\n",
        "\n",
        "    A = atmsum / numpx;\n",
        "    return A\n",
        "\n",
        "def TransmissionEstimate(im,A,sz):\n",
        "    omega = 0.95;\n",
        "    im3 = np.empty(im.shape,im.dtype);\n",
        "\n",
        "    for ind in range(0,3):\n",
        "        im3[:,:,ind] = im[:,:,ind]/A[0,ind]\n",
        "\n",
        "    transmission = 1 - omega*DarkChannel(im3,sz);\n",
        "    return transmission\n",
        "\n",
        "def Guidedfilter(im,p,r,eps):\n",
        "    mean_I = cv2.boxFilter(im,cv2.CV_64F,(r,r));\n",
        "    mean_p = cv2.boxFilter(p, cv2.CV_64F,(r,r));\n",
        "    mean_Ip = cv2.boxFilter(im*p,cv2.CV_64F,(r,r));\n",
        "    cov_Ip = mean_Ip - mean_I*mean_p;\n",
        "\n",
        "    mean_II = cv2.boxFilter(im*im,cv2.CV_64F,(r,r));\n",
        "    var_I   = mean_II - mean_I*mean_I;\n",
        "\n",
        "    a = cov_Ip/(var_I + eps);\n",
        "    b = mean_p - a*mean_I;\n",
        "\n",
        "    mean_a = cv2.boxFilter(a,cv2.CV_64F,(r,r));\n",
        "    mean_b = cv2.boxFilter(b,cv2.CV_64F,(r,r));\n",
        "\n",
        "    q = mean_a*im + mean_b;\n",
        "    return q;\n",
        "\n",
        "def TransmissionRefine(im,et):\n",
        "    gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY);\n",
        "    gray = np.float64(gray)/255;\n",
        "    r = 60;\n",
        "    eps = 0.0001;\n",
        "    t = Guidedfilter(gray,et,r,eps);\n",
        "\n",
        "    return t;\n",
        "\n",
        "def Recover(im,t,A,tx = 0.1):\n",
        "    res = np.empty(im.shape,im.dtype);\n",
        "    t = cv2.max(t,tx);\n",
        "\n",
        "    for ind in range(0,3):\n",
        "        res[:,:,ind] = (im[:,:,ind]-A[0,ind])/t + A[0,ind]\n",
        "    return res\n",
        "\n",
        "def deHazingDarkChannelPriorPy(matrix, mask):\n",
        "\n",
        "    I = matrix.astype(np.float64)/255\n",
        " \n",
        "    dark = DarkChannel(I,15)\n",
        "    A = AtmLight(I,dark)\n",
        "    te = TransmissionEstimate(I,A,15)\n",
        "    t = TransmissionRefine(matrix,te)\n",
        "    J = Recover(I,t,A,0.1)\n",
        "    preprocessed = J\n",
        "    return preprocessed\n",
        "\n",
        "# image = cv2.imread(DATA_DIR+\"/images/53582422_3f0db31711fc9795_MG_R_ML_ANON.tif\")\n",
        "# dark, t, matrix, J = deHazingDarkChannelPriorPy(image, image)\n"
      ],
      "metadata": {
        "id": "v9lVjTTcoC2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0KHFD9fFC9y"
      },
      "outputs": [],
      "source": [
        "# candidateExtraction\n",
        "# here explain what you code\n",
        "# Hessian-matrix-based analysis or difference of gaussians (DoH) blob detection from skimage\n",
        "# https://scikit-image.org/docs/stable/api/skimage.feature.html?highlight=local%20binary%20pattern#skimage.feature.blob_doh\n",
        "def candidateExtraction(matrix, mask):\n",
        "\n",
        "  from skimage import feature\n",
        "\n",
        "  # returns x,y,sigma of the blob\n",
        "  blobs = feature.blob_doh(matrix,\n",
        "                           min_sigma=1,\n",
        "                           max_sigma=30,\n",
        "                           num_sigma=10,\n",
        "                           # The absolute lower bound for scale space maxima.\n",
        "                           # Local maxima smaller than threshold are ignored.\n",
        "                           # Reduce this to detect blobs with lower intensities.\n",
        "                           # If threshold_rel is also specified, whichever threshold is larger will be used.\n",
        "                           # If None, threshold_rel is used instead.\n",
        "                           threshold=0.005,\n",
        "                           # lower more sensible, more false positives bad also tinier calcifications detected\n",
        "                           overlap=0.5,\n",
        "                           log_scale=False,\n",
        "                           threshold_rel=None\n",
        "                           )\n",
        "  # taken from the documentation\n",
        "  # ...The downside is that this method can’t be used for detecting blobs of radius less than 3px\n",
        "  # due to the box filters used in the approximation of Hessian Determinant.\n",
        "  result = blobs\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23WbyaI2FhzF"
      },
      "outputs": [],
      "source": [
        "# featuresExtraction\n",
        "# here explain what you code\n",
        "def featuresExtraction(matrix, candidates, features, mask):\n",
        "  \n",
        "  # distances\n",
        "  # taking into account the microcalcifications can be less that 5 pixels\n",
        "  # distances should vary with that\n",
        "  distances = [1, 3, 5, 7]# probably we need bigger\n",
        "  # angles\n",
        "  angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
        "\n",
        "\n",
        "  # patches\n",
        "  patches = []\n",
        "  for candidate in candidates:\n",
        "    # candidate points are save as np.float64\n",
        "    # to use them as coordinates they have to be integers\n",
        "    candidate = candidate.astype(np.int64)\n",
        "    # candidates are x,y and sigma\n",
        "    patchCandidate = matrix[candidate[0]:candidate[0] + candidate[2],\n",
        "                   candidate[1]:candidate[1] + candidate[2]]\n",
        "\n",
        "    # reduce even more the shades of gray T-T\n",
        "    # graycomatrix, glcm, receive unsigned integer type\n",
        "    # but if it is bigger that np.uint8 you have to change the levels argument\n",
        "    # of graycomatrix for the shades of gray, if it is np.uint16, levels shoud be\n",
        "    # aprox 65 535. that break the colab :c \n",
        "    patchCandidate = patchCandidate.astype(np.uint8)\n",
        "    # add to the list\n",
        "    patches.append(patchCandidate)\n",
        "  \n",
        "  # https://www.youtube.com/watch?v=5x-CIHRmMNY\n",
        "  # https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_glcm.html\n",
        "  # https://ijcrr.com/uploads/3454_pdf.pdf\n",
        "\n",
        "  from skimage import feature\n",
        "\n",
        "  for patch, distance in zip(patches, distances):\n",
        "    \n",
        "    for angle, name in zip(angles, ['0','90','180','270']):\n",
        "      \n",
        "      # input image, distance in pixels, angles\n",
        "      glcm = feature.graycomatrix(patch, [ distance ], [ angle ])\n",
        "      # Output: the gray-level co-occurrence histogram. The value P[i,j,d,theta]\n",
        "      # is the number of times that gray-level j occurs at a distance d\n",
        "      # and at an angle theta from gray-level i.\n",
        "      # If normed is False, the output is of type uint32, otherwise it is float64.\n",
        "      # The dimensions are: levels x levels x number of distances x number of angles.\n",
        "\n",
        "      # properties: {‘contrast’, ‘dissimilarity’, ‘homogeneity’, ‘energy’, ‘correlation’, ‘ASM’}\n",
        "      features['contrast'+ str(distance) + name] = feature.graycoprops(glcm, 'contrast')[0]\n",
        "      features['dissimilarity' + str(distance) + name] = feature.graycoprops(glcm, 'dissimilarity')[0]\n",
        "      features['homogeneity' + str(distance) + name] = feature.graycoprops(glcm, 'homogeneity')[0]\n",
        "      features['energy' + str(distance) + name] = feature.graycoprops(glcm, 'energy')[0]\n",
        "      features['correlation' + str(distance) + name] = feature.graycoprops(glcm, 'correlation')[0]\n",
        "      features['ASM' + str(distance) + name] = feature.graycoprops(glcm, 'ASM')[0]\n",
        "\n",
        "      # https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.graycoprops\n",
        "      # Compute a feature of a gray level co-occurrence matrix to serve as a compact summary of the matrix.\n",
        "      # The properties are computed as follows:\n",
        "      # contrast\n",
        "      # dissimilarity\n",
        "      # homogeneity\n",
        "      # ASM\n",
        "      # energy\n",
        "\n",
        "  return features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get connected components of the ground truth binary image\n",
        "def componentsStatsGroundTruth(matrix):\n",
        "    # getting the info of the components in the ground truth\n",
        "    # second value is connectivity 4 or 8\n",
        "    connectedComponentsGroundTruth = cv2.connectedComponentsWithStats(matrix, 8, cv2.CV_32S)\n",
        "\n",
        "    # Get the results\n",
        "    # The first cell is the number of labels\n",
        "    num_labels = connectedComponentsGroundTruth[0]\n",
        "    # The second cell is the label matrix\n",
        "    labels = connectedComponentsGroundTruth[1]\n",
        "    # The third cell is the stat matrix\n",
        "    stats = connectedComponentsGroundTruth[2]\n",
        "    # The fourth cell is the centroid matrix\n",
        "    centroids = connectedComponentsGroundTruth[3]\n",
        "\n",
        "    return num_labels, labels, stats, centroids"
      ],
      "metadata": {
        "id": "d7G2psxqargQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAXteeoxLM_z"
      },
      "outputs": [],
      "source": [
        "#import show special for google colab\n",
        "from google.colab.patches import cv2_imshow\n",
        "#import plt for display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#go into de directory of the images\n",
        "\n",
        "# this have 3 outputs root directory, the folders in the path and the files in the path.\n",
        "# we ignore _ the two first because we are not interested in those\n",
        "_, _, images = next(os.walk(os.path.join(DATA_DIR,'images')))\n",
        "_, _, breastMasks = next(os.walk(os.path.join(DATA_DIR,'masks')))\n",
        "_, _, groundTruths = next(os.walk(os.path.join(DATA_DIR, 'groundtruths')))\n",
        "\n",
        "images.sort()\n",
        "breastMasks.sort()\n",
        "groundTruths.sort()\n",
        "\n",
        "# read numbers of normal images\n",
        "normals = []\n",
        "with open(os.path.join(DATA_DIR,'normals.txt')) as f:\n",
        "    for line in f:\n",
        "        normals.append(line[:-1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/@robertbracco1/configuring-google-colab-like-a-pro-d61c253f7573#a642\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gGhsM747LRj7",
        "outputId": "90605737-4818-43b1-ebde-f3b097cb15ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.patches import Circle\n",
        "\n",
        "# function to draw the grid to display\n",
        "def display_grid(figure, axis, img, imgGroundTruth, preprocessed, candidates, features):\n",
        "  # draw in the axis the img\n",
        "  axis[0][0].imshow(img)\n",
        "  # switch off the axis of the plot\n",
        "  axis[0][0].axis('off')\n",
        "  # set a title for the plot\n",
        "  axis[0][0].set_title('Image')\n",
        "\n",
        "  axis[0][1].imshow(imgGroundTruth, cmap='gray')\n",
        "  axis[0][1].axis('off')\n",
        "  axis[0][1].set_title('Ground Truth')\n",
        "\n",
        "  axis[0][2].imshow(imgMask)\n",
        "  axis[0][2].axis('off')\n",
        "  axis[0][2].set_title('Breast Mask')\n",
        "\n",
        "  axis[1][0].imshow(preprocessed, cmap='gray')\n",
        "  axis[1][0].axis('off')\n",
        "  axis[1][0].set_title('Preprocessed')\n",
        "\n",
        "  # draw candidates as circles\n",
        "  axis[1][1].imshow(preprocessed, cmap='gray')\n",
        "  axis[1][1].axis('off')\n",
        "  axis[1][1].set_title('Candidates')\n",
        "\n",
        "  # Now, loop through coord arrays, and create a circle at each x,y pair\n",
        "  for y,x,sigma in candidates:\n",
        "    blob = Circle((x,y), sigma*5, color='blue', fill=False)\n",
        "    axis[1][1].add_patch(blob)\n",
        "\n",
        "  axis[1][2].imshow(imgGroundTruth, cmap='gray')\n",
        "  axis[1][2].axis('off')\n",
        "  axis[1][2].set_title('compare with ground truth and candidates')\n",
        "\n",
        "  # Now, loop through coord arrays, and create a circle at each x,y pair\n",
        "  for y,x,sigma in candidates:\n",
        "    blob = Circle((x,y), sigma, color='blue', fill=False)\n",
        "    axis[1][2].add_patch(blob)\n",
        "  \n",
        "  return figure, axis\n"
      ],
      "metadata": {
        "id": "G6mejNA1fgIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQXTnel9jwgS",
        "outputId": "23c20c1a-8341-4e88-9c6a-763897e13dbf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqqX2PGi6-X8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "cfb8561e-d3cc-41e9-968b-f18bb2d321e5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='360' class='' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      87.80% [360/410 1:30:34<12:34]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "\n",
        "#go through the image files \n",
        "for image, breastMask, groundTruth in zip(progress_bar(images), breastMasks, groundTruths):\n",
        "  # choose one\n",
        "  # this are the last 4 of the number of the image name {numbers}_{}_{}_{}_{}_{}.tif\n",
        "  # i suppose those are unique\n",
        "  # if your code is working try other images!\n",
        "  #print(breastMask)\n",
        "  \n",
        "  # restart variables for memory\n",
        "  # to save the candidates\n",
        "  blobs = {}\n",
        "  # to sabe the ground truth connected components\n",
        "  groundTruthsComponents = {}\n",
        "  # to save the features generated with the glcm\n",
        "  features = pd.DataFrame()\n",
        "\n",
        "  # 20588020, 7717, 5328, 3787, 5725, 3859, 6934, 50995872\n",
        "  digits = '5328'\n",
        "\n",
        "  #if ((digits in image) and (digits in breastMask) and ('mask' in breastMask)):\n",
        "  if ('mask' in breastMask):\n",
        "\n",
        "    #upload images\n",
        "    img = cv2.imread(os.path.join(DATA_DIR,'images',image))\n",
        "    imgMask = cv2.imread(os.path.join(DATA_DIR, 'masks', breastMask))\n",
        "    imgGroundTruth = cv2.imread(os.path.join(DATA_DIR, 'groundtruths', image), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "\n",
        "    # processing necessary for evaluation\n",
        "\n",
        "    # Get components of the ground truth\n",
        "    '''\n",
        "    num_labels, labels, stats, centroids = componentsStatsGroundTruth(imgGroundTruth)\n",
        "    groundTruthsComponents[image] = {}\n",
        "    groundTruthsComponents[image]['num_labels'] = num_labels\n",
        "    groundTruthsComponents[image]['labels'] = labels\n",
        "    groundTruthsComponents[image]['stats'] = stats\n",
        "    groundTruthsComponents[image]['centroids'] = centroids\n",
        "    '''\n",
        "    # preprocessing #\n",
        "\n",
        "    imgCopy = copy.deepcopy(img)\n",
        "\n",
        "    # preprocessed = preprocessingDeHazingPy(img, imgMask)\n",
        "    preprocessed = deHazingDarkChannelPriorPy(imgCopy, imgMask)\n",
        "    # after preprocessingDeHazingPy + contrast streching to see\n",
        "    # preprocessed = preprocessingDeHazingPy(imgCopy,imgMask)\n",
        "\n",
        "      # still missing quantum noise supression\n",
        "      # details in the phd defense file\n",
        "\n",
        "      # still missing linear streching\n",
        "\n",
        "      # still missing CLAHE (adaptive histogram equalization) opencv library\n",
        "\n",
        "      # Observations from the results:\n",
        "\n",
        "        # fiber intersections may also appear as bright spots (false positives)\n",
        "\n",
        "      # THINGS WE NOTICE BETWEEN BOTH DEHAZING METHODS\n",
        "\n",
        "        # Better suppression of fatty tissue (noise) and greater enhancement of brightness of desired feature (microcalcifications)\n",
        "\n",
        "        # sometimes for the other dehazing method black patches become present in the fatty tissue\n",
        "        # this did not happen in the dehazing with dark channel prior (and guided filter)\n",
        "\n",
        "        # sharper\n",
        "\n",
        "        # enhance the contrast\n",
        "\n",
        "        # details were enhanced\n",
        "\n",
        "    # candidate extraction #\n",
        "\n",
        "    copyPreprocessed = copy.deepcopy(preprocessed)\n",
        "\n",
        "    # we have to change np.float64 to np.float32 for the grayscale conversion\n",
        "    # leading to a reduction of gray values\n",
        "    copyPreprocessed = copyPreprocessed.astype(np.float32)\n",
        "    copyPreprocessed = cv2.cvtColor(copyPreprocessed, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    candidates = candidateExtraction(copyPreprocessed, imgMask)\n",
        "\n",
        "    # Observations from the results:\n",
        "\n",
        "      # images with pectoral muscule cause false positives\n",
        "\n",
        "    # feature extraction #\n",
        "    features = featuresExtraction(copyPreprocessed, candidates, features, imgMask)\n",
        "\n",
        "\n",
        "    features.to_csv(os.path.join('/content',\n",
        "                                 'drive',\n",
        "                                 'MyDrive',\n",
        "                                 'Results',\n",
        "                                 'features.csv'),\n",
        "                    mode='a',\n",
        "                    index=False,\n",
        "                    header=False)\n",
        "    # machine learning must be applied for the classification of the features extracted\n",
        "\n",
        "    import gc\n",
        "\n",
        "    del features\n",
        "    del preprocessed\n",
        "    del candidates\n",
        "    del blobs\n",
        "    del copyPreprocessed\n",
        "    del imgCopy\n",
        "    del img\n",
        "    del imgMask\n",
        "    del imgGroundTruth\n",
        "    \n",
        "    gc.collect()\n",
        "\n",
        "    # end image processing part #\n",
        "\n",
        "\n",
        "    # processing necessary for evaluation #\n",
        "\n",
        "    # save blobs results to check groundtruth\n",
        "    #blobs[image] = candidates\n",
        "\n",
        "\n",
        "    # display related #\n",
        "\n",
        "    # matrix of plots and size of the figure\n",
        "    #figure, axis = plt.subplots(2, 3, figsize=(15,15))\n",
        "\n",
        "    #display_grid(figure, axis, img, imgGroundTruth, preprocessed, candidates, features)\n",
        "\n",
        "    #plt.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "    # display figure with image\n",
        "    #plt.show()\n",
        "\n",
        "    # display image with other function\n",
        "    #cv2_imshow(features)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "uVP2PHW9E66x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "9781ce88-66cd-45c2-c6fa-2dec787f2ab6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   contrast10  dissimilarity10  homogeneity10  energy10  correlation10  \\\n",
              "0    0.088889         0.088889       0.955556  0.681683       0.823055   \n",
              "\n",
              "      ASM10  contrast190  dissimilarity190  homogeneity190  energy190  ...  \\\n",
              "0  0.464691      0.17284           0.17284         0.91358   0.642094  ...   \n",
              "\n",
              "   homogeneity7180  energy7180  correlation7180   ASM7180  contrast7270  \\\n",
              "0         0.938235    0.884351        -0.028088  0.782076      0.145833   \n",
              "\n",
              "   dissimilarity7270  homogeneity7270  energy7270  correlation7270   ASM7270  \n",
              "0           0.145833         0.927083    0.864409        -0.046269  0.747203  \n",
              "\n",
              "[1 rows x 96 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff5e0c05-d206-453a-8c8b-07f5c1d0827b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>contrast10</th>\n",
              "      <th>dissimilarity10</th>\n",
              "      <th>homogeneity10</th>\n",
              "      <th>energy10</th>\n",
              "      <th>correlation10</th>\n",
              "      <th>ASM10</th>\n",
              "      <th>contrast190</th>\n",
              "      <th>dissimilarity190</th>\n",
              "      <th>homogeneity190</th>\n",
              "      <th>energy190</th>\n",
              "      <th>...</th>\n",
              "      <th>homogeneity7180</th>\n",
              "      <th>energy7180</th>\n",
              "      <th>correlation7180</th>\n",
              "      <th>ASM7180</th>\n",
              "      <th>contrast7270</th>\n",
              "      <th>dissimilarity7270</th>\n",
              "      <th>homogeneity7270</th>\n",
              "      <th>energy7270</th>\n",
              "      <th>correlation7270</th>\n",
              "      <th>ASM7270</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.088889</td>\n",
              "      <td>0.088889</td>\n",
              "      <td>0.955556</td>\n",
              "      <td>0.681683</td>\n",
              "      <td>0.823055</td>\n",
              "      <td>0.464691</td>\n",
              "      <td>0.17284</td>\n",
              "      <td>0.17284</td>\n",
              "      <td>0.91358</td>\n",
              "      <td>0.642094</td>\n",
              "      <td>...</td>\n",
              "      <td>0.938235</td>\n",
              "      <td>0.884351</td>\n",
              "      <td>-0.028088</td>\n",
              "      <td>0.782076</td>\n",
              "      <td>0.145833</td>\n",
              "      <td>0.145833</td>\n",
              "      <td>0.927083</td>\n",
              "      <td>0.864409</td>\n",
              "      <td>-0.046269</td>\n",
              "      <td>0.747203</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 96 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff5e0c05-d206-453a-8c8b-07f5c1d0827b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff5e0c05-d206-453a-8c8b-07f5c1d0827b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff5e0c05-d206-453a-8c8b-07f5c1d0827b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gag035mepoZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0251eb59-6c3f-46ab-9c44-f988650801f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ncv2.imwrite(os.path.join('/content',\\n                        'drive',\\n                        'MyDrive',\\n                        'Results',\\n                         'features.csv'),\\n            features)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# save images\n",
        "'''\n",
        "cv2.imwrite(os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results',\n",
        "                         'features.csv'),\n",
        "            features)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation froc curve #\n",
        "\n",
        "''''\n",
        "fp = 0 # false positive, findings on normal images, don't forget the normals variable\n",
        "tp = 0 # true positive, for the blobs that are inside a component\n",
        "fn = 0 # false negative, for the blobs that do not belong to any component\n",
        "\n",
        "for key in groundTruthsComponents:\n",
        "  # list of features found with y,x and sigma\n",
        "  featuresImg = blobs[key]\n",
        "  \n",
        "  # restart the variables\n",
        "  fp = 0\n",
        "  tp = 0\n",
        "  fn = 0\n",
        "\n",
        "  # is the image a normal image?\n",
        "  if (key in normals):\n",
        "    # if it is false positive\n",
        "    fp = fp + 1\n",
        "    continue\n",
        "\n",
        "  # if it is not register as normal\n",
        "  # stat have 5 items: leftmost x coordinate,\n",
        "  #                    topmost y coordinate,\n",
        "  #                    horizontal size of the bounding box\n",
        "  #                    vertical size of the bounding box\n",
        "  #                    total area in pixels of the connected component\n",
        "  for centroid, stat in zip(groundTruthsComponents[key]['centroids'], groundTruthsComponents[key]['stats']):\n",
        "    # remember one component is the background\n",
        "    \n",
        "    if (stat[0] == 0):\n",
        "      # is the background\n",
        "      continue\n",
        "\n",
        "    # top left is the 0,0 of the image\n",
        "    \n",
        "    topX = stat[0]\n",
        "    bottomX = stat[0] + stat[2]\n",
        "\n",
        "    topY = stat[1]\n",
        "    bottomY = stat[1] + stat[3]\n",
        "\n",
        "    matchs = [1 for feature in featuresImg if (( feature[1] >= topX ) and\n",
        "                                               ( feature[1] <= bottomX ) and\n",
        "                                               ( feature[0] >= topY ) and\n",
        "                                               ( feature[0] <= bottomY )) ]\n",
        "\n",
        "    # true positives\n",
        "    tp = tp + np.sum(matchs)\n",
        "\n",
        "\n",
        "    #false negatives will be the difference between the total true positives and the blobs that we receive\n",
        "\n",
        "  fn = len(featuresImg) - tp\n",
        "\n",
        "  # save and compute tpr and fpr\n",
        "\n",
        "  # to make te roc curve i need scores... clasification :|\n",
        "'''"
      ],
      "metadata": {
        "id": "EJrrb3dxTR6P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "87e67d7a-777b-452a-f875-a5fb99a54b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'\\nfp = 0 # false positive, findings on normal images, don't forget the normals variable\\ntp = 0 # true positive, for the blobs that are inside a component\\nfn = 0 # false negative, for the blobs that do not belong to any component\\n\\nfor key in groundTruthsComponents:\\n  # list of features found with y,x and sigma\\n  featuresImg = blobs[key]\\n  \\n  # restart the variables\\n  fp = 0\\n  tp = 0\\n  fn = 0\\n\\n  # is the image a normal image?\\n  if (key in normals):\\n    # if it is false positive\\n    fp = fp + 1\\n    continue\\n\\n  # if it is not register as normal\\n  # stat have 5 items: leftmost x coordinate,\\n  #                    topmost y coordinate,\\n  #                    horizontal size of the bounding box\\n  #                    vertical size of the bounding box\\n  #                    total area in pixels of the connected component\\n  for centroid, stat in zip(groundTruthsComponents[key]['centroids'], groundTruthsComponents[key]['stats']):\\n    # remember one component is the background\\n    \\n    if (stat[0] == 0):\\n      # is the background\\n      continue\\n\\n    # top left is the 0,0 of the image\\n    \\n    topX = stat[0]\\n    bottomX = stat[0] + stat[2]\\n\\n    topY = stat[1]\\n    bottomY = stat[1] + stat[3]\\n\\n    matchs = [1 for feature in featuresImg if (( feature[1] >= topX ) and\\n                                               ( feature[1] <= bottomX ) and\\n                                               ( feature[0] >= topY ) and\\n                                               ( feature[0] <= bottomY )) ]\\n\\n    # true positives\\n    tp = tp + np.sum(matchs)\\n\\n\\n    #false negatives will be the difference between the total true positives and the blobs that we receive\\n\\n  fn = len(featuresImg) - tp\\n\\n  # save and compute tpr and fpr\\n\\n  # to make te roc curve i need scores... clasification :|\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "features.to_csv( os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results',\n",
        "                        'features.csv'))\n"
      ],
      "metadata": {
        "id": "uo57LPvjVEEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6B64-S9blVud"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of calcificationTestImageProcessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}