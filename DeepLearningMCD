{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hAem5pBbW9eM"},"outputs":[],"source":["from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u27ZtWK1XAY-"},"outputs":[],"source":["import os \n","\n","#first put a shortcut in your drive to the image processing folder\n","\n","RESULTS_DIR = os.path.join('/content',\n","                        'drive',\n","                        'MyDrive',\n","                        'Results')\n","\n","\n","DATA_DIR = os.path.join('/content',\n","                        'drive',\n","                        'MyDrive',\n","                        'Image Processing and Analysis 2022',\n","                        'projects',\n","                        'Calcification Detection',\n","                        'dataset')\n","\n","DATA_PREPROCESSED = os.path.join('/content',\n","                        'drive',\n","                        'MyDrive',\n","                        'Results',\n","                        'CLAHE+Dehazing')\n","\n","PROI_DIR = os.path.join('/content',\n","                        'drive',\n","                        'MyDrive',\n","                        'Results', \n","                        'proi_files')\n","\n","print(os.listdir(RESULTS_DIR))\n","\n","data_file = os.listdir(RESULTS_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqbV7UqFBeKA"},"outputs":[],"source":["pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9rhux8SW0Ts"},"outputs":[],"source":["# import the required packages\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.utils.data as data\n","import copy\n","\n","from torchmetrics import F1Score\n","from torchmetrics.functional import auroc\n","from torchmetrics.functional import precision_recall\n","from torchmetrics import AUROC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0CzC_YGW1Yx"},"outputs":[],"source":["# hyperparameters\n","batch_size = 32\n","learning_rate = 0.001\n","epochs = 30\n","momentum = 0.9\n","lr_step_size = 6   # if < epochs, we are using decaying learning rate\n","lr_gamma = 0.1\n","data_augmentation = True\n","dropout = 0.5\n","activation = nn.ReLU()\n","\n","# make visible only one GPU at the time\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # <-- should be the ID of the GPU you want to use\n","\n","# options\n","# device = \"cuda:0\"           # put here \"cuda:0\" if you want to run on GPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","monitor_display = True      # whether to display monitored performance plots\n","display_first_n = 0         # how many samples/batches are displayed\n","num_workers = 2             # how many workers (=threads) for fetching data\n","pretrained = False          # whether to test a pretrained model (to be loaded) or train a new one\n","display_errors = True       # whether to display errors (only in pretrained mode)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMoibauPrDjm"},"outputs":[],"source":["import torchvision.transforms.functional as TF\n","import random\n","\n","random.seed(1)\n","\n","# -90, 0, 90, and 180 degrees rotation\n","class MyRotationTransform:\n","    \"\"\"Rotate by one of the given angles.\"\"\"\n","\n","    def __init__(self, times, mode):\n","        self.times = times\n","        self.mode = mode\n","\n","    def __call__(self, x):\n","        mode = random.choice(self.mode)\n","        if mode == 0:\n","          return np.fliplr(x)\n","        elif mode == 1:\n","          return np.flipud(x)\n","        else:   \n","          times = random.choice(self.times)\n","          return np.rot90(x, times)\n","\n","\n","transform_train = transforms.Compose(\n","    [MyRotationTransform(times=[1,2,3], mode=[0,1,2,3])]) \n"]},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","#import plt for display\n","import matplotlib.pyplot as plt\n","\n","def calculate_mu_std(image_path):\n","\n","  #go into de directory of the images\n","\n","  # this have 3 outputs root directory, the folders in the path and the files in the path.\n","  # we ignore _ the two first because we are not interested in those\n","  _, _, images = next(os.walk(os.path.join(DATA_DIR,'images')))\n","  _, _, breastMasks = next(os.walk(os.path.join(DATA_DIR,'masks')))\n","  _, _, groundTruths = next(os.walk(os.path.join(DATA_DIR, 'groundtruths')))\n","\n","  images.sort()\n","  breastMasks.sort()\n","  groundTruths.sort()\n","\n","  # read numbers of normal images\n","  normals = []\n","  with open(os.path.join(RESULTS_DIR,'normals_final.txt')) as f:\n","      for line in f:\n","          normals.append(line[:-1])\n","\n","  count = 0\n","\n","  for image, breastMask in zip(images, breastMasks):\n","    if int(image.split(\"_\")[0]) in test_keys:\n","      img = cv2.imread(os.path.join(image_path,image))\n","      mask = cv2.imread(os.path.join(DATA_DIR,'masks',breastMask), cv2.IMREAD_GRAYSCALE) > 0\n","      maskedImg = img[mask]\n","      if count == 0:\n","        concatImages = maskedImg\n","        count += 1\n","      else:\n","        concatImages = np.concatenate((concatImages, maskedImg))\n","\n","  totalmean = np.mean(concatImages)\n","  totalstd = np.std(concatImages)\n","  print(totalmean)\n","  print(totalstd)\n","\n","  return totalmean, totalstd"],"metadata":{"id":"BcdUS_p2ExJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHt3_FWoJHa0"},"outputs":[],"source":["# Calculated from the original training set \n","# mu = [23227.7412] # mean for train set - validation)\n","# std = [3587.8953] # Standard deviationa\n","\n","# FOR PREPROCESSED IMAGES\n","# mu = [120.5729638299031] # mean for train set complete, validation is now taken from the test set\n","# std = [29.40322122720545]\n","\n","\n","mu, std = calculate_mu_std(os.path.join(DATA_DIR,'images'))\n","\n","\n","mu = [23239.753785769954] # mean for train set complete, validation is now taken from the test set\n","std = [3221.0769309030593]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJ4ZsdP4ZVj5"},"outputs":[],"source":["transform_normalize = transforms.Normalize(mean=[mu], std=[std])"]},{"cell_type":"markdown","metadata":{"id":"gm8zZBDDYdjy"},"source":["**Load information**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZxH2BvIYZae"},"outputs":[],"source":["windowSize = 12\n","noMC_file_train = os.path.join(PROI_DIR, 'train-validation-test-overlap','2-3', 'noMC.w{}.train.proi'.format(windowSize))\n","MC_file_train = os.path.join(PROI_DIR, 'train-validation-test-overlap','2-3', 'MC.w{}.train.proi'.format(windowSize))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC4zps30ZIpJ"},"outputs":[],"source":["noMC_file_test = os.path.join(PROI_DIR, 'train-validation-test-overlap','2-3', 'noMC.w{}.test.proi'.format(windowSize))\n","MC_file_test = os.path.join(PROI_DIR, 'train-validation-test-overlap','2-3', 'MC.w{}.test.proi'.format(windowSize))\n","noMC_file_validation = os.path.join(PROI_DIR, 'train-validation-test-overlap','2-3', 'noMC.w{}.validation.proi'.format(windowSize))\n","MC_file_validation = os.path.join(PROI_DIR, 'train-validation-test-overlap','2-3', 'MC.w{}.validation.proi'.format(windowSize))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSDpWbCcXc9o"},"outputs":[],"source":["import roi_cc_project_3 as roi_cc_project\n","\n","dataset_train = roi_cc_project.CvROI([noMC_file_train, MC_file_train], os.path.join(DATA_DIR, 'images'), img_prefix='', img_suffix='', img_channel=None, img_list='', train=True,\n","                 crossvalid=(1, 1), class_weights=[10,1], class_max_counts=None, class_min_counts=None,\n","                 preprocessing=transform_normalize, augmentation=transform_train, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOX28sLE04eK"},"outputs":[],"source":["dataset_train.class_augmentations"]},{"cell_type":"code","source":["dataset_train.class_total_counts"],"metadata":{"id":"ghoiFjYpzAWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBc-tCQtH_1u"},"outputs":[],"source":["def plot_true_patches():\n","  roi_1 = []\n","  for roi, label in dataset_train:\n","    if label == 1:\n","      roi_1.append(roi)\n","    if len(roi_1)>1:\n","      break\n","  for i in range(len(roi_1)):\n","    plt.imshow(roi_1[i].numpy()[0,:,:], cmap='gray')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgZ7fwwzZNZC"},"outputs":[],"source":["import roi_cc_project_3 as roi_cc_project\n","\n","dataset_validation = roi_cc_project.CvROI([noMC_file_validation, MC_file_validation], os.path.join(DATA_DIR, 'images'), img_prefix='', img_suffix='', img_channel=None, img_list='', train=False,\n","                 crossvalid=(1, 1), class_weights=None, class_max_counts=None, class_min_counts=None,\n","                 preprocessing=transform_normalize, augmentation=None, verbose=True)\n"]},{"cell_type":"markdown","metadata":{"id":"pz8hf2H6uAaQ"},"source":["**Dataloader**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GM_jp5acK70y"},"outputs":[],"source":["random.seed(1)\n","torch.manual_seed(1)\n","torch.cuda.manual_seed(1)\n","np.random.seed(1)\n","\n","\n","dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True) \n","dataloader_val = torch.utils.data.DataLoader(dataset_validation, batch_size=batch_size, num_workers=num_workers, pin_memory=True)  # what is pin memory true\n","\n","dataloader = {'train': dataloader_train, 'eval': dataloader_val}\n","dataset_sizes = {'train': len(dataset_train), 'eval': len(dataset_validation)}"]},{"cell_type":"markdown","metadata":{"id":"9DDuCOfTYaKM"},"source":["## **Model definition**"]},{"cell_type":"markdown","source":["### CNN with incremental and classification block "],"metadata":{"id":"dOc4dSKtXNq7"}},{"cell_type":"code","source":["import numpy as np\n","from collections import OrderedDict\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torchsummary import summary\n","\n","class IncrementalBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super(IncrementalBlock, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.init_weigts()\n","        \n","    def init_weigts(self):\n","        nn.init.xavier_uniform_(self.conv1.weight)\n","        self.conv1.bias.data.fill_(0.0)\n","        nn.init.xavier_uniform_(self.conv2.weight)\n","        self.conv2.bias.data.fill_(0.0)\n","        \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        return x\n","\n","\n","class ClassificationBlock(nn.Module):\n","    def __init__(self, in_features=288):\n","        super(ClassificationBlock, self).__init__()\n","        \n","        self.fc1 = nn.Linear(in_features=in_features, out_features=256)\n","        self.fc2 = nn.Linear(in_features=256, out_features=256)\n","        self.out = nn.Linear(in_features=256, out_features=2)\n","        self.init_weigts()\n","        \n","    def init_weigts(self):\n","        nn.init.xavier_uniform_(self.fc1.weight)\n","        self.fc1.bias.data.fill_(0.0)\n","        nn.init.xavier_uniform_(self.fc2.weight)\n","        self.fc2.bias.data.fill_(0.0)\n","        nn.init.xavier_uniform_(self.out.weight)\n","        self.out.bias.data.fill_(0.0)\n","        \n","    def forward(self, x):\n","        x = F.dropout(self.fc1(x))\n","        x = F.dropout(self.fc2(x))\n","        x = self.out(x)\n","        return x\n","\n","\n","class CNN_final(nn.Module):\n","    def __init__(self, input_size):\n","        super(CNN_final, self).__init__()\n","        \n","        self.num_ic = int(np.log2(input_size))-1\n","        incremental_blocks = OrderedDict([])\n","        for i in range(self.num_ic):\n","            if i == 0:\n","                in_channels = 1\n","            else:\n","                in_channels = 32\n","            incremental_blocks.update({f\"IC{i}\": IncrementalBlock(in_channels)})\n","        self.conv = nn.ModuleDict({\"incremental_block\": nn.Sequential(incremental_blocks)})\n","        self.flatten = nn.Flatten()\n","        self.classifier = ClassificationBlock()\n","\n","    def forward(self, x):\n","        for i in range(self.num_ic):\n","            x = self.conv.incremental_block[i](x)\n","        x = self.flatten(x)\n","        x = self.classifier(x)\n","        return x\n"],"metadata":{"id":"e0dWqccXXnAQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### CNN using depthwise convolution"],"metadata":{"id":"1Y4rAKLbXew0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KHpATwIW29l"},"outputs":[],"source":["\n","###\n","# Haq, I.U., Ali, H., Yu, W.H., Lei, C., Ali, H., Feature fusion and ensemble learning\u0002based CNN model for mammographic image classification, Journal of King Saud University - Computer and\n","# Information Sciences (2022), doi: https://doi.org/10.1016/j.jksuci.2022.03.023\n","###\n","\n","class CD_CNN(nn.Module):\n","    def __init__(self):\n","        super(CD_CNN, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) \n","        self.dropout1=nn.Dropout(p=0.2)\n","        self.bn3 = nn.BatchNorm2d(64)\n","\n","        self.dwconv1 = nn.Conv2d(in_channels=64, out_channels=128, groups= 64, kernel_size=3, padding=1) \n","        self.bn4 = nn.BatchNorm2d(128)\n","        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n","        self.bn5 = nn.BatchNorm2d(128)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.dropout2=nn.Dropout(p=0.2)\n","        self.bn6 = nn.BatchNorm2d(128)\n","\n","        self.fc1 = nn.Linear(128*3*3,512) #128*3*3 is output size of previous stage\n","        self.fc2 = nn.Linear(512,2)\n","\n","      \n","        \n","        self.act = activation\n","\n","\n","        nn.init.xavier_normal_(self.conv1.weight)\n","        nn.init.xavier_normal_(self.conv2.weight)\n","        nn.init.xavier_normal_(self.conv3.weight)\n","        nn.init.xavier_normal_(self.fc1.weight)\n","        nn.init.xavier_normal_(self.fc2.weight)\n","        nn.init.xavier_normal_(self.dwconv1.weight)\n","\n","\n","    def forward(self, x):\n","        \n","        x = self.act(self.bn1(self.conv1(x)))\n","        x = self.act(self.bn2(self.conv2(x)))\n","        x = self.pool1(x)\n","        x = self.bn3(self.dropout1(x))\n","        # print(x.size())\n","\n","        x = self.act(self.bn4(self.dwconv1(x)))\n","        x = self.act(self.bn5(self.conv3(x)))\n","        x = self.pool2(x)\n","        x = self.bn6(self.dropout2(x))\n","\n","        x = x.view(-1, 128*3*3)\n","        x = self.act(self.fc1(x))\n","\n","\n","        x = self.fc2(x)\n","\n","        return x"]},{"cell_type":"markdown","source":["## Resnet 50"],"metadata":{"id":"fYculfVJW7_H"}},{"cell_type":"code","source":["\n","import torchvision.models as models\n","\n","# pretrained resnet 50\n","resnet50 = models.resnet50(pretrained=True)\n","\n","# freezing the first 10 layers\n","ct = 0\n","# model.children() which returns layers.\n","for child in resnet50.children():\n"," ct += 1\n"," if ct < 3:\n","     for param in child.parameters():\n","      # True means it will be backpropagrated\n","      # to freeze a layer you need to set requires_grad to False for all parameters of a layer. \n","       param.requires_grad = False\n","\n","# change last fully convolutional layer so it returns just 2 classes\n","resnet50.fc = nn.Linear(2048,2)\n"],"metadata":{"id":"bsvjDiFRD1ON"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Useful Metrics and Functions**"],"metadata":{"id":"T-foaC4XPd9A"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uB8A7rDS0EG5"},"outputs":[],"source":["def confusion(prediction, truth):\n","    \"\"\" Returns the confusion matrix for the values in the `prediction` and `truth`\n","    tensors, i.e. the amount of positions where the values of `prediction`\n","    and `truth` are\n","    - 1 and 1 (True Positive)\n","    - 1 and 0 (False Positive)\n","    - 0 and 0 (True Negative)\n","    - 0 and 1 (False Negative)\n","    \"\"\"\n","\n","    confusion_vector = prediction / truth\n","    # Element-wise division of the 2 tensors returns a new tensor which holds a\n","    # unique value for each case:\n","    #   1     where prediction and truth are 1 (True Positive)\n","    #   inf   where prediction is 1 and truth is 0 (False Positive)\n","    #   nan   where prediction and truth are 0 (True Negative)\n","    #   0     where prediction is 0 and truth is 1 (False Negative)\n","\n","    true_positives = torch.sum(confusion_vector == 1).item()\n","    false_positives = torch.sum(confusion_vector == float('inf')).item()\n","    true_negatives = torch.sum(torch.isnan(confusion_vector)).item()\n","    false_negatives = torch.sum(confusion_vector == 0).item()\n","\n","    return true_positives, false_positives, true_negatives, false_negatives"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EyyszbtD02Pd"},"outputs":[],"source":["def mcc(tp, fp, tn, fn):\n","  # Formula taken from https://en.wikipedia.org/wiki/Phi_coefficient\n","  num = (tp*tn) - (fp*fn)\n","  den = ((tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)) ** 0.5\n","\n","  if den != 0:\n","    return num/den\n","  else:\n","    return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qd_99B-tUrRV"},"outputs":[],"source":["def save(model, path_to_save: str) -> None:\n","    torch.save(model.state_dict(), path_to_save)\n","\n","def load(model, path_to_model: str):\n","    return model.load_state_dict(torch.load(path_to_model))"]},{"cell_type":"markdown","source":["# **Train and test function**"],"metadata":{"id":"W1pUh_a6PqMN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLRT9aFctJm9"},"outputs":[],"source":["def train_model(model, criterion, optimizer, scheduler, model_name, num_epochs=30, load_trained=False):\n","    since = time.time()\n","    f1 = F1Score(num_classes=2).to(device)\n","\n","    if load_trained:\n","      checkpoint = torch.load(RESULTS_DIR + model_name)\n","      model.load_state_dict(checkpoint['model_state_dict'])\n","      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","      last_epoch = checkpoint['epoch']+1\n","      loss = checkpoint['loss']\n","\n","    else:\n","      last_epoch=0\n","\n","\n","    best_model_wts = copy.deepcopy(model.state_dict()) #It keeps track of the parameters of the model in certain state\n","    best_auc = 0.0\n","    losses_train = []\n","    losses_val = []\n","    pAUCs_train = []\n","    pAUCs_val = []\n","  \n","    for epoch in range(last_epoch, num_epochs):\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-' * 10)\n","\n","\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'eval']:\n","          \n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","    #            running_corrects = 0\n","            f1_history = list()\n","\n","            y_true = list()\n","            y_probs = list()\n","\n","\n","            tp_total = 0\n","            fp_total = 0\n","            tn_total = 0\n","            fn_total = 0 \n","\n","            # Iterate over data.\n","            for inputs, labels in dataloader[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","                \n","                with torch.no_grad():\n","                    preds = torch.argmax(outputs, dim=1)\n","                    probs = F.softmax(outputs, dim=1)[:,1]\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                f1_history.append(f1(preds, labels.data).double().cpu().numpy()) \n","                y_true.append(labels.data.cpu())\n","                y_probs.append(probs.cpu())\n","\n","                tp, fp, tn, fn = confusion(preds, labels.data)\n","                tp_total += tp\n","                fp_total += fp\n","                tn_total += tn\n","                fn_total += fn\n","    #                print(tp, fp, tn, fn) \n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_f1 = np.mean(f1_history)\n","            epoch_auc = auroc(torch.cat(y_probs, dim=0), torch.cat(y_true, dim=0), max_fpr=0.0001).item()\n","            epoch_mcc = mcc(tp_total, fp_total, tn_total, fn_total)\n","\n","            if phase == 'train':\n","                scheduler.step()\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': epoch_loss,\n","                    }, RESULTS_DIR+ model_name)  \n","                losses_train.append(epoch_loss)\n","                pAUCs_train.append(epoch_auc)\n","            else:\n","                losses_val.append(epoch_loss)\n","                pAUCs_val.append(epoch_auc)\n","\n","\n","            print('{} Loss: {:.4f} F1-score: {:.4f}'.format(\n","                phase, epoch_loss, epoch_f1))\n","            print('MCC: ', epoch_mcc)\n","            print('AUC: ', epoch_auc)                        \n","\n","            # deep copy the model\n","            if phase == 'eval' and epoch_auc > best_auc:\n","                best_auc = epoch_auc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","            del f1_history, y_true, y_probs\n","\n","\n","        print()\n","\n","    plt.title('Loss per epoch')\n","    plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), losses_train, label ='train')\n","    plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), losses_val, label ='test')\n","    plt.legend()    \n","    plt.show()\n","\n","    plt.title('pAUC per epoch')\n","    plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), pAUCs_train, label ='train')\n","    plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), pAUCs_val, label ='test')\n","    plt.legend()    \n","    plt.show()\n","\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val mcc: {:4f}'.format(best_auc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnYAcElltZhk"},"outputs":[],"source":["# define test function\n","# returns predictions\n","def test(dataset, dataloader):\n","\n","    # switch to test mode\n","    net.eval()  \n","\n","    # initialize predictions\n","    predictions = []\n","    probs = []\n","    reals = [] #torch.zeros(len(dataset), dtype=torch.int64)\n","    sample_counter = 0\n","\n","    # do not accumulate gradients (faster)\n","    with torch.no_grad():\n","\n","        # test all batches\n","        for inputs, labels in dataloader:\n","\n","            # get data from dataloader [ignore labels/targets as they are not used in test mode]\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            # forward pass\n","            outputs = net(inputs)\n","\n","            # store predictions\n","            preds = torch.argmax(outputs, dim=1)\n","            prob = F.softmax(outputs, dim=1)[:,1]\n","\n","            predictions.append( preds)\n","            probs.append(prob.cpu())\n","            reals.append(labels.data)\n","            sample_counter += 1 \n","\n","    return predictions, probs, reals"]},{"cell_type":"code","source":["# define test function\n","# returns predictions\n","def test_probabilities(dataloader, net):\n","\n","    # switch to test mode\n","    net.eval()  \n","\n","    # initialize predictions\n","    predictions = []\n","    probs = []\n","    indexes = [] #torch.zeros(len(dataset), dtype=torch.int64)\n","    sample_counter = 0\n","\n","    # do not accumulate gradients (faster)\n","    with torch.no_grad():\n","\n","        # test all batches\n","        for inputs, idx in dataloader:\n","\n","            # get data from dataloader [ignore labels/targets as they are not used in test mode]\n","            inputs = inputs.to(device)\n","            # forward pass\n","            outputs = net(inputs)\n","\n","            # store predictions\n","            preds = torch.argmax(outputs, dim=1)\n","            prob = F.softmax(outputs, dim=1)[:,1]\n","            indexes.append(idx)\n","\n","            predictions.append( preds)\n","            probs.append(prob.cpu())\n","            sample_counter += 1 \n","\n","    return predictions, probs, indexes"],"metadata":{"id":"NNRDPZOgbCRP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Model initialization and Training**"],"metadata":{"id":"jbWX0kP4Qhrw"}},{"cell_type":"code","source":["windowSize"],"metadata":{"id":"a3bj1qorxG1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Emo3pdAQIPe6"},"outputs":[],"source":["net = CNN_final(windowSize).to(device) #we need to also send the model to the GPU as well\n","\n","# create loss function\n","criterion = nn.CrossEntropyLoss() #most used for classification purposes\n","\n","# create SGD optimizer\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005) #most common optimizer is adam\n","\n","# create learning rate scheduler\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=lr_gamma)\n","\n","# experiment ID\n","experiment_ID = \"%s_%s_%s_bs(%d)lr(%.4f_%d_%.1f)m(%.1f)e(%d)act(%s)xavier(yes)da(%s)do(%.1f)BN\" % (type(net).__name__, type(criterion).__name__, type(optimizer).__name__,\n","                batch_size, learning_rate, lr_step_size, lr_gamma, momentum, epochs, type(activation).__name__, data_augmentation, dropout)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SugDntlJSAgh"},"outputs":[],"source":["model = train_model(net, criterion, optimizer, scheduler,f'/model_{windowSize}_5_fixedlabels_2-3sep.pt', num_epochs=epochs, load_trained=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rf_vzbUBZA-P"},"outputs":[],"source":["save(model, RESULTS_DIR+f'/model_{windowSize}_5_fixedlabels_2-3sep_best.pt')"]},{"cell_type":"markdown","source":["## **Model Testing**"],"metadata":{"id":"XFBKnnwrTgaU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Ji-9niw54kh"},"outputs":[],"source":["del dataset_train\n","del dataset_validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ww11T0595xhv"},"outputs":[],"source":["import roi_cc_project_3 as roi_cc_project\n","\n","dataset_test = roi_cc_project.CvROI([noMC_file_test, MC_file_test], DATA_PREPROCESSED, img_prefix='', img_suffix='', img_channel=None, img_list='', train=False,\n","                 crossvalid=(1, 1), class_weights=None, class_max_counts=None, class_min_counts=None,\n","                 preprocessing=transform_normalize, augmentation=None, verbose=True)\n","# dataset = roi_cc_project.UnlabeledImageROI(os.path.join(DATA_DIR, 'images', '20586908_6c613a14b80a8591_MG_R_CC_ANON.tif'),os.path.join(DATA_DIR, 'groundtruths', '20586908_6c613a14b80a8591_MG_R_CC_ANON.tif'), (12,12), img_channel=None, preprocessing=None, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pGR-Sur6NSm"},"outputs":[],"source":["dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, num_workers=num_workers, pin_memory=True) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wy_vO9jx5cl9"},"outputs":[],"source":["predictions, probs, reals = test(dataset_test, dataloader_test)"]},{"cell_type":"code","source":["tp, fp, tn, fn = confusion(torch.cat(predictions, dim=0), torch.cat(reals, dim=0))\n","final_mcc = mcc(tp, fp, tn, fn)\n","pAUC = auroc(torch.cat(probs, dim=0).cpu(), torch.cat(reals, dim=0).cpu(), max_fpr=0.0001).item()\n","\n","print('Final MCC: ', final_mcc)\n","print('Final pAUC: ', pAUC)"],"metadata":{"id":"Ca06KjjiDBC6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Probability Maps**"],"metadata":{"id":"_OMeV92DKEg5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfjBGhBEaSoh"},"outputs":[],"source":["def create_probability_map(imageKey, roi_size):\n","  dataset_test_prob = roi_cc_project.UnlabeledImageROI(os.path.join(DATA_DIR, 'images', imageKey), (roi_size,roi_size), img_channel=None, preprocessing=transform_normalize, verbose=True)\n","  dataloader_test_prob = torch.utils.data.DataLoader(dataset_test_prob, batch_size=50, num_workers=num_workers, pin_memory=True) \n","  predictions, probs, indexes = test_probabilities(dataloader_test_prob, net)\n","  probs = torch.cat(probs, dim=0).cpu()\n","  indexes = torch.cat(indexes, dim=0).cpu()\n","  dataset_test_prob.store(probs, indexes) # This line generates the probability map that can be accessed by dataset_test_prob.prob_image\n","  cv2.imwrite(os.path.join(RESULTS_DIR, 'Probability_maps', str(roi_size), imageKey), dataset_test_prob.prob_image)\n","  df = pd.DataFrame()\n","  df['indexes'] = indexes\n","  df['probs'] = probs\n","  df.to_csv(os.path.join(RESULTS_DIR, 'Probability_maps', str(roi_size), imageKey+'.csv'), index=False)\n","  print('Probability map saved')\n","  del dataset_test_prob, dataloader_test_prob, predictions, probs, indexes\n","#  return dataset_test_prob.prob_image, probs, indexes\n"]},{"cell_type":"code","source":["import cv2\n","import pandas as pd\n","from skimage import measure\n","from tqdm.notebook import tqdm_notebook\n","import gc\n","import roi_cc_project_3 as roi_cc_project\n","\n","def calculateFROC(groundTruthsList, normals, roi_size, pipeline, version=3):\n","\n","  # evaluation froc curve #\n","\n","  fn = 0 # false negative, for the blobs that do not belong to any component\n","  positive_candidates = 0\n"," \n","  tpROC = [] # To store all the True Positives\n","  fp_1 = [] # This will take all of the probabilities in a normal image as FP\n","  fp_2 = [] # This will take all the probabilities >0.5 as FP\n","  fp_3 = [] # This will take the max probability of Connected component with threshold of 0.5\n","  already_saved = os.listdir(os.path.join(RESULTS_DIR, 'Probability_maps', str(roi_size)))  \n","\n","  for imageKey in tqdm_notebook(groundTruthsList): # Go through all the grountruth images in the test set\n","\n","    evaluationList = []\n","\n","    # Obtain probability of the image with the dataloader\n","    if imageKey not in already_saved:\n","      prob_image, probs, indexes = create_probability_map(imageKey, roi_size)\n","    else:\n","      prob_image = cv2.imread(os.path.join(RESULTS_DIR, 'Probability_maps', str(roi_size), imageKey), cv2.IMREAD_UNCHANGED)\n","      dfs_probs_im = pd.read_csv(os.path.join(RESULTS_DIR, 'Probability_maps', str(roi_size), imageKey+'.csv'))\n","      probs = dfs_probs_im['probs'].values.tolist()\n","      indexes = dfs_probs_im['indexes'].values.tolist()\n","      print('Probability map restored')\n","\n","    # If the image is within the normals, find only FP \n","    if (imageKey.split('_')[0] in normals):\n","\n","      fp_threshold = prob_image >= 0.5  #Thresholding the probability image\n","      fp_threshold_labels, fp_thr_count = measure.label(fp_threshold, background=0, return_num=True) #Getting labels of the connected components and the amount of them without considering the zero\n","\n","      if version == 3:\n","        for thrs in range(1,fp_thr_count+1): # Go through the connected components found in the thresholded prob image\n","          fp_mask = (fp_threshold_labels == thrs) # Get the mask of a specific connected component (cc)\n","          fp_3.append(['FP', max(prob_image[fp_mask])]) # save highest prob of that specific CC\n","      else:\n","        for index in indexes: # Go through all of the patches in the image\n","          if version == 1:\n","            fp_1.append(['FP', probs[index]])  # Save every probability as FP\n","          elif probs[index] >= 0.5:\n","            fp_2.append(['FP', probs[index]])  # Save probabilities greater than 0.5 has FP\n","\n","        \n","      continue #Skip to the next image\n","\n","    # Read the groundTruth as mask\n","    mask = cv2.imread(os.path.join(DATA_DIR, 'groundtruths', imageKey), cv2.IMREAD_GRAYSCALE)\n","\n","    blobs = mask > 0.7 * mask.mean() #Thresholding the backgroudnd\n","    blobs_labels, count = measure.label(blobs, background=0, return_num=True) #Getting labels of the connected components and the amount of them without considering the zero\n","\n","    # dictionaries, to keep track of probabilities of each CC in the GroundTruth\n","    dictCounting={}\n","\n","  #  print(count)\n","    for index in range(1, count+1):\n","      dictCounting[index] = 0 #initialize dictionaries with 0\n","    \n","\n","    for mc in range(1, count+1): #count is the amount of MC that the groundTruth has\n","      mc_mask = (blobs_labels == mc) # get the mask of a specific MC\n","      # area there is no function?\n","      if np.sum(mc_mask) > np.floor(np.pi*(15/2.0)**2):\n","#        print(\"Too big... discarded\")\n","        continue\n","      dictCounting[mc] = np.max(prob_image[mc_mask])\n","\n","    for key,value in dictCounting.items():\n","      if value >= 0.5:\n","        tpROC.append(['TP', value])\n","      else:\n","        fn = fn + 1\n","\n","    del probs, indexes, prob_image, mask, blobs_labels, count\n","    gc.collect()\n","\n","  if version == 3:\n","    tpROC.extend(fp_3)\n","  elif version == 2:\n","    tpROC.extend(fp_2)\n","  else:\n","    tpROC.extend(fp_1)\n","\n","  filename_key = 'FROC_calculations_{}_roi{}_fn{}_normals{}_version{}.csv'.format(pipeline, roi_size, fn, len(normals), version)\n","\n","  dfROC = pd.DataFrame(tpROC, columns=['type', 'prob'])\n","\n","  dfROC.to_csv(os.path.join('/content',\n","                                'drive',\n","                                'MyDrive',\n","                                'Results',\n","                                filename_key), index=False)\n","  \n","  print(\"File saved as \", filename_key)\n","  del tpROC\n","\n","\n","  return fn, dfROC, filename_key"],"metadata":{"id":"b82g7s2ELFCv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fastprogress import progress_bar\n","import matplotlib.pyplot as plt\n","from numpy import trapz\n","\n","def draw_curve(fn, normals, dfROC, name):\n","  tpc = 0\n","  fpc = 0\n","  tpr = []\n","  fppi = []\n","\n","  dfROC['prob'] = [round(i,4) for i in dfROC['prob'].values]\n","  dfROC = dfROC.sort_values('prob', ascending=False)\n","\n","  thresholds = dfROC.prob.unique()\n","  print('Number of thresholds: ', len(thresholds))\n","  print(thresholds)\n","\n","\n","  tp = len(dfROC.loc[dfROC.type == 'TP'])\n","  print('true positives: ', tp)\n","\n","  print('false positives: ',  len(dfROC.loc[dfROC.type == 'FP']))\n","\n","  print(\"Total number of positives: \", tp+fn)\n","\n","  for i in progress_bar(range(len(thresholds))):\n","\n","    tpc += len(dfROC.loc[(dfROC.prob > thresholds[i]) & (dfROC.type=='TP')])\n","    fpc += len(dfROC.loc[(dfROC.prob > thresholds[i]) & (dfROC.type!='TP')])\n","              \n","    # print('TP amount {} in threshold {}'.format(tpc, thresholds[i]))\n","    # print('FP amount {} in threshold {}'.format(fpc, thresholds[i]))\n","    \n","\n","    tpr.append( tpc/(tp+fn) )\n","    fppi.append( fpc/normals )\n","    tpc = 0\n","    fpc = 0\n","\n","\n","  print('AUC TOTAL:', trapz(tpr, x=fppi)/max(fppi))\n","  fig, ax = plt.subplots()\n","  ax.plot(fppi, tpr)\n","  ax.set_xlabel('fPpI', fontsize=15)\n","  ax.set_ylabel('TPR', fontsize=15)\n","  ax.grid(True)\n","  plt.ylim(0,1)\n","  plt.show()\n","\n","  gamma = [i for i in fppi if i <= 50 ]\n","\n","  print('AUC final:', trapz(tpr[0:len(gamma)], x=gamma)/max(gamma))\n","  fig1, ax1 = plt.subplots()\n","  ax1.plot(gamma, tpr[0:len(gamma)])\n","  ax1.set_xlabel('fPpI', fontsize=12)\n","  ax1.set_ylabel('TPR', fontsize=12)\n","  ax1.grid(True)\n","  plt.ylim(0,1)\n","  plt.xlim(0,50)\n","\n","  plt.savefig(os.path.join('/content',\n","                          'drive',\n","                          'MyDrive',\n","                          'Results', name+'.eps'), format='eps')\n","\n","  plt.show()"],"metadata":{"id":"TjgPSNkPiLv7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["normals = []\n","with open(os.path.join(RESULTS_DIR,'normals_final.txt')) as f:\n","    for line in f:\n","        normals.append(line[:-1])\n","\n","test_keys = list(dataset_test.images_pool.keys())\n","len(test_keys)"],"metadata":{"id":"zph19iFgMBZa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import roi_cc_project_3 as roi_cc_project\n","from fastprogress import progress_bar\n","import cv2 \n","import pandas as pd\n","from tqdm.notebook import tqdm_notebook\n","\n","\n","test_keys_gt = [i.split('_')[0] for i in test_keys]\n","normals_short = [i for i in normals if i in test_keys_gt]\n","len(normals_short)\n","\n","already_saved = os.listdir(os.path.join(RESULTS_DIR, 'Probability_maps', str(12)))\n","len(already_saved)\n","\n","\n","for key in tqdm_notebook(test_keys):\n","  if key not in already_saved:\n","    create_probability_map(key, 12)"],"metadata":{"id":"wcCIRTh7Lh9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import roi_cc_project_3 as roi_cc_project\n","from fastprogress import progress_bar\n","import cv2 \n","import pandas as pd\n","from tqdm.notebook import tqdm_notebook\n","\n","for key in tqdm_notebook(test_keys):\n","  if key not in already_saved:\n","    create_probability_map(key, 24)\n","\n","\n","fn, dfROC, filename_key = calculateFROC(test_keys, normals_short, 12, 'CNN_final')\n","\n","draw_curve(fn, len(normals_short), dfROC, filename_key.split('.')[0])\n"],"metadata":{"id":"WXnsUV7SLmHN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generation of average probability Map\n","\n","This is an example using a roi_size of 24 and 48 since that combination gave the best results in our experiments."],"metadata":{"id":"ejjrdsAfTvd3"}},{"cell_type":"code","source":["_, _, probs24 = next(os.walk(os.path.join(PROB_MAPS_DIR,'24')))\n","_, _, probs48 = next(os.walk(os.path.join(PROB_MAPS_DIR,'48')))\n","\n","probs24.sort()\n","probs48.sort()\n","\n","count = 0\n","\n","for  prob24, prob48 in zip(tqdm_notebook(probs24), probs48):\n","    \n","    if (('.tif.csv' in prob24) and ('.tif.csv' in prob48)):\n","        continue\n","        \n","    else:\n","        img24 = cv2.imread(os.path.join(PROB_MAPS_DIR, '24', prob24), cv2.IMREAD_UNCHANGED)\n","        img48 = cv2.imread(os.path.join(PROB_MAPS_DIR, '48', prob48), cv2.IMREAD_UNCHANGED)\n","\n","        img = (img24+img48)*0.5\n","\n","        if not cv2.imwrite(os.path.join(PROB_MAPS_DIR, '24-48', prob24 ), img):\n","            print('fail write')\n","    \n","        del img24, img48, img\n","        \n","    \n","    "],"metadata":{"id":"a3tiO5TCS5kt"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"DeepLearningMCD","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}