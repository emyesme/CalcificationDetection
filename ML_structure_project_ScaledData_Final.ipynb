{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emyesme/CalcificationDetection/blob/feature-pm/ML_structure_project_ScaledData_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wthu5HssWsF3"
      },
      "source": [
        "\n",
        "# Load the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dd477p2erqy",
        "outputId": "dcd76885-0afd-48c0-b0c0-700ecbd31793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDRr9M44sYxK",
        "outputId": "825e8c42-af23-4c4a-acd9-b411bca0e388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['20586934_6c613a14b80a8591_MG_L_CC_ANON__haar_fos_glcm.csv', '20586960_6c613a14b80a8591_MG_R_ML_ANON__haar_fos_glcm.csv', '20586986_6c613a14b80a8591_MG_L_ML_ANON__haar_fos_glcm.csv', '20587054_b6a4f750c6df4f90_MG_R_CC_ANON__haar_fos_glcm.csv', '20587080_b6a4f750c6df4f90_MG_R_ML_ANON__haar_fos_glcm.csv', '20587148_fd746d25eb40b3dc_MG_R_CC_ANON__haar_fos_glcm.csv', '20587174_fd746d25eb40b3dc_MG_L_CC_ANON__haar_fos_glcm.csv', '20587200_fd746d25eb40b3dc_MG_R_ML_ANON__haar_fos_glcm.csv', '20587226_fd746d25eb40b3dc_MG_L_ML_ANON__haar_fos_glcm.csv', '20587294_e634830794f5c1bd_MG_R_CC_ANON__haar_fos_glcm.csv', '20587320_e634830794f5c1bd_MG_L_CC_ANON__haar_fos_glcm.csv', '20587346_e634830794f5c1bd_MG_R_ML_ANON__haar_fos_glcm.csv', '20587372_e634830794f5c1bd_MG_L_ML_ANON__haar_fos_glcm.csv', '20587466_d571b5880ad2a016_MG_L_CC_ANON__haar_fos_glcm.csv', '20587492_d571b5880ad2a016_MG_R_ML_ANON__haar_fos_glcm.csv', '20587518_d571b5880ad2a016_MG_L_ML_ANON__haar_fos_glcm.csv', '20587544_d571b5880ad2a016_MG_R_CC_ANON__haar_fos_glcm.csv', '20587612_f4b2d377f43ba0bd_MG_R_CC_ANON__haar_fos_glcm.csv', '20587638_f4b2d377f43ba0bd_MG_L_CC_ANON__haar_fos_glcm.csv', '20587664_f4b2d377f43ba0bd_MG_R_ML_ANON__haar_fos_glcm.csv', '20587690_f4b2d377f43ba0bd_MG_L_ML_ANON__haar_fos_glcm.csv', '20587758_81cd83d2f4d78528_MG_L_CC_ANON__haar_fos_glcm.csv', '20587784_81cd83d2f4d78528_MG_R_ML_ANON__haar_fos_glcm.csv', '20587810_81cd83d2f4d78528_MG_L_ML_ANON__haar_fos_glcm.csv', '20587836_81cd83d2f4d78528_MG_R_CC_ANON__haar_fos_glcm.csv', '20587902_8dbbd4e51f549ff0_MG_R_CC_ANON__haar_fos_glcm.csv', '20587928_8dbbd4e51f549ff0_MG_R_ML_ANON__haar_fos_glcm.csv', '20587994_024ee3569b2605dc_MG_R_CC_ANON__haar_fos_glcm.csv', '20588020_024ee3569b2605dc_MG_L_CC_ANON__haar_fos_glcm.csv', '20588046_024ee3569b2605dc_MG_R_ML_ANON__haar_fos_glcm.csv', '20588072_024ee3569b2605dc_MG_L_ML_ANON__haar_fos_glcm.csv', '20588138_8d0b9620c53c0268_MG_R_ML_ANON__haar_fos_glcm.csv', '20588164_8d0b9620c53c0268_MG_R_CC_ANON__haar_fos_glcm.csv', '20588190_8d0b9620c53c0268_MG_L_CC_ANON__haar_fos_glcm.csv', '20588216_8d0b9620c53c0268_MG_L_ML_ANON__haar_fos_glcm.csv', '20588308_493155e17143edef_MG_L_ML_ANON__haar_fos_glcm.csv', '20588334_493155e17143edef_MG_L_CC_ANON__haar_fos_glcm.csv', '20588458_bf1a6aaadb05e3df_MG_R_CC_ANON__haar_fos_glcm.csv', '20588510_bf1a6aaadb05e3df_MG_R_ML_ANON__haar_fos_glcm.csv', '20588536_bf1a6aaadb05e3df_MG_L_ML_ANON__haar_fos_glcm.csv', '20588562_bf1a6aaadb05e3df_MG_L_CC_ANON__haar_fos_glcm.csv', '20588654_036aff49b8ac84f0_MG_R_ML_ANON__haar_fos_glcm.csv', '20588680_036aff49b8ac84f0_MG_L_ML_ANON__haar_fos_glcm.csv', '22427682_d713ef5849f98b6c_MG_R_CC_ANON__haar_fos_glcm.csv', '22427705_d713ef5849f98b6c_MG_L_CC_ANON__haar_fos_glcm.csv', '22427728_d713ef5849f98b6c_MG_R_ML_ANON__haar_fos_glcm.csv', '22427751_d713ef5849f98b6c_MG_L_ML_ANON__haar_fos_glcm.csv', '22427840_bbd6a3a35438c11b_MG_R_CC_ANON__haar_fos_glcm.csv', '22427864_bbd6a3a35438c11b_MG_L_CC_ANON__haar_fos_glcm.csv', '22579730_bbd6a3a35438c11b_MG_R_ML_ANON__haar_fos_glcm.csv', '22579754_bbd6a3a35438c11b_MG_L_ML_ANON__haar_fos_glcm.csv', '22579847_301f1776aebbf5d2_MG_R_CC_ANON__haar_fos_glcm.csv', '22579870_301f1776aebbf5d2_MG_L_CC_ANON__haar_fos_glcm.csv', '22579893_301f1776aebbf5d2_MG_R_ML_ANON__haar_fos_glcm.csv', '22579916_301f1776aebbf5d2_MG_L_ML_ANON__haar_fos_glcm.csv', '22580015_6200187f3f1ccc18_MG_R_CC_ANON__haar_fos_glcm.csv', '22580038_6200187f3f1ccc18_MG_L_CC_ANON__haar_fos_glcm.csv', '22580068_6200187f3f1ccc18_MG_R_ML_ANON__haar_fos_glcm.csv', '22580098_6200187f3f1ccc18_MG_L_ML_ANON__haar_fos_glcm.csv', '22580192_5530d5782fc89dd7_MG_R_CC_ANON__haar_fos_glcm.csv', '22580218_5530d5782fc89dd7_MG_L_CC_ANON__haar_fos_glcm.csv', '22580244_5530d5782fc89dd7_MG_R_ML_ANON__haar_fos_glcm.csv', '22580270_5530d5782fc89dd7_MG_L_ML_ANON__haar_fos_glcm.csv', '22580341_5eae9beae14d26fd_MG_R_CC_ANON__haar_fos_glcm.csv', '22580367_5eae9beae14d26fd_MG_L_CC_ANON__haar_fos_glcm.csv', '22580393_5eae9beae14d26fd_MG_R_ML_ANON__haar_fos_glcm.csv', '22580419_5eae9beae14d26fd_MG_L_ML_ANON__haar_fos_glcm.csv', '22580492_2a5b932da4ce5ca1_MG_R_CC_ANON__haar_fos_glcm.csv', '22580520_2a5b932da4ce5ca1_MG_L_CC_ANON__haar_fos_glcm.csv', '22580548_2a5b932da4ce5ca1_MG_R_ML_ANON__haar_fos_glcm.csv', '22580576_2a5b932da4ce5ca1_MG_L_ML_ANON__haar_fos_glcm.csv', '22580654_fe7d005dcbbfb46d_MG_R_CC_ANON__haar_fos_glcm.csv', '22580680_fe7d005dcbbfb46d_MG_L_CC_ANON__haar_fos_glcm.csv', '22580706_fe7d005dcbbfb46d_MG_R_ML_ANON__haar_fos_glcm.csv', '22580732_fe7d005dcbbfb46d_MG_L_ML_ANON__haar_fos_glcm.csv', '22613624_dcafa6ba6374ec07_MG_R_CC_ANON__haar_fos_glcm.csv', '22613650_dcafa6ba6374ec07_MG_L_CC_ANON__haar_fos_glcm.csv', '22613676_dcafa6ba6374ec07_MG_R_ML_ANON__haar_fos_glcm.csv', '22613702_dcafa6ba6374ec07_MG_L_ML_ANON__haar_fos_glcm.csv', '22613770_45c7f44839fd9e68_MG_R_CC_ANON__haar_fos_glcm.csv', '22613796_45c7f44839fd9e68_MG_L_CC_ANON__haar_fos_glcm.csv', '22613822_45c7f44839fd9e68_MG_R_ML_ANON__haar_fos_glcm.csv', '22613848_45c7f44839fd9e68_MG_L_ML_ANON__haar_fos_glcm.csv', '22613918_f23fa352e7de3dc7_MG_R_CC_ANON__haar_fos_glcm.csv', '22613944_f23fa352e7de3dc7_MG_L_CC_ANON__haar_fos_glcm.csv', '22613970_f23fa352e7de3dc7_MG_R_ML_ANON__haar_fos_glcm.csv', '22613996_f23fa352e7de3dc7_MG_L_ML_ANON__haar_fos_glcm.csv', '22614074_6bd24a0a42c19ce1_MG_R_CC_ANON__haar_fos_glcm.csv', '20586908_6c613a14b80a8591_MG_R_CC_ANON__haar_fos_glcm.csv', '22614097_6bd24a0a42c19ce1_MG_L_CC_ANON__haar_fos_glcm.csv', '22614127_6bd24a0a42c19ce1_MG_R_ML_ANON__haar_fos_glcm.csv', '22614150_6bd24a0a42c19ce1_MG_L_ML_ANON__haar_fos_glcm.csv', '22614236_1e5c3af078f74b05_MG_L_CC_ANON__haar_fos_glcm.csv', '22614266_1e5c3af078f74b05_MG_L_ML_ANON__haar_fos_glcm.csv', '22614353_d065adcb9905b973_MG_R_CC_ANON__haar_fos_glcm.csv', '22614379_d065adcb9905b973_MG_L_CC_ANON__haar_fos_glcm.csv', '22614405_d065adcb9905b973_MG_R_ML_ANON__haar_fos_glcm.csv', '22614431_d065adcb9905b973_MG_L_ML_ANON__haar_fos_glcm.csv', '22614499_2dec4948fbe6336d_MG_R_CC_ANON__haar_fos_glcm.csv', '22614522_2dec4948fbe6336d_MG_L_CC_ANON__haar_fos_glcm.csv', '22614545_2dec4948fbe6336d_MG_R_ML_ANON__haar_fos_glcm.csv', '22614568_2dec4948fbe6336d_MG_L_ML_ANON__haar_fos_glcm.csv', '22670094_e1f51192f7bf3f5f_MG_R_CC_ANON__haar_fos_glcm.csv', '22670124_e1f51192f7bf3f5f_MG_L_CC_ANON__haar_fos_glcm.csv', '22670147_e1f51192f7bf3f5f_MG_R_ML_ANON__haar_fos_glcm.csv', '22670177_e1f51192f7bf3f5f_MG_L_ML_ANON__haar_fos_glcm.csv', '22670278_98429c0bdf78c0c7_MG_R_CC_ANON__haar_fos_glcm.csv', '22670301_98429c0bdf78c0c7_MG_L_CC_ANON__haar_fos_glcm.csv', '22670324_98429c0bdf78c0c7_MG_R_ML_ANON__haar_fos_glcm.csv', '22670347_98429c0bdf78c0c7_MG_L_ML_ANON__haar_fos_glcm.csv', '22670442_7e677f3d530e41ed_MG_R_CC_ANON__haar_fos_glcm.csv', '22670465_7e677f3d530e41ed_MG_L_CC_ANON__haar_fos_glcm.csv', '22670488_7e677f3d530e41ed_MG_R_ML_ANON__haar_fos_glcm.csv', '22670511_7e677f3d530e41ed_MG_L_ML_ANON__haar_fos_glcm.csv', '22670620_e15a16f87b4f9782_MG_R_CC_ANON__haar_fos_glcm.csv', '22670643_e15a16f87b4f9782_MG_L_CC_ANON__haar_fos_glcm.csv', '22670673_e15a16f87b4f9782_MG_R_ML_ANON__haar_fos_glcm.csv', '22670703_e15a16f87b4f9782_MG_L_ML_ANON__haar_fos_glcm.csv', '22670809_0b7396cdccacca82_MG_R_CC_ANON__haar_fos_glcm.csv', '22670832_0b7396cdccacca82_MG_L_CC_ANON__haar_fos_glcm.csv', '22670855_0b7396cdccacca82_MG_R_ML_ANON__haar_fos_glcm.csv', '22670878_0b7396cdccacca82_MG_L_ML_ANON__haar_fos_glcm.csv', '22670978_f571fd4e63c718e3_MG_L_CC_ANON__haar_fos_glcm.csv', '22671003_f571fd4e63c718e3_MG_L_ML_ANON__haar_fos_glcm.csv', '22678449_60995d51033e24b8_MG_R_CC_ANON__haar_fos_glcm.csv', '22678472_60995d51033e24b8_MG_L_CC_ANON__haar_fos_glcm.csv', '22678495_60995d51033e24b8_MG_R_ML_ANON__haar_fos_glcm.csv', '22678518_60995d51033e24b8_MG_L_ML_ANON__haar_fos_glcm.csv', '22678622_61b13c59bcba149e_MG_R_CC_ANON__haar_fos_glcm.csv', '22678646_61b13c59bcba149e_MG_L_CC_ANON__haar_fos_glcm.csv', '22678670_61b13c59bcba149e_MG_R_ML_ANON__haar_fos_glcm.csv', '22678694_61b13c59bcba149e_MG_L_ML_ANON__haar_fos_glcm.csv', '22678787_64a22c47765f0c5c_MG_R_CC_ANON__haar_fos_glcm.csv', '22678810_64a22c47765f0c5c_MG_L_CC_ANON__haar_fos_glcm.csv', '22678833_64a22c47765f0c5c_MG_R_ML_ANON__haar_fos_glcm.csv', '22678856_64a22c47765f0c5c_MG_L_ML_ANON__haar_fos_glcm.csv', '22678953_b9a4da5f2dae63a9_MG_R_CC_ANON__haar_fos_glcm.csv', '22678980_b9a4da5f2dae63a9_MG_L_CC_ANON__haar_fos_glcm.csv', '22679008_b9a4da5f2dae63a9_MG_R_ML_ANON__haar_fos_glcm.csv', '22679036_b9a4da5f2dae63a9_MG_L_ML_ANON__haar_fos_glcm.csv', '24054997_2f1104b3cda7f145_MG_L_ML_ANON__haar_fos_glcm.csv', '24055024_2f1104b3cda7f145_MG_R_ML_ANON__haar_fos_glcm.csv', '24055051_2f1104b3cda7f145_MG_L_CC_ANON__haar_fos_glcm.csv', '24055078_2f1104b3cda7f145_MG_R_CC_ANON__haar_fos_glcm.csv', '24055149_606e9b184978a350_MG_L_ML_ANON__haar_fos_glcm.csv', '24055176_606e9b184978a350_MG_R_ML_ANON__haar_fos_glcm.csv', '24055203_606e9b184978a350_MG_L_CC_ANON__haar_fos_glcm.csv', '24055274_1e10aef17c9fe149_MG_L_ML_ANON__haar_fos_glcm.csv', '24055328_1e10aef17c9fe149_MG_R_ML_ANON__haar_fos_glcm.csv', '24055355_1e10aef17c9fe149_MG_L_CC_ANON__haar_fos_glcm.csv', '24055382_1e10aef17c9fe149_MG_R_CC_ANON__haar_fos_glcm.csv', '24055445_ac3185e18ffdc7b6_MG_L_ML_ANON__haar_fos_glcm.csv', '24055464_ac3185e18ffdc7b6_MG_R_ML_ANON__haar_fos_glcm.csv', '24055483_ac3185e18ffdc7b6_MG_L_CC_ANON__haar_fos_glcm.csv', '24055502_ac3185e18ffdc7b6_MG_R_CC_ANON__haar_fos_glcm.csv', '24055573_6f1aef40b3775182_MG_L_ML_ANON__haar_fos_glcm.csv', '24055600_6f1aef40b3775182_MG_R_ML_ANON__haar_fos_glcm.csv', '24055627_6f1aef40b3775182_MG_L_CC_ANON__haar_fos_glcm.csv', '24055654_6f1aef40b3775182_MG_R_CC_ANON__haar_fos_glcm.csv', '24055725_f0f1a133837b5137_MG_L_ML_ANON__haar_fos_glcm.csv', '24055752_f0f1a133837b5137_MG_R_ML_ANON__haar_fos_glcm.csv', '24055779_f0f1a133837b5137_MG_L_CC_ANON__haar_fos_glcm.csv', '24055806_f0f1a133837b5137_MG_R_CC_ANON__haar_fos_glcm.csv', '24055877_839819f2eadaf325_MG_L_ML_ANON__haar_fos_glcm.csv', '24055904_839819f2eadaf325_MG_R_ML_ANON__haar_fos_glcm.csv', '24055931_839819f2eadaf325_MG_L_CC_ANON__haar_fos_glcm.csv', '24055958_839819f2eadaf325_MG_R_CC_ANON__haar_fos_glcm.csv', '24058660_9e8db9e34d5275ef_MG_R_CC_ANON__haar_fos_glcm.csv', '24058686_9e8db9e34d5275ef_MG_L_CC_ANON__haar_fos_glcm.csv', '24058712_9e8db9e34d5275ef_MG_R_ML_ANON__haar_fos_glcm.csv', '24058738_9e8db9e34d5275ef_MG_L_ML_ANON__haar_fos_glcm.csv', '24065251_c4b995eddb3c510c_MG_L_ML_ANON__haar_fos_glcm.csv', '24065270_c4b995eddb3c510c_MG_R_ML_ANON__haar_fos_glcm.csv', '24065289_c4b995eddb3c510c_MG_L_CC_ANON__haar_fos_glcm.csv', '24065308_c4b995eddb3c510c_MG_R_CC_ANON__haar_fos_glcm.csv', '24065380_83db89f57aea498a_MG_L_ML_ANON__haar_fos_glcm.csv', '24065407_83db89f57aea498a_MG_R_ML_ANON__haar_fos_glcm.csv', '24065434_83db89f57aea498a_MG_L_CC_ANON__haar_fos_glcm.csv', '24065461_83db89f57aea498a_MG_R_CC_ANON__haar_fos_glcm.csv', '24065530_d8205a09c8173f44_MG_L_ML_ANON__haar_fos_glcm.csv', '24065557_d8205a09c8173f44_MG_R_ML_ANON__haar_fos_glcm.csv', '24065584_d8205a09c8173f44_MG_L_CC_ANON__haar_fos_glcm.csv', '24065611_d8205a09c8173f44_MG_R_CC_ANON__haar_fos_glcm.csv', '24065680_5291e1aee2bbf5df_MG_L_ML_ANON__haar_fos_glcm.csv', '24065707_5291e1aee2bbf5df_MG_R_ML_ANON__haar_fos_glcm.csv', '24065734_5291e1aee2bbf5df_MG_L_CC_ANON__haar_fos_glcm.csv', '24065761_5291e1aee2bbf5df_MG_R_CC_ANON__haar_fos_glcm.csv', '24065833_c01f83a1eb283270_MG_L_ML_ANON__haar_fos_glcm.csv', '24065860_c01f83a1eb283270_MG_R_ML_ANON__haar_fos_glcm.csv', '24065887_c01f83a1eb283270_MG_L_CC_ANON__haar_fos_glcm.csv', '24065914_c01f83a1eb283270_MG_R_CC_ANON__haar_fos_glcm.csv', '26933772_f8bfddc28e8045c0_MG_R_CC_ANON__haar_fos_glcm.csv', '26933801_f8bfddc28e8045c0_MG_L_CC_ANON__haar_fos_glcm.csv', '26933830_f8bfddc28e8045c0_MG_R_ML_ANON__haar_fos_glcm.csv', '26933859_f8bfddc28e8045c0_MG_L_ML_ANON__haar_fos_glcm.csv', '27829134_fbb55bf7fff48540_MG_R_CC_ANON__haar_fos_glcm.csv', '27829161_fbb55bf7fff48540_MG_L_CC_ANON__haar_fos_glcm.csv', '27829188_fbb55bf7fff48540_MG_R_ML_ANON__haar_fos_glcm.csv', '27829215_fbb55bf7fff48540_MG_L_ML_ANON__haar_fos_glcm.csv', '30011484_349323117bf0fd93_MG_R_CC_ANON__haar_fos_glcm.csv', '30011507_349323117bf0fd93_MG_L_CC_ANON__haar_fos_glcm.csv', '30011530_349323117bf0fd93_MG_R_ML_ANON__haar_fos_glcm.csv', '30011553_349323117bf0fd93_MG_L_ML_ANON__haar_fos_glcm.csv', '30011647_6968748e66837bc7_MG_R_CC_ANON__haar_fos_glcm.csv', '30011674_6968748e66837bc7_MG_L_CC_ANON__haar_fos_glcm.csv', '30011700_6968748e66837bc7_MG_R_ML_ANON__haar_fos_glcm.csv', '30011727_6968748e66837bc7_MG_L_ML_ANON__haar_fos_glcm.csv', '30011798_4f20c1285d8f0b1f_MG_R_CC_ANON__haar_fos_glcm.csv', '30011824_4f20c1285d8f0b1f_MG_L_CC_ANON__haar_fos_glcm.csv', '30011850_4f20c1285d8f0b1f_MG_R_ML_ANON__haar_fos_glcm.csv', '30318067_4f20c1285d8f0b1f_MG_L_ML_ANON__haar_fos_glcm.csv', '50993399_5d85ecc9cf26b254_MG_L_ML_ANON__haar_fos_glcm.csv', '50993426_5d85ecc9cf26b254_MG_L_CC_ANON__haar_fos_glcm.csv', '50993616_b03f1dd34eb3c55f_MG_L_ML_ANON__haar_fos_glcm.csv', '50993643_b03f1dd34eb3c55f_MG_L_CC_ANON__haar_fos_glcm.csv', '50993670_b03f1dd34eb3c55f_MG_L_ML_ANON__haar_fos_glcm.csv', '50993697_b03f1dd34eb3c55f_MG_L_CC_ANON__haar_fos_glcm.csv', '50993787_de5e8d61e501a71b_MG_L_ML_ANON__haar_fos_glcm.csv', '50993814_de5e8d61e501a71b_MG_R_ML_ANON__haar_fos_glcm.csv', '50993841_de5e8d61e501a71b_MG_L_CC_ANON__haar_fos_glcm.csv', '50993868_de5e8d61e501a71b_MG_R_CC_ANON__haar_fos_glcm.csv', '50993895_de5e8d61e501a71b_MG_L_ML_ANON__haar_fos_glcm.csv', '50993922_de5e8d61e501a71b_MG_R_ML_ANON__haar_fos_glcm.csv', '50993949_de5e8d61e501a71b_MG_L_CC_ANON__haar_fos_glcm.csv', '50993976_de5e8d61e501a71b_MG_R_CC_ANON__haar_fos_glcm.csv', '50994110_cc9e66c5b31baab8_MG_L_ML_ANON__haar_fos_glcm.csv', '50994137_cc9e66c5b31baab8_MG_R_ML_ANON__haar_fos_glcm.csv', '50994164_cc9e66c5b31baab8_MG_L_CC_ANON__haar_fos_glcm.csv', '50994191_cc9e66c5b31baab8_MG_R_CC_ANON__haar_fos_glcm.csv', '50994273_cc9e66c5b31baab8_MG_R_CC_ANON__haar_fos_glcm.csv', '50994300_cc9e66c5b31baab8_MG_R_FB_ANON__haar_fos_glcm.csv', '50994327_cc9e66c5b31baab8_MG_L_ML_ANON__haar_fos_glcm.csv', '50994354_cc9e66c5b31baab8_MG_R_ML_ANON__haar_fos_glcm.csv', '50994381_cc9e66c5b31baab8_MG_L_CC_ANON__haar_fos_glcm.csv', '50994408_cc9e66c5b31baab8_MG_R_CC_ANON__haar_fos_glcm.csv', '50994535_de4c34099d6ef8de_MG_R_ML_ANON__haar_fos_glcm.csv', '50994562_de4c34099d6ef8de_MG_R_CC_ANON__haar_fos_glcm.csv', '50994589_de4c34099d6ef8de_MG_R_ML_ANON__haar_fos_glcm.csv', '50994616_de4c34099d6ef8de_MG_R_CC_ANON__haar_fos_glcm.csv', '50994706_069212ec65a94339_MG_R_CC_ANON__haar_fos_glcm.csv', '50994733_069212ec65a94339_MG_L_ML_ANON__haar_fos_glcm.csv', '50994760_069212ec65a94339_MG_R_ML_ANON__haar_fos_glcm.csv', '50994787_069212ec65a94339_MG_L_CC_ANON__haar_fos_glcm.csv', '50994814_069212ec65a94339_MG_L_ML_ANON__haar_fos_glcm.csv', '50994841_069212ec65a94339_MG_R_ML_ANON__haar_fos_glcm.csv', '50994868_069212ec65a94339_MG_L_CC_ANON__haar_fos_glcm.csv', '50994895_069212ec65a94339_MG_R_CC_ANON__haar_fos_glcm.csv', '50995762_0c735e8768d276b4_MG_R_ML_ANON__haar_fos_glcm.csv', '50995789_0c735e8768d276b4_MG_R_CC_ANON__haar_fos_glcm.csv', '50995872_c94d8a1ebd452afe_MG_L_ML_ANON__haar_fos_glcm.csv', '50995899_c94d8a1ebd452afe_MG_L_CC_ANON__haar_fos_glcm.csv', '50995963_d742ec2f9b90aa62_MG_L_ML_ANON__haar_fos_glcm.csv', '50995990_d742ec2f9b90aa62_MG_L_CC_ANON__haar_fos_glcm.csv', '50996056_71c1a60d57c5322f_MG_L_ML_ANON__haar_fos_glcm.csv', '50996083_71c1a60d57c5322f_MG_R_ML_ANON__haar_fos_glcm.csv', '50996110_71c1a60d57c5322f_MG_L_CC_ANON__haar_fos_glcm.csv', '50996137_71c1a60d57c5322f_MG_R_CC_ANON__haar_fos_glcm.csv', '50996201_8c1b2bd64ca4d778_MG_L_ML_ANON__haar_fos_glcm.csv', '50996228_8c1b2bd64ca4d778_MG_L_CC_ANON__haar_fos_glcm.csv', '50996325_6aba0b402889a16f_MG_L_ML_ANON__haar_fos_glcm.csv', '50996352_6aba0b402889a16f_MG_R_ML_ANON__haar_fos_glcm.csv', '50996379_6aba0b402889a16f_MG_L_CC_ANON__haar_fos_glcm.csv', '50996406_6aba0b402889a16f_MG_R_CC_ANON__haar_fos_glcm.csv', '50996709_330e5fe16929eed4_MG_R_ML_ANON__haar_fos_glcm.csv', '50996736_330e5fe16929eed4_MG_R_CC_ANON__haar_fos_glcm.csv', '50996800_fdf4a1516f88b280_MG_L_ML_ANON__haar_fos_glcm.csv', '50996827_fdf4a1516f88b280_MG_R_ML_ANON__haar_fos_glcm.csv', '50996854_fdf4a1516f88b280_MG_L_CC_ANON__haar_fos_glcm.csv', '50996881_fdf4a1516f88b280_MG_R_CC_ANON__haar_fos_glcm.csv', '50996945_ce5e5e18a261cd29_MG_L_ML_ANON__haar_fos_glcm.csv', '50996972_ce5e5e18a261cd29_MG_R_ML_ANON__haar_fos_glcm.csv', '50996999_ce5e5e18a261cd29_MG_L_CC_ANON__haar_fos_glcm.csv', '50997026_ce5e5e18a261cd29_MG_R_CC_ANON__haar_fos_glcm.csv', '50997053_ce5e5e18a261cd29_MG_L_CC_ANON__haar_fos_glcm.csv', '50997080_ce5e5e18a261cd29_MG_L_ML_ANON__haar_fos_glcm.csv', '50997107_ce5e5e18a261cd29_MG_R_ML_ANON__haar_fos_glcm.csv', '50997134_ce5e5e18a261cd29_MG_R_CC_ANON__haar_fos_glcm.csv', '50997223_9054942f7be52dd9_MG_L_ML_ANON__haar_fos_glcm.csv', '50997250_9054942f7be52dd9_MG_R_ML_ANON__haar_fos_glcm.csv', '50997277_9054942f7be52dd9_MG_L_CC_ANON__haar_fos_glcm.csv', '50997304_9054942f7be52dd9_MG_R_CC_ANON__haar_fos_glcm.csv', '50997434_97ec8cadfca70d32_MG_L_ML_ANON__haar_fos_glcm.csv', '50997461_97ec8cadfca70d32_MG_R_ML_ANON__haar_fos_glcm.csv', '50997488_97ec8cadfca70d32_MG_L_CC_ANON__haar_fos_glcm.csv', '50997515_97ec8cadfca70d32_MG_R_CC_ANON__haar_fos_glcm.csv', '50997597_67cc8c9939d74a9a_MG_L_ML_ANON__haar_fos_glcm.csv', '50997624_67cc8c9939d74a9a_MG_R_ML_ANON__haar_fos_glcm.csv', '50997651_67cc8c9939d74a9a_MG_L_CC_ANON__haar_fos_glcm.csv', '50997678_67cc8c9939d74a9a_MG_R_CC_ANON__haar_fos_glcm.csv', '50997742_cbb6c98a81e69eeb_MG_L_ML_ANON__haar_fos_glcm.csv', '50997769_cbb6c98a81e69eeb_MG_R_ML_ANON__haar_fos_glcm.csv', '50997796_cbb6c98a81e69eeb_MG_L_CC_ANON__haar_fos_glcm.csv', '50997823_cbb6c98a81e69eeb_MG_R_CC_ANON__haar_fos_glcm.csv', '50998032_66adfbb4f19c76d2_MG_R_CC_ANON__haar_fos_glcm.csv', '50998059_66adfbb4f19c76d2_MG_L_ML_ANON__haar_fos_glcm.csv', '50998086_66adfbb4f19c76d2_MG_R_ML_ANON__haar_fos_glcm.csv', '50998113_66adfbb4f19c76d2_MG_L_CC_ANON__haar_fos_glcm.csv', '50998177_f34ee0ab6591b792_MG_L_ML_ANON__haar_fos_glcm.csv', '50998204_f34ee0ab6591b792_MG_R_ML_ANON__haar_fos_glcm.csv', '50998231_f34ee0ab6591b792_MG_L_CC_ANON__haar_fos_glcm.csv', '50998258_f34ee0ab6591b792_MG_R_CC_ANON__haar_fos_glcm.csv', '50998322_1e4b534393d18753_MG_R_ML_ANON__haar_fos_glcm.csv', '50998349_1e4b534393d18753_MG_R_CC_ANON__haar_fos_glcm.csv', '50998413_1f139436acfc5467_MG_L_ML_ANON__haar_fos_glcm.csv', '50998440_1f139436acfc5467_MG_R_ML_ANON__haar_fos_glcm.csv', '50998467_1f139436acfc5467_MG_L_CC_ANON__haar_fos_glcm.csv', '50998494_1f139436acfc5467_MG_R_CC_ANON__haar_fos_glcm.csv', '50998580_cd12bc20b3d27d0b_MG_L_ML_ANON__haar_fos_glcm.csv', '50998607_cd12bc20b3d27d0b_MG_R_ML_ANON__haar_fos_glcm.csv', '50998634_cd12bc20b3d27d0b_MG_L_CC_ANON__haar_fos_glcm.csv', '50998661_cd12bc20b3d27d0b_MG_R_CC_ANON__haar_fos_glcm.csv', '50998981_a78eba834ef6ee88_MG_R_ML_ANON__haar_fos_glcm.csv', '50999008_a78eba834ef6ee88_MG_R_CC_ANON__haar_fos_glcm.csv', '50999094_cb65e8dac169f596_MG_L_ML_ANON__haar_fos_glcm.csv', '50999121_cb65e8dac169f596_MG_R_ML_ANON__haar_fos_glcm.csv', '50999148_cb65e8dac169f596_MG_L_CC_ANON__haar_fos_glcm.csv', '50999175_cb65e8dac169f596_MG_R_CC_ANON__haar_fos_glcm.csv', '50999246_cb65e8dac169f596_MG_L_ML_ANON__haar_fos_glcm.csv', '50999273_cb65e8dac169f596_MG_R_ML_ANON__haar_fos_glcm.csv', '50999300_cb65e8dac169f596_MG_L_CC_ANON__haar_fos_glcm.csv', '50999327_cb65e8dac169f596_MG_R_CC_ANON__haar_fos_glcm.csv', '50999432_f62fbf38fb208316_MG_L_ML_ANON__haar_fos_glcm.csv', '50999459_f62fbf38fb208316_MG_L_CC_ANON__haar_fos_glcm.csv', '51048738_3f22cdda8da215e3_MG_R_ML_ANON__haar_fos_glcm.csv', '51048765_3f22cdda8da215e3_MG_R_CC_ANON__haar_fos_glcm.csv', '51048891_f3e93e889a7746f0_MG_L_ML_ANON__haar_fos_glcm.csv', '51048918_f3e93e889a7746f0_MG_R_ML_ANON__haar_fos_glcm.csv', '51048945_f3e93e889a7746f0_MG_L_CC_ANON__haar_fos_glcm.csv', '51048972_f3e93e889a7746f0_MG_R_CC_ANON__haar_fos_glcm.csv', '51049053_8c105bb715bf1c3c_MG_L_ML_ANON__haar_fos_glcm.csv', '51049080_8c105bb715bf1c3c_MG_R_ML_ANON__haar_fos_glcm.csv', '51049107_8c105bb715bf1c3c_MG_L_CC_ANON__haar_fos_glcm.csv', '51049134_8c105bb715bf1c3c_MG_R_CC_ANON__haar_fos_glcm.csv', '51049249_832ebce700241036_MG_L_CC_ANON__haar_fos_glcm.csv', '51049276_832ebce700241036_MG_L_ML_ANON__haar_fos_glcm.csv', '51049462_6f64793857feb5d0_MG_L_ML_ANON__haar_fos_glcm.csv', '51049489_6f64793857feb5d0_MG_R_ML_ANON__haar_fos_glcm.csv', '51049516_6f64793857feb5d0_MG_L_CC_ANON__haar_fos_glcm.csv', '51049543_6f64793857feb5d0_MG_R_CC_ANON__haar_fos_glcm.csv', '51049628_6f64793857feb5d0_MG_L_ML_ANON__haar_fos_glcm.csv', '51049655_6f64793857feb5d0_MG_R_ML_ANON__haar_fos_glcm.csv', '51049682_6f64793857feb5d0_MG_L_CC_ANON__haar_fos_glcm.csv', '51070197_6f64793857feb5d0_MG_R_CC_ANON__haar_fos_glcm.csv', '53580611_40e22f2e3215b954_MG_L_ML_ANON__haar_fos_glcm.csv', '53580638_40e22f2e3215b954_MG_R_ML_ANON__haar_fos_glcm.csv', '53580665_40e22f2e3215b954_MG_L_CC_ANON__haar_fos_glcm.csv', '53580692_40e22f2e3215b954_MG_R_CC_ANON__haar_fos_glcm.csv', '53580804_51bec6477a7898b9_MG_L_ML_ANON__haar_fos_glcm.csv', '53580831_51bec6477a7898b9_MG_R_ML_ANON__haar_fos_glcm.csv', '53580858_51bec6477a7898b9_MG_L_CC_ANON__haar_fos_glcm.csv', '53580885_51bec6477a7898b9_MG_R_CC_ANON__haar_fos_glcm.csv', '53580979_4c341dad22471922_MG_L_ML_ANON__haar_fos_glcm.csv', '53581006_4c341dad22471922_MG_R_ML_ANON__haar_fos_glcm.csv', '53581033_4c341dad22471922_MG_L_CC_ANON__haar_fos_glcm.csv', '53581060_4c341dad22471922_MG_R_CC_ANON__haar_fos_glcm.csv', '53581124_3be876aecfaad4ca_MG_L_ML_ANON__haar_fos_glcm.csv', '53581151_3be876aecfaad4ca_MG_L_CC_ANON__haar_fos_glcm.csv', '53581237_80123a24997098dc_MG_R_ML_ANON__haar_fos_glcm.csv', '53581264_80123a24997098dc_MG_R_CC_ANON__haar_fos_glcm.csv', '53581379_b231a8ba4dd4214f_MG_L_ML_ANON__haar_fos_glcm.csv', '53581406_b231a8ba4dd4214f_MG_R_ML_ANON__haar_fos_glcm.csv', '53581433_b231a8ba4dd4214f_MG_L_CC_ANON__haar_fos_glcm.csv', '53581460_b231a8ba4dd4214f_MG_R_CC_ANON__haar_fos_glcm.csv', '53581769_573747ee33ef6e5a_MG_L_ML_ANON__haar_fos_glcm.csv', '53581796_573747ee33ef6e5a_MG_L_CC_ANON__haar_fos_glcm.csv', '53581860_21e6cc12630e5e9f_MG_L_ML_ANON__haar_fos_glcm.csv', '53581887_21e6cc12630e5e9f_MG_R_ML_ANON__haar_fos_glcm.csv', '53581914_21e6cc12630e5e9f_MG_L_CC_ANON__haar_fos_glcm.csv', '53581941_21e6cc12630e5e9f_MG_R_CC_ANON__haar_fos_glcm.csv', '53582304_8913a7e0cf3bd74e_MG_R_ML_ANON__haar_fos_glcm.csv', '53582331_8913a7e0cf3bd74e_MG_R_CC_ANON__haar_fos_glcm.csv', '53582395_3f0db31711fc9795_MG_L_ML_ANON__haar_fos_glcm.csv', '53582422_3f0db31711fc9795_MG_R_ML_ANON__haar_fos_glcm.csv', '53582449_3f0db31711fc9795_MG_L_CC_ANON__haar_fos_glcm.csv', '53582476_3f0db31711fc9795_MG_R_CC_ANON__haar_fos_glcm.csv', '53582540_3e73f1c0670cfb0a_MG_R_ML_ANON__haar_fos_glcm.csv', '53582567_3e73f1c0670cfb0a_MG_R_CC_ANON__haar_fos_glcm.csv', '53582656_465aa5ec1b59efc6_MG_L_ML_ANON__haar_fos_glcm.csv', '53582683_465aa5ec1b59efc6_MG_L_CC_ANON__haar_fos_glcm.csv', '53582710_465aa5ec1b59efc6_MG_R_ML_ANON__haar_fos_glcm.csv', '53582737_465aa5ec1b59efc6_MG_L_ML_ANON__haar_fos_glcm.csv', '53582764_465aa5ec1b59efc6_MG_R_ML_ANON__haar_fos_glcm.csv', '53582791_465aa5ec1b59efc6_MG_L_CC_ANON__haar_fos_glcm.csv', '53582818_465aa5ec1b59efc6_MG_R_CC_ANON__haar_fos_glcm.csv', '53586361_dda3c6969a34ff8e_MG_L_ML_ANON__haar_fos_glcm.csv', '53586388_dda3c6969a34ff8e_MG_R_ML_ANON__haar_fos_glcm.csv', '53586415_dda3c6969a34ff8e_MG_L_CC_ANON__haar_fos_glcm.csv', '53586442_dda3c6969a34ff8e_MG_R_CC_ANON__haar_fos_glcm.csv', '53586724_e5f3f68b9ce31228_MG_L_ML_ANON__haar_fos_glcm.csv', '53586751_e5f3f68b9ce31228_MG_R_ML_ANON__haar_fos_glcm.csv', '53586778_e5f3f68b9ce31228_MG_L_CC_ANON__haar_fos_glcm.csv', '53586805_e5f3f68b9ce31228_MG_R_CC_ANON__haar_fos_glcm.csv', '53586869_6ac23356b912ee9b_MG_L_ML_ANON__haar_fos_glcm.csv', '53586896_6ac23356b912ee9b_MG_L_CC_ANON__haar_fos_glcm.csv', '53586960_809e3f43339f93c6_MG_L_ML_ANON__haar_fos_glcm.csv', '53586987_809e3f43339f93c6_MG_R_ML_ANON__haar_fos_glcm.csv', '53587014_809e3f43339f93c6_MG_L_CC_ANON__haar_fos_glcm.csv', '53587041_809e3f43339f93c6_MG_R_CC_ANON__haar_fos_glcm.csv', '53587104_7b71aa9928e6975e_MG_L_ML_ANON__haar_fos_glcm.csv', '53587131_7b71aa9928e6975e_MG_L_CC_ANON__haar_fos_glcm.csv', '53587427_d2befe622e188943_MG_L_ML_ANON__haar_fos_glcm.csv', '53587454_d2befe622e188943_MG_R_ML_ANON__haar_fos_glcm.csv', '53587481_d2befe622e188943_MG_L_CC_ANON__haar_fos_glcm.csv', '53587508_d2befe622e188943_MG_R_CC_ANON__haar_fos_glcm.csv', '53587572_11e6732579acf692_MG_L_ML_ANON__haar_fos_glcm.csv', '53587599_11e6732579acf692_MG_L_CC_ANON__haar_fos_glcm.csv', '53587663_5fb370d4c1c71974_MG_R_CC_ANON__haar_fos_glcm.csv', '53587690_5fb370d4c1c71974_MG_L_ML_ANON__haar_fos_glcm.csv', '53587717_5fb370d4c1c71974_MG_R_ML_ANON__haar_fos_glcm.csv', '53587744_5fb370d4c1c71974_MG_L_CC_ANON__haar_fos_glcm.csv']\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "\n",
        "#first put a shortcut in your drive to the image processing folder\n",
        "\n",
        "#Choose the image folder you want to try\n",
        "\n",
        "pipeFolder = '10.8Prepro+haar+fixed+CorrectedLabels_full'\n",
        "\n",
        "RESULTS_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results', \n",
        "                        pipeFolder)\n",
        "\n",
        "RESULTS_DIR_MAIN = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results')\n",
        "\n",
        "MODELS_DIR =  os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results', \n",
        "                        'MLModelsFinal')\n",
        "\n",
        "\n",
        "\n",
        "DATA_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Image Processing and Analysis 2022',\n",
        "                        'projects',\n",
        "                        'Calcification Detection',\n",
        "                        'dataset')\n",
        "\n",
        "\n",
        "print(os.listdir(RESULTS_DIR))\n",
        "\n",
        "results_file = os.listdir(RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6-OnlgGfsSdp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5os_ORY98A4Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1-TQuxisz-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import tree\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pickle\n",
        "from sklearn.calibration import CalibratedClassifierCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qMqDYgXyktxL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier # random forest\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "eJbtKF-ylFf0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "#!pip install fastprogress\n",
        "from fastprogress import master_bar, progress_bar\n",
        "#!pip install tqdm\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from skimage import measure\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "neMctBoMuIMD"
      },
      "outputs": [],
      "source": [
        "def read_results_train_test(path, keys):\n",
        "  df_result = pd.DataFrame()\n",
        "  for result in os.listdir(path):\n",
        "    if result.split('.')[-1] == 'csv':\n",
        "      if int(result.split('_')[0]) in list(keys):\n",
        "        try:\n",
        "          df = pd.read_csv(path+'/'+result)\n",
        "          df_result = df_result.append(df)\n",
        "        except:\n",
        "          print(\"Empty file \", result)\n",
        "  return df_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZkqzRwsJlfUs"
      },
      "outputs": [],
      "source": [
        "def read_results(path):\n",
        "  dfs = pd.DataFrame()\n",
        "  for result in results_file:\n",
        "    try:\n",
        "      df = pd.read_csv(path+'/'+result)\n",
        "    except:\n",
        "      print(\"Empty file \", result)\n",
        "\n",
        "    dfs = dfs.append(df)\n",
        "  \n",
        "  return dfs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "huyN37RmC_uw"
      },
      "outputs": [],
      "source": [
        "def train_test_function():\n",
        "  df_train_test = pd.read_csv(os.path.join('/content',\n",
        "                                'drive',\n",
        "                                'MyDrive',\n",
        "                                'Results',\n",
        "                                'standard_partitions.csv'), index_col=0)\n",
        "  train_keys = df_train_test.loc[df_train_test.partition == 'train']['image_id'].values\n",
        "  test_keys = df_train_test.loc[df_train_test.partition == 'test']['image_id'].values\n",
        "\n",
        "\n",
        "  dfs_train = read_results_train_test(RESULTS_DIR, train_keys)\n",
        "  dfs_test = read_results_train_test(RESULTS_DIR, test_keys)\n",
        "  return dfs_train,dfs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4UKgbKXPDJA9"
      },
      "outputs": [],
      "source": [
        "# dfs_train, dfs_test = train_test_function()\n",
        "# dfs_train.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiwTtcPjrMRK"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "udO3hTL2c0W7"
      },
      "outputs": [],
      "source": [
        "# We will balance data randomly taking  the same amount of 0 and 1 label inputs. There are several options to manage unbalanced data,\n",
        "# this is just one approach, we should try different approaches.\n",
        "# https://towardsdatascience.com/comparing-different-classification-machine-learning-models-for-an-imbalanced-dataset-fdae1af3677f\n",
        "def rus(dfs, not_consider=0):\n",
        "  # df_negative = dfs.loc[dfs.label == dfs.label.value_counts().idxmax()] #This is for making it more general\n",
        "  df_negative = dfs.loc[dfs.label == 0] #Take all negative samples from dfs -> label = 0\n",
        "  print('rus negative 1: ', len(df_negative))\n",
        "  print('rus negative 1 without duplicates: ', len(df_negative.drop_duplicates()))\n",
        "  #Here we are taking a subset from the 'negative pool' called df_negative with length equal to the number of positive candidates I have\n",
        "  if not_consider >= len(dfs.loc[dfs.label == 1]):\n",
        "    df_0 = pd.DataFrame()\n",
        "  else:\n",
        "    df_0 = df_negative.sample(len(dfs.loc[dfs.label == 1])-not_consider, random_state = 1) #not consider is the number of negative samples that I already have and that I misclassified the first time\n",
        "  print('df_0: ', len(df_0))\n",
        "  print('append ', len(df_negative.append(df_0)))\n",
        "  print('df_negative drop no keeping', len(df_negative.drop_duplicates(keep=False)))\n",
        "  df_negative = df_negative.append(df_0).drop_duplicates(keep=False) #df_negative is the samples that have not been used, and have not been selected yet for the training\n",
        "  print('after drop duplicates with appended: ',len(df_negative))\n",
        "  dfs_eq = df_0.append(dfs.loc[dfs.label == 1]) #dfs.label.value_counts().idxmin() #Here we are taking the balanced tada\n",
        "  print(\"Length of balanced data: \", len(dfs_eq))\n",
        "  print(dfs_eq['label'].value_counts())\n",
        "  return dfs_eq, df_negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "dLoDror0v6el"
      },
      "outputs": [],
      "source": [
        "def plotFeatures(data):\n",
        "  #plot the scatter matrix\n",
        "  pd.plotting.scatter_matrix(data,figsize=(25,25))\n",
        "  #correlation plot\n",
        "  corr = data.corr()\n",
        "  f, ax = plt.subplots(figsize=(25, 25))\n",
        "  sns.heatmap(corr,annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hNRW1sKxHoOu"
      },
      "outputs": [],
      "source": [
        "def standardScaler(X_train, X_test):\n",
        "  standard_scaler = StandardScaler()\n",
        "  X_train = standard_scaler.fit_transform(X_train)\n",
        "  X_test = standard_scaler.transform(X_test)\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXE13LCNrZcX"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "z0Icd_DnzSdZ"
      },
      "outputs": [],
      "source": [
        "def featureSelectionTrees(estimators, X_train, y_train, X_test):\n",
        "  clf = ExtraTreesClassifier(n_estimators=estimators)\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "  model = SelectFromModel(clf, prefit=True)\n",
        "  X_train = model.transform(X_train)\n",
        "  X_test = model.transform(X_test)\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "jdnQpMHUI-0T"
      },
      "outputs": [],
      "source": [
        "def pcaAnalysis(components, X_train, X_test):\n",
        "  pca = PCA(components)\n",
        "  X_train = pca.fit_transform(X_train)\n",
        "  X_test = pca.transform(X_test)\n",
        "  print(\"PCA variance ratio: \", pca.explained_variance_ratio_)\n",
        "  print(\"Total variance Explained by PCA: \", sum(pca.explained_variance_ratio_))\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "uTqXxdqBLFYS"
      },
      "outputs": [],
      "source": [
        "def selectFeaturesChi(k, X_train, X_test, y_train):\n",
        "  sel = SelectKBest(chi2, k=k)\n",
        "  sel.fit(X_train, y_train)\n",
        "  X_train = sel.transform(X_train)\n",
        "  X_test = sel.transform(X_test)\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b3nb74MroMO"
      },
      "source": [
        "# **Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "SGbJYG0szmJW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import matthews_corrcoef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kN7GRye0Fjb2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def RandomForest(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    print(\"Searching for best hyperparameters\")\n",
        "    params = {'criterion': ['gini'],\n",
        "              'n_estimators': [100], # , 500, 900\n",
        "              'max_features': ['auto', 'sqrt'],#, 'sqrt', 'log2'\n",
        "              'max_depth' : [10, 12]}\n",
        "    grid = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs = -1), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for rf are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = RandomForestClassifier(random_state=42, n_jobs = -1)\n",
        "  \n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "vI8BeXyIJIqj"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier # gradient boosting regressor\n",
        "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
        "def GradientBoosting(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'learning_rate': [0.1],#0.05, 0.2\n",
        "              #'min_samples_split': [0.5, 0.8],\n",
        "              #'min_samples_leaf': [0.1, 0.2, 0.5],\n",
        "              'max_depth':[8],\n",
        "              #'max_features':['sqrt'],#'log2'\n",
        "              #'criterion': ['friedman_mse',  'mae'],\n",
        "              #'subsample':[0.5, 1.0],\n",
        "              'n_estimators':[600]}\n",
        "    grid = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for gb are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "\n",
        "  classifier = GradientBoostingClassifier(random_state=42)\n",
        "  \n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "oZcwaXUQK71l"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier # adaboost, adaboostRegressor for regression problems\n",
        "\n",
        "def AdaBoost(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'n_estimators': [600],\n",
        "              'learning_rate': [0.5]}\n",
        "    grid = GridSearchCV(AdaBoostClassifier(random_state=42), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for ab are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = AdaBoostClassifier(random_state=42)\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "KiHOO-9tLlBK"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def LogRegre(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'multi_class':['multinomial'],\n",
        "              'solver': ['sag'],#,'saga'\n",
        "              'penalty': ['l2']}#'elasticnet',\n",
        "    grid = GridSearchCV(LogisticRegression(random_state=42), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for lrg are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = LogisticRegression(random_state=42)\n",
        "  \n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "kkiwq_C0lD38"
      },
      "outputs": [],
      "source": [
        "def KNN(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'n_neighbors':[3,5]}\n",
        "    grid = GridSearchCV(KNeighborsClassifier(), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "  classifier = KNeighborsClassifier()\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "5hohRhuWbs3W"
      },
      "outputs": [],
      "source": [
        "def DecisionTree(X_train, y_train, cv=5, best_params = dict()):\n",
        "  classifier = tree.DecisionTreeClassifier()\n",
        "  #classifier = CalibratedClassifierCV(base_estimator=dtclf, cv='prefit')\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "pjVbyTcmwekE"
      },
      "outputs": [],
      "source": [
        "# This is an example of how to use a Pipe inside a function we are training, as done in Challenge 3 by the professor\n",
        "def SVC_linear(X_train, y_train, cv=5, best_params = dict()):\n",
        "\n",
        "  if len(best_params) == 0:\n",
        "    lower_value_C = 1\n",
        "    higher_value_C = 10\n",
        "    n_values = 10\n",
        "    base = 10\n",
        "    params = {'C': [0.1, 1, 3, 5,9,11], #12\n",
        "              'kernel' : ['rbf'],\n",
        "              'gamma': [2.5, 5, 10]}\n",
        "\n",
        "    grid = GridSearchCV(SVC(random_state = 42, probability=True), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for svm are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = SVC(random_state = 42, probability=True)\n",
        "  #classifier = CalibratedClassifierCV(base_estimator=svcclf, cv='prefit')\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "oZcA9Dy5CtbA"
      },
      "outputs": [],
      "source": [
        "def negative_pool(dataBalanced, dfNegative, hyperOptBalanceData, X_test, y_test, classifier, calibrated=False, hyperParam=dict() ):\n",
        "  count = 0\n",
        "  f1_scores = []\n",
        "  pAUCs = []\n",
        "\n",
        "  trainBalanceData = dataBalanced.drop('label', axis=1)\n",
        "  trainBalanceLabel = dataBalanced['label']\n",
        "\n",
        "  hyperOptTrainData = hyperOptBalanceData.drop('label', axis=1)\n",
        "  hyperOptLabel = hyperOptBalanceData['label']\n",
        "\n",
        "\n",
        "# Test on the training set to check for overfitting\n",
        "# We can try with a cascade - Viola Jones\n",
        "# Adaboost for classifier - Rankboost\n",
        "# Nor PCA because not a lot of features\n",
        "# Correlation check.\n",
        "\n",
        "  while True:\n",
        "\n",
        "    #Classification\n",
        "\n",
        "    if count==0:\n",
        "\n",
        "      print(f\"First iteration {classifier}... looking for best hyperparameters\")\n",
        "\n",
        "      if classifier == 'svm':\n",
        "        clf, best_params = SVC_linear(hyperOptTrainData, hyperOptLabel, best_params = hyperParam)\n",
        "      elif classifier == 'rf':\n",
        "        clf, best_params = RandomForest(hyperOptTrainData, hyperOptLabel, best_params = {'criterion': 'gini', 'max_depth': 12, 'max_features': 'auto', 'n_estimators': 100})\n",
        "      elif classifier == 'gb':\n",
        "        clf, best_params = GradientBoosting(hyperOptTrainData, hyperOptLabel, best_params = {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 600})\n",
        "      elif classifier == 'adaboost':\n",
        "        clf, best_params = AdaBoost(hyperOptTrainData, hyperOptLabel, best_params = {'learning_rate': 0.5, 'n_estimators': 600})\n",
        "      elif classifier == 'logreg':\n",
        "        clf, best_params = LogRegre(hyperOptTrainData, hyperOptLabel, best_params = hyperParam)\n",
        "      elif classifier == 'knn':\n",
        "        clf, best_params = KNN(hyperOptTrainData, hyperOptLabel, best_params = hyperParam)\n",
        "      elif classifier == 'dt':\n",
        "        clf, best_params = DecisionTree(hyperOptTrainData, hyperOptLabel,best_params = hyperParam)\n",
        "\n",
        "      clf.set_params(**best_params)    \n",
        "      if calibrated:\n",
        "        clf.fit(trainBalanceData, trainBalanceLabel)\n",
        "        clf = CalibratedClassifierCV(base_estimator=clf, cv='prefit')\n",
        "    \n",
        "\n",
        "    clf.fit(trainBalanceData, trainBalanceLabel)\n",
        "    pred = clf.predict(trainBalanceData)\n",
        "\n",
        "\n",
        "    print(\"TRAIN\")\n",
        "    print('F1-score train: ', f1_score(trainBalanceLabel, pred))\n",
        "    y_pred_proba_tr = clf.predict_proba(trainBalanceData)[:, 1]\n",
        "    print('pAUC train: ', roc_auc_score(trainBalanceLabel, y_pred_proba_tr, max_fpr = 0.0001))\n",
        "    print(confusion_matrix(trainBalanceLabel, pred))\n",
        "\n",
        "    print(\"TEST\")\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print('F1-score test: ', f1_score(y_test, y_pred))\n",
        "    f1_scores.append(f1_score(y_test, y_pred))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print('pAUC test: ', roc_auc_score(y_test, y_pred_proba, max_fpr = 0.0001))\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    pAUCs.append(roc_auc_score(y_test, y_pred_proba, max_fpr = 0.0001))\n",
        "\n",
        "    misclassified = pd.DataFrame(columns = trainBalanceData.columns)\n",
        "    labels_pool = []\n",
        "    print(len(pred))\n",
        "    print(len(trainBalanceLabel))\n",
        "    for i in range(len(pred)):\n",
        "      if (pred[i]==1) & (trainBalanceLabel.iloc[i] == 0): #FP False positive\n",
        "#        new_row = pd.DataFrame(X_test[i].reshape(1, data.shape[1] ), columns=data.columns)\n",
        "        new_row = trainBalanceData.iloc[i]\n",
        "        misclassified = misclassified.append(new_row)\n",
        "        labels_pool.append(trainBalanceLabel.iloc[i])\n",
        "\n",
        "    dfPositive = dataBalanced.loc[dataBalanced.label == 1]\n",
        "\n",
        "    if len(dfPositive)*0.9 >= (len(dfNegative)+len(misclassified)):\n",
        "      print(\"Negative pool is too short...\")\n",
        "      print(\"Len of positive samples: \", len(dfPositive))\n",
        "      print(\"Len of negative samples: \", len(dfNegative)+len(misclassified))\n",
        "      break\n",
        "\n",
        "    if (len(f1_scores) > 5 and (f1_scores[-1] - f1_scores[-2])< 0.001): \n",
        "      print('last ', f1_scores[-1])\n",
        "      print('second last ', f1_scores[-2])\n",
        "      print(\"No significant change in 5 iterations. Escaping\")\n",
        "      break\n",
        "\n",
        "    dataBalanced, dfNegative = rus(dfPositive.append(dfNegative), len(misclassified)) #Get a new pool of dfNegative\n",
        "    \n",
        "    trainBalanceLabel = dataBalanced['label'].append(pd.Series(labels_pool))\n",
        "    trainBalanceData = dataBalanced.drop(['label'], axis=1).append(misclassified)\n",
        "\n",
        "    print('Data len: ', len(trainBalanceData))\n",
        "    print('labels len: ', len(trainBalanceLabel))\n",
        "    \n",
        "    count +=1\n",
        "\n",
        "  return clf, best_params, y_pred_proba, y_pred, np.mean(f1_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXAhatZZ_Ap-"
      },
      "source": [
        "# **Scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgc5Kg2FLMMC",
        "outputId": "51b4e944-ddaf-4e2a-c482-5cd144c79ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['normals.txt', 'images', 'groundtruths', 'masks']\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "\n",
        "#first put a shortcut in your drive to the image processing folder\n",
        "\n",
        "DATA_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Image Processing and Analysis 2022',\n",
        "                        'projects',\n",
        "                        'Calcification Detection',\n",
        "                        'dataset')\n",
        "\n",
        "\n",
        "print(os.listdir(DATA_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Ao5mV9BCL4sa"
      },
      "outputs": [],
      "source": [
        "_, _, groundTruths = next(os.walk(os.path.join(DATA_DIR, 'groundtruths')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "8U-oRZ_h_AJB"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.function_base import average\n",
        "import cv2\n",
        "\n",
        "def calculateFROC(groundTruthsList, normals, candidates, prediction, model, pipeline):\n",
        "\n",
        "  #List needed for counting\n",
        "  evaluationList = []\n",
        "\n",
        "  # evaluation froc curve #\n",
        "\n",
        "  fn = 0 # false negative, for the blobs that do not belong to any component\n",
        "  positive_candidates = 0\n",
        "  flag = True\n",
        "\n",
        "  rowListdfROC = []\n",
        "\n",
        "  candidates_copy= candidates.copy()\n",
        "  candidates_copy['prediction'] = prediction\n",
        "\n",
        "  for imageKey in tqdm_notebook(groundTruthsList):\n",
        "\n",
        "\n",
        "    evaluationList = []\n",
        " \n",
        "    # list of features found with y,x and sigma\n",
        "    candidatesImg = candidates_copy.loc[candidates_copy['name'].str.contains(imageKey.split('_')[0], regex=False)]\n",
        "    candidates_number = len(candidatesImg)\n",
        "#    print('cand number: ', candidates_number)\n",
        "\n",
        "    mask = cv2.imread(os.path.join(DATA_DIR, 'groundtruths', imageKey), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        " #   print('# IMAGEKEY: ', imageKey)\n",
        "    blobs = mask > 0.7 * mask.mean() #Thresholding the backgroudnd\n",
        "    blobs_labels, count = measure.label(blobs, background=0, return_num=True) #Getting labels of the connected components and the amount of them without considering the count\n",
        "\n",
        "    # dictionaries\n",
        "    dictCounting={}\n",
        "\n",
        "  #  print(count)\n",
        "    for index in range(1, count+1):\n",
        "      dictCounting[index] = 0\n",
        "    \n",
        "    evaluationList = []\n",
        "    dictMean = {}\n",
        "\n",
        "    for index2, candidate in candidatesImg.iterrows():\n",
        "      sigma = 7\n",
        "      \n",
        "      if (imageKey.split('_')[0] in normals):\n",
        "        if candidate['name'].split('_')[1] not in dictMean.keys():\n",
        "          dictMean[candidate['name'].split('_')[1]] = [candidate.prediction]\n",
        "        else:\n",
        "          dictMean[candidate['name'].split('_')[1]].append(candidate.prediction)\n",
        "        continue\n",
        "      \n",
        "\n",
        "      # is the image a normal image?\n",
        "\n",
        "      if (((candidate.x - sigma) < 0) or \n",
        "        ((candidate.x + sigma) > mask.shape[0]) or \n",
        "        ((candidate.y - sigma) < 0) or\n",
        "        ((candidate.y + sigma) > mask.shape[1])):\n",
        "        continue\n",
        "\n",
        "\n",
        "      # n = 3\n",
        "      left = int(candidate.x - sigma)\n",
        "      right = int(candidate.x + sigma)\n",
        "      top = int(candidate.y - sigma)\n",
        "      bottom = int(candidate.y + sigma) \n",
        "\n",
        "      #    y : y + w , x : x +  h\n",
        "      nonzero = cv2.countNonZero(blobs_labels[top:bottom, left: right])\n",
        "\n",
        "      if nonzero > 0:\n",
        "        # Find all connected components (cc) that intersect with the candidate\n",
        "        foundCC = [i for i in np.unique(blobs_labels[top:bottom, left: right]) if i!= 0]\n",
        "\n",
        "        # Keep the maximum prediction of the candidates that intsersect with at least one component\n",
        "        for cc in foundCC:\n",
        "          dictCounting[cc] = max(candidate.prediction, dictCounting[cc])\n",
        "\n",
        "    if (imageKey.split('_')[0] not in normals):\n",
        "      for key,value in dictCounting.items():\n",
        "        if np.sum(blobs_labels == key) > np.floor(np.pi*(15/2.0)**2):\n",
        "#          print(\"Too big... discarded\")\n",
        "          continue\n",
        "        if value > 0:\n",
        "            rowListdfROC.append(['TP', value])\n",
        "        else:\n",
        "          fn = fn + 1\n",
        "    else:\n",
        "      if len(dictMean.keys()) > 0:\n",
        "        for key, value in dictMean.items():\n",
        "          rowListdfROC.append(['FP', np.max(np.array(value))])\n",
        "\n",
        "\n",
        "  filename_key = 'FROC_calculations_{}_pip{}_fn{}_normals{}_negative_pool_scaled.csv'.format(model, pipeline, fn, len(normals))\n",
        "\n",
        "  dfROC = pd.DataFrame(rowListdfROC, columns=['type', 'prob'])\n",
        "\n",
        "  dfROC.to_csv(os.path.join(MODELS_DIR,\n",
        "                            'negative_pool_scaled',\n",
        "                            filename_key))\n",
        "  \n",
        "  print(\"File saved as \", filename_key)\n",
        "\n",
        "\n",
        "\n",
        "  return fn, dfROC, filename_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "a-mMXd-6JYpT"
      },
      "outputs": [],
      "source": [
        "def writeFile(df, flag, name):\n",
        "  if(flag):\n",
        "    df.to_csv(os.path.join('/content',\n",
        "                                 'drive',\n",
        "                                 'MyDrive',\n",
        "                                 'Results',\n",
        "                                 name),\n",
        "                    mode='a',\n",
        "                    index=False)\n",
        "    flag = False\n",
        "  else:\n",
        "    df.to_csv(os.path.join('/content',\n",
        "                                 'drive',\n",
        "                                 'MyDrive',\n",
        "                                 'Results',\n",
        "                                 name),\n",
        "                  mode='a',\n",
        "                  header=False,\n",
        "                  index=False)\n",
        "  return flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "K2iVgV-pBsQA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from numpy import trapz\n",
        "\n",
        "def draw_curve(fn, normals, dfROC, name, clf_name):\n",
        "\n",
        "  experiments_results_path = os.path.join(MODELS_DIR, 'negative_pool_scaled')\n",
        "  results_filename = f'results_AUC_negative_pool_scaled_{pipeFolder}.csv'\n",
        "  confusion_matrix_filename = f'results_ConfMat_negative_pool_scaled_{pipeFolder}.csv'\n",
        "\n",
        "  if results_filename not in os.listdir(experiments_results_path):\n",
        "    results_df = pd.DataFrame(columns=[\"model\", \"AUC_TOTAL\", \"AUC_final_50fPpi\", \"F1_final\"])\n",
        "    results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "    \n",
        "    results_confmat = pd.DataFrame(columns=[\"model\", \"TP\", \"FP\", \"FN\", \"Total True\"])\n",
        "    results_confmat.to_csv(os.path.join(experiments_results_path, confusion_matrix_filename), index=False)\n",
        "\n",
        "  else:\n",
        "    results_df = pd.read_csv(os.path.join(experiments_results_path, results_filename))\n",
        "    results_confmat = pd.read_csv(os.path.join(experiments_results_path, confusion_matrix_filename))\n",
        "\n",
        "  tpc = 0\n",
        "  fpc = 0\n",
        "  tpr = []\n",
        "  fppi = []\n",
        "\n",
        "  dfROC['prob'] = [round(i,4) for i in dfROC['prob'].values]\n",
        "  dfROC = dfROC.sort_values('prob', ascending=False)\n",
        "\n",
        "  thresholds = dfROC.prob.unique()\n",
        "  print('Number of thresholds: ', len(thresholds))\n",
        "  print(thresholds)\n",
        "\n",
        "\n",
        "  tp = len(dfROC.loc[dfROC.type == 'TP'])\n",
        "  print('true positives: ', tp)\n",
        "  fp = len(dfROC.loc[dfROC.type == 'FP'])\n",
        "  print('false positives: ',  fp)\n",
        "  print(\"Total number of positives: \", tp+fn)\n",
        "\n",
        "  for i in progress_bar(range(len(thresholds))):\n",
        "\n",
        "    tpc += len(dfROC.loc[(dfROC.prob > thresholds[i]) & (dfROC.type=='TP')])\n",
        "    fpc += len(dfROC.loc[(dfROC.prob > thresholds[i]) & (dfROC.type!='TP')])\n",
        "              \n",
        "    # print('TP amount {} in threshold {}'.format(tpc, thresholds[i]))\n",
        "    # print('FP amount {} in threshold {}'.format(fpc, thresholds[i]))\n",
        "    \n",
        "\n",
        "    tpr.append( tpc/(tp+fn) )\n",
        "    fppi.append( fpc/normals )\n",
        "    tpc = 0\n",
        "    fpc = 0\n",
        "  \n",
        "  gamma = [i for i in fppi if i <= 50 ]\n",
        "  print(max(fppi))\n",
        "  auc_total = trapz(tpr, x=fppi)/max(fppi)\n",
        "  print(max(gamma))\n",
        "  auc_final = trapz(tpr[0:len(gamma)], x=gamma)/max(gamma)\n",
        "\n",
        "  print('AUC TOTAL:', auc_total)\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(fppi, tpr)\n",
        "  ax.set_xlabel('fPpI', fontsize=15)\n",
        "  ax.set_ylabel('TPR', fontsize=15)\n",
        "  ax.grid(True)\n",
        "  plt.ylim(0,1)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  print('AUC final:', auc_final)\n",
        "  fig1, ax1 = plt.subplots()\n",
        "  ax1.plot(gamma, tpr[0:len(gamma)])\n",
        "  ax1.set_xlabel('fPpI', fontsize=12)\n",
        "  ax1.set_ylabel('TPR', fontsize=12)\n",
        "  ax1.grid(True)\n",
        "  plt.ylim(0,1)\n",
        "  plt.xlim(0,50)\n",
        "\n",
        "  plt.savefig(os.path.join(MODELS_DIR,\n",
        "                          'negative_pool_scaled', \n",
        "                           name+'.eps'), format='eps')\n",
        "  \n",
        "  plt.show()\n",
        "  \n",
        "  final_f1 = tp / (tp + (fp + fn)/2)\n",
        "\n",
        "  row_results = [clf_name, auc_total, auc_final, final_f1]\n",
        "  results_df = results_df.append(pd.DataFrame([row_results], columns=[\"model\", \"AUC_TOTAL\", \"AUC_final_50fPpi\", \"F1_final\"]))\n",
        "  results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "\n",
        "  confmat_results = [clf_name, tp, fp, fn, tp+fn]\n",
        "  results_confmat = results_confmat.append(pd.DataFrame([confmat_results], columns=[\"model\", \"TP\", \"FP\", \"FN\", \"Total True\"]))\n",
        "  results_confmat.to_csv(os.path.join(experiments_results_path, confusion_matrix_filename), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "HCEqoIMVadcS"
      },
      "outputs": [],
      "source": [
        "def load_and_predict(selectedFold, classifier_name):\n",
        "\n",
        "  #Reading data\n",
        "  dfs_train, dfs_test = train_test_function()\n",
        "  print('Length of train set: ', len(dfs_train))\n",
        "  print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "  dfs_tr_hyperopt = dfs_train.sample(frac=0.1, random_state=42)\n",
        "  dfs_ts_hyperopt = dfs_test.sample(frac=0.1, random_state=42)\n",
        "\n",
        "  dfs_train = dfs_train.append(dfs_tr_hyperopt).drop_duplicates(keep=False)\n",
        "  dfs_test = dfs_test.append(dfs_ts_hyperopt).drop_duplicates(keep=False)\n",
        "  print('Length of train set: ', len(dfs_train))\n",
        "  print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "  try:  \n",
        "    if dfs_train.isnull().values.any():\n",
        "      colm = dfs_train.columns[dfs_train.isna().any()]\n",
        "      dfs_train = dfs_train[dfs_train[colm[0]].notna()]\n",
        "\n",
        "    if dfs_test.isnull().values.any():\n",
        "      colm = dfs_test.columns[dfs_test.isna().any()]    \n",
        "      dfs_test = dfs_test[dfs_test[colm[0]].notna()]\n",
        "\n",
        "    print(\"Train dfs without nans: \", len(dfs_train))\n",
        "    print(\"Test dfs without nans: \",len(dfs_test))\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"No nan values to drop, or not columns\")\n",
        "    print(e)\n",
        "\n",
        "  y_train = dfs_train['label'].reset_index(drop=True)\n",
        "  candidates_train = dfs_train[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "  X_train = dfs_train.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "  y_test = dfs_test['label'].reset_index(drop=True)\n",
        "  candidates_test = dfs_test[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "  X_test = dfs_test.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "  #Obtain train-test datasets\n",
        "  if selectedFold == 2:\n",
        "    chosenCandidates = candidates_train\n",
        "    trainDataFull = pd.DataFrame(X_test).copy()\n",
        "    ss_f2 = StandardScaler()\n",
        "    trainDataFull = pd.DataFrame(ss_f2.fit_transform(trainDataFull), columns=X_test.columns)\n",
        "    trainDataFull['label'] = y_test\n",
        "\n",
        "    model = pickle.load(open(os.path.join(MODELS_DIR,'negative_pool_scaled', f'best_model_{classifier_name}_{selectedFold}_{pipeFolder}_negative_pool_scaled.pkl'), 'rb'))\n",
        "    y_pred = model.predict(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))\n",
        "    y_pred_proba = model.predict_proba(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))[:,1]\n",
        "\n",
        "  else:\n",
        "    chosenCandidates = candidates_test\n",
        "    trainDataFull = pd.DataFrame(X_train).copy()\n",
        "    ss_f1 = StandardScaler()\n",
        "    trainDataFull = pd.DataFrame(ss_f1.fit_transform(trainDataFull), columns=X_train.columns)\n",
        "    trainDataFull['label'] = y_train\n",
        "\n",
        "    model = pickle.load(open(os.path.join(MODELS_DIR,'negative_pool_scaled', f'best_model_{classifier_name}_{selectedFold}_{pipeFolder}_negative_pool_scaled.pkl'), 'rb'))\n",
        "    y_pred = model.predict(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))\n",
        "    y_pred_proba = model.predict_proba(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))[:,1]\n",
        "\n",
        "\n",
        "  normals = []\n",
        "  with open(os.path.join(RESULTS_DIR_MAIN,'normals_final.txt')) as f:\n",
        "      for line in f:\n",
        "          normals.append(line[:-1])\n",
        "\n",
        "  if 'glcm' in pipeFolder:\n",
        "    imageKeyName = 2\n",
        "  else:\n",
        "    imageKeyName = 3\n",
        "\n",
        "  candidates_unique = [cand.split('_')[imageKeyName] for cand in chosenCandidates['name'].values.tolist()]\n",
        "  candidates_unique = list(set(candidates_unique))\n",
        "\n",
        "  groundTruthShort = [i for i in groundTruths if i.split('_')[0] in candidates_unique]\n",
        "  print(len(groundTruthShort))\n",
        "\n",
        "  normals = [i for i in normals if i in candidates_unique]\n",
        "\n",
        "  print(\"Calculating fROC file...\") \n",
        "  fn, dfROC, filename_key = calculateFROC(groundTruthShort, normals, chosenCandidates, y_pred_proba, classifier_name, pipeFolder+'_loaded_')\n",
        "  print(len(dfROC.prob.unique()))\n",
        "\n",
        "  return fn, dfROC, filename_key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "lJUk1fLQ2uCa"
      },
      "outputs": [],
      "source": [
        "def generate_fold_results(selected_candidates, normals, imageKeyLocation, trained_clf, y_test, y_pred, y_pred_proba, selectedFold, classifier):\n",
        "\n",
        "  df_cm = confusion_matrix(y_test, y_pred )\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(df_cm, annot=True, fmt='d') # font size\n",
        "  plt.show()\n",
        "\n",
        "  filename = f'best_model_{classifier}_{selectedFold}_{pipeFolder}_negative_pool_scaled.pkl'\n",
        "  pickle.dump(trained_clf, open(os.path.join(MODELS_DIR,'negative_pool_scaled', filename), 'wb'))\n",
        "  print(f\"Fold {selectedFold} of {classifier} was saved!\")  \n",
        "\n",
        "  candidates_unique = [cand.split('_')[imageKeyLocation] for cand in selected_candidates['name'].values.tolist()]\n",
        "\n",
        "  candidates_unique = list(set(candidates_unique))\n",
        "  groundTruthShort = [i for i in groundTruths if i.split('_')[0] in candidates_unique]\n",
        "\n",
        "  print(len(groundTruthShort))\n",
        "\n",
        "  normals = [i for i in normals if i in candidates_unique]\n",
        "\n",
        "  print(\"Calculating fROC file...\")\n",
        "  fn, dfROC, filename_key = calculateFROC(groundTruthShort, normals, selected_candidates, y_pred_proba, classifier, pipeFolder)\n",
        "\n",
        "  return fn, dfROC, filename_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_liWLRjttKkK"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ro6OlkCe7a",
        "outputId": "d9fd3b3a-433e-4502-908c-d23941f84117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train set:  378938\n",
            "Length of test set:  393246\n",
            "Length of train set:  341044\n",
            "Length of test set:  353921\n",
            "Train dfs without nans:  337133\n",
            "Test dfs without nans:  350004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Median_LL1', 'sumVariance_LL1', 'sumAverage_LL1', 'sumAverage_LL2', 'Entropy_LH2', 'sumAverage_LH2', 'Variance_HL2', 'kurtosis_HL2', 'diffEntropy_HL2', 'sumAverage_HL2', 'homogeneity_LH1', 'sumEntropy_LH1', 'diffEntropy_LH1', 'sumAverage_LH1', 'sumEntropy_HL1', 'diffEntropy_HL1', 'sumAverage_HL1', 'skewness_HH1', 'kurtosis_HH1', 'Entropy_HH1', 'CoefficientOfVariation_HH1', 'homogeneity_HH1', 'sumEntropy_HH1', 'diffEntropy_HH1', 'sumAverage_HH1']\n",
            "rus negative 1:  330767\n",
            "rus negative 1 without duplicates:  330742\n",
            "df_0:  6366\n",
            "append  337133\n",
            "df_negative drop no keeping 330729\n",
            "after drop duplicates with appended:  324363\n",
            "Length of balanced data:  12732\n",
            "0    6366\n",
            "1    6366\n",
            "Name: label, dtype: int64\n",
            "Data Balanced fold 1:  12732\n",
            "Negative pool fold 1:  324363\n",
            "rus negative 1:  346674\n",
            "rus negative 1 without duplicates:  346610\n",
            "df_0:  3330\n",
            "append  350004\n",
            "df_negative drop no keeping 346583\n",
            "after drop duplicates with appended:  343254\n",
            "Length of balanced data:  6660\n",
            "0    3330\n",
            "1    3330\n",
            "Name: label, dtype: int64\n",
            "Data Balanced fold 2:  6660\n",
            "Negative pool fold 2:  343254\n",
            "Training new model...\n",
            "******************** RESULTS FOLD 1 **********************\n",
            "First iteration gb... looking for best hyperparameters\n",
            "TRAIN\n",
            "F1-score train:  1.0\n",
            "pAUC train:  1.0\n",
            "[[6366    0]\n",
            " [   0 6366]]\n",
            "TEST\n",
            "F1-score test:  0.05239179954441913\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.76      0.86    346674\n",
            "           1       0.03      0.71      0.05      3330\n",
            "\n",
            "    accuracy                           0.76    350004\n",
            "   macro avg       0.51      0.73      0.46    350004\n",
            "weighted avg       0.99      0.76      0.85    350004\n",
            "\n",
            "pAUC test:  0.5016080144996069\n",
            "[[261939  84735]\n",
            " [   961   2369]]\n",
            "12732\n",
            "12732\n",
            "rus negative 1:  324363\n",
            "rus negative 1 without duplicates:  324363\n",
            "df_0:  6366\n",
            "append  330729\n",
            "df_negative drop no keeping 324363\n",
            "after drop duplicates with appended:  317997\n",
            "Length of balanced data:  12732\n",
            "0    6366\n",
            "1    6366\n",
            "Name: label, dtype: int64\n",
            "Data len:  12732\n",
            "labels len:  12732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ]
        }
      ],
      "source": [
        "# def main():\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "normals = []\n",
        "with open(os.path.join(RESULTS_DIR_MAIN,'normals_final.txt')) as f:\n",
        "  for line in f:\n",
        "      normals.append(line[:-1])\n",
        "\n",
        "if 'glcm' in pipeFolder:\n",
        "  imageKeyName = 2\n",
        "  usePCA = False\n",
        "else:\n",
        "  imageKeyName = 3\n",
        "  usePCA = True\n",
        "\n",
        "\n",
        "calibration_classifiers = ['adaboost', 'svm']\n",
        "\n",
        "classifiers = [ 'gb', 'rf', 'logreg', 'knn', 'dt']\n",
        "best_hyperparameters = dict()\n",
        "y_pred_proba = []\n",
        "f1_scores = []\n",
        "\n",
        "# Creating the results file\n",
        "experiments_results_path = os.path.join(MODELS_DIR, 'negative_pool_scaled')\n",
        "results_filename = f'results_negative_pool_scaled_{pipeFolder}.csv'\n",
        "\n",
        "if results_filename not in os.listdir(experiments_results_path):\n",
        "  results_df = pd.DataFrame(columns=[\"model\", \"pAUC_mean\", \"f1_mean\", \"mcc_mean\", \"pAUC\", \"f1\", \"mcc\", \"selectedFold\", \"parameters\"])\n",
        "  results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "else:\n",
        "  results_df = pd.read_csv(os.path.join(experiments_results_path, results_filename))\n",
        "\n",
        "\n",
        "#Reading data\n",
        "dfs_train, dfs_test = train_test_function()\n",
        "print('Length of train set: ', len(dfs_train))\n",
        "print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "dfs_tr_hyperopt = dfs_train.sample(frac=0.1, random_state=42)\n",
        "dfs_ts_hyperopt = dfs_test.sample(frac=0.1, random_state=42)\n",
        "\n",
        "dfs_train = dfs_train.append(dfs_tr_hyperopt).drop_duplicates(keep=False)\n",
        "dfs_test = dfs_test.append(dfs_ts_hyperopt).drop_duplicates(keep=False)\n",
        "print('Length of train set: ', len(dfs_train))\n",
        "print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "dfs_hyperOpt = shuffle(dfs_tr_hyperopt.append(dfs_ts_hyperopt))\n",
        "\n",
        "\n",
        "#Preprocessing steps\n",
        "#Manage unbalanced data\n",
        "# dataBalanced, dfNegative = rus(dfs) #Random majority undersampling\n",
        "\n",
        "try:  \n",
        "  if dfs_train.isnull().values.any():\n",
        "    colm = dfs_train.columns[dfs_train.isna().any()]\n",
        "    for c in colm:\n",
        "      dfs_train = dfs_train[dfs_train[c].notna()]\n",
        "\n",
        "  if dfs_test.isnull().values.any():\n",
        "    colm = dfs_test.columns[dfs_test.isna().any()]    \n",
        "    for c in colm:\n",
        "      dfs_test = dfs_test[dfs_test[c].notna()]\n",
        "\n",
        "  if dfs_hyperOpt.isnull().values.any():\n",
        "    colm = dfs_hyperOpt.columns[dfs_hyperOpt.isna().any()]\n",
        "    for c in colm:\n",
        "      dfs_hyperOpt = dfs_hyperOpt[dfs_hyperOpt[c].notna()]\n",
        "\n",
        "  print(\"Train dfs without nans: \", len(dfs_train))\n",
        "  print(\"Test dfs without nans: \",len(dfs_test))\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"No nan values to drop, or not columns\")\n",
        "  print(e)\n",
        "\n",
        "# DROP THE CORRELATED VARIABLES\n",
        "cor_matrix = dfs_hyperOpt.corr().abs()\n",
        "upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
        "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
        "print(to_drop)\n",
        "\n",
        "dfs_hyperOpt = dfs_hyperOpt.drop([i for i in dfs_hyperOpt.columns if i in to_drop], axis=1)\n",
        "dfs_train = dfs_train.drop([i for i in dfs_train.columns if i in to_drop], axis=1)\n",
        "dfs_test = dfs_test.drop([i for i in dfs_test.columns if i in to_drop], axis=1)\n",
        "\n",
        "# Data Splitting\n",
        "\n",
        "y_train = dfs_train['label'].reset_index(drop=True)\n",
        "candidates_train = dfs_train[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "X_train = dfs_train.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "y_test = dfs_test['label'].reset_index(drop=True)\n",
        "candidates_test = dfs_test[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "X_test = dfs_test.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "# Normalization and balancing of data\n",
        "\n",
        "# Data for FOLD 1\n",
        "trainDataFull = pd.DataFrame(X_train).copy()\n",
        "ss_f1 = StandardScaler()\n",
        "trainDataFull = pd.DataFrame(ss_f1.fit_transform(trainDataFull), columns=X_train.columns)\n",
        "if usePCA:\n",
        "  # Make an instance of the Model\n",
        "  pca = PCA(.95)\n",
        "  trainDataFull = pd.DataFrame(pca.fit_transform(trainDataFull))\n",
        "\n",
        "trainDataFull['label'] = y_train\n",
        "dataBalanced, dfNegative = rus(trainDataFull)\n",
        "print(\"Data Balanced fold 1: \", len(dataBalanced))\n",
        "print(\"Negative pool fold 1: \",len(dfNegative))\n",
        "\n",
        "\n",
        "# Data for FOLD 2\n",
        "trainDataFull_2 = pd.DataFrame(X_test).copy()\n",
        "ss_f2 = StandardScaler()\n",
        "trainDataFull_2 = pd.DataFrame(ss_f2.fit_transform(trainDataFull_2), columns=X_test.columns)\n",
        "if usePCA:\n",
        "  # Make an instance of the Model\n",
        "  pca2 = PCA(.95)\n",
        "  trainDataFull_2 = pd.DataFrame(pca2.fit_transform(trainDataFull_2))    \n",
        "\n",
        "trainDataFull_2['label'] = y_test\n",
        "\n",
        "dataBalanced_2, dfNegative_2 = rus(trainDataFull_2)\n",
        "print(\"Data Balanced fold 2: \", len(dataBalanced_2))\n",
        "print(\"Negative pool fold 2: \",len(dfNegative_2))\n",
        "\n",
        "\n",
        "\n",
        "hyperOptBalanceData = dfs_hyperOpt.drop(['name',  'x', 'y'], axis=1)\n",
        "ss = StandardScaler()\n",
        "hyperOptBalanceDataScaled = pd.DataFrame(ss.fit_transform(hyperOptBalanceData.drop('label', axis=1)), columns=X_train.columns)\n",
        "if usePCA:\n",
        "  # Make an instance of the Model\n",
        "  pca_hp = PCA(.95)\n",
        "  hyperOptBalanceDataScaled = pd.DataFrame(pca_hp.fit_transform(hyperOptBalanceDataScaled))\n",
        "\n",
        "hyperOptBalanceDataScaled['label'] = hyperOptBalanceData['label'].values\n",
        "\n",
        "cvf1Score = []\n",
        "cvMccScore = []\n",
        "cvpAUCScore = []\n",
        "\n",
        "\n",
        "for classifier in classifiers:\n",
        "\n",
        "    print(\"Training new model...\")\n",
        "    print('******************** RESULTS FOLD 1 **********************')\n",
        "\n",
        "    prediction_set_1 = pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns)\n",
        "    if usePCA:\n",
        "      prediction_set_1 = pd.DataFrame(pca.transform(prediction_set_1))\n",
        "\n",
        "\n",
        "    clf, best_params, y_pred_proba, y_pred, f1_score_1 = negative_pool(dataBalanced.copy(), dfNegative.copy(), hyperOptBalanceDataScaled, \n",
        "                                                                      prediction_set_1, y_test, classifier, classifier in calibration_classifiers)\n",
        "    best_hyperparameters[classifier] = best_params\n",
        "\n",
        "    cvf1Score.append(f1_score(y_test, y_pred, average='binary'))\n",
        "    cvMccScore.append(matthews_corrcoef(y_test, y_pred))\n",
        "    cvpAUCScore.append(roc_auc_score(y_test, y_pred_proba, max_fpr = 0.0001))\n",
        "\n",
        "    fn_1, dfROC_1, filename_key_1 = generate_fold_results(candidates_test, normals, imageKeyName, clf, y_test, y_pred, y_pred_proba, 1, classifier)\n",
        "    \n",
        "    print('******************** RESULTS FOLD 2 **********************')\n",
        " \n",
        "    prediction_set_2 = pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns)\n",
        "    if usePCA:\n",
        "      prediction_set_2 = pd.DataFrame(pca2.transform(prediction_set_2))\n",
        "\n",
        "    clf2, best_params, y_pred_proba_2, y_pred_2, f1_score_2 = negative_pool(dataBalanced_2.copy(), dfNegative_2.copy(), hyperOptBalanceDataScaled, \n",
        "                                                                            prediction_set_2, y_train, classifier, classifier in calibration_classifiers, best_params)  \n",
        "    cvf1Score.append(f1_score(y_train, y_pred_2, average='binary'))\n",
        "    cvMccScore.append(matthews_corrcoef(y_train, y_pred_2))\n",
        "    cvpAUCScore.append(roc_auc_score(y_train, y_pred_proba_2, max_fpr = 0.0001))\n",
        "\n",
        "    fn_2, dfROC_2, filename_key_2 = generate_fold_results(candidates_train, normals, imageKeyName, clf2, y_train, y_pred_2, y_pred_proba_2, 2, classifier)\n",
        "\n",
        "\n",
        "    if np.array(cvf1Score).argmax() == 0:\n",
        "      best_f1 = cvf1Score[0]\n",
        "      best_mcc = cvMccScore[0]\n",
        "      best_pAUC = cvpAUCScore[0]\n",
        "\n",
        "    else:\n",
        "      best_f1 = cvf1Score[1]\n",
        "      best_mcc = cvMccScore[1]\n",
        "      best_pAUC = cvpAUCScore[1]\n",
        "\n",
        "    print(\"cvf1Score: \", cvf1Score)\n",
        "\n",
        "    row_results = [classifier, np.mean(cvpAUCScore), np.mean(cvf1Score), np.mean(cvMccScore), best_pAUC, best_f1, best_mcc, np.array(cvf1Score).argmax()+1, best_params]\n",
        "    results_df = results_df.append(pd.DataFrame([row_results], columns=[\"model\", \"pAUC_mean\", \"f1_mean\", \"mcc_mean\", \"pAUC\", \"f1\", \"mcc\", \"selectedFold\", \"parameters\"]))\n",
        "    results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "\n",
        "    dfROC_1 = dfROC_1.append(dfROC_2) \n",
        "\n",
        "    print(len(dfROC_1.prob.unique()))\n",
        "    draw_curve(fn_1+fn_2, len(normals), dfROC_1, f'FROC_curve_{classifier}_pip{pipeFolder}_fn{fn_1+fn_2}_normals{len(normals)}_negative_pool_scaled', classifier)\n",
        "\n",
        "    del clf, y_pred_proba, y_pred, f1_score_1, clf2, y_pred_proba_2, y_pred_2, f1_score_2, best_f1, best_mcc, best_pAUC, dfROC_1, dfROC_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehu9MRsAtM02"
      },
      "outputs": [],
      "source": [
        "# if __name__== \"__main__\":\n",
        "#   main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWUpDyxbzvYi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ML_structure_project_ScaledData_Final.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}