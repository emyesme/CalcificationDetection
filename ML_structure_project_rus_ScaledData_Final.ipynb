{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wthu5HssWsF3"
      },
      "source": [
        "\n",
        "# Load the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dd477p2erqy",
        "outputId": "36651c86-d152-4d99-fcef-6aa3bd9ab665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDRr9M44sYxK",
        "outputId": "bb0fc433-0a9c-4ca0-ef96-793692498b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['20586908_6c613a14b80a8591_MG_R_CC_ANON__fos_glcm.csv', '20586934_6c613a14b80a8591_MG_L_CC_ANON__fos_glcm.csv', '20586960_6c613a14b80a8591_MG_R_ML_ANON__fos_glcm.csv', '20586986_6c613a14b80a8591_MG_L_ML_ANON__fos_glcm.csv', '20587054_b6a4f750c6df4f90_MG_R_CC_ANON__fos_glcm.csv', '20587080_b6a4f750c6df4f90_MG_R_ML_ANON__fos_glcm.csv', '20587148_fd746d25eb40b3dc_MG_R_CC_ANON__fos_glcm.csv', '20587174_fd746d25eb40b3dc_MG_L_CC_ANON__fos_glcm.csv', '20587200_fd746d25eb40b3dc_MG_R_ML_ANON__fos_glcm.csv', '20587226_fd746d25eb40b3dc_MG_L_ML_ANON__fos_glcm.csv', '20587294_e634830794f5c1bd_MG_R_CC_ANON__fos_glcm.csv', '20587320_e634830794f5c1bd_MG_L_CC_ANON__fos_glcm.csv', '20587346_e634830794f5c1bd_MG_R_ML_ANON__fos_glcm.csv', '20587372_e634830794f5c1bd_MG_L_ML_ANON__fos_glcm.csv', '20587466_d571b5880ad2a016_MG_L_CC_ANON__fos_glcm.csv', '20587492_d571b5880ad2a016_MG_R_ML_ANON__fos_glcm.csv', '20587518_d571b5880ad2a016_MG_L_ML_ANON__fos_glcm.csv', '20587544_d571b5880ad2a016_MG_R_CC_ANON__fos_glcm.csv', '20587612_f4b2d377f43ba0bd_MG_R_CC_ANON__fos_glcm.csv', '20587638_f4b2d377f43ba0bd_MG_L_CC_ANON__fos_glcm.csv', '20587664_f4b2d377f43ba0bd_MG_R_ML_ANON__fos_glcm.csv', '20587690_f4b2d377f43ba0bd_MG_L_ML_ANON__fos_glcm.csv', '20587758_81cd83d2f4d78528_MG_L_CC_ANON__fos_glcm.csv', '20587784_81cd83d2f4d78528_MG_R_ML_ANON__fos_glcm.csv', '20587810_81cd83d2f4d78528_MG_L_ML_ANON__fos_glcm.csv', '20587836_81cd83d2f4d78528_MG_R_CC_ANON__fos_glcm.csv', '20587902_8dbbd4e51f549ff0_MG_R_CC_ANON__fos_glcm.csv', '20587928_8dbbd4e51f549ff0_MG_R_ML_ANON__fos_glcm.csv', '20587994_024ee3569b2605dc_MG_R_CC_ANON__fos_glcm.csv', '20588020_024ee3569b2605dc_MG_L_CC_ANON__fos_glcm.csv', '20588046_024ee3569b2605dc_MG_R_ML_ANON__fos_glcm.csv', '20588072_024ee3569b2605dc_MG_L_ML_ANON__fos_glcm.csv', '20588138_8d0b9620c53c0268_MG_R_ML_ANON__fos_glcm.csv', '20588164_8d0b9620c53c0268_MG_R_CC_ANON__fos_glcm.csv', '20588190_8d0b9620c53c0268_MG_L_CC_ANON__fos_glcm.csv', '20588216_8d0b9620c53c0268_MG_L_ML_ANON__fos_glcm.csv', '20588308_493155e17143edef_MG_L_ML_ANON__fos_glcm.csv', '20588334_493155e17143edef_MG_L_CC_ANON__fos_glcm.csv', '20588458_bf1a6aaadb05e3df_MG_R_CC_ANON__fos_glcm.csv', '20588510_bf1a6aaadb05e3df_MG_R_ML_ANON__fos_glcm.csv', '20588536_bf1a6aaadb05e3df_MG_L_ML_ANON__fos_glcm.csv', '20588562_bf1a6aaadb05e3df_MG_L_CC_ANON__fos_glcm.csv', '20588654_036aff49b8ac84f0_MG_R_ML_ANON__fos_glcm.csv', '20588680_036aff49b8ac84f0_MG_L_ML_ANON__fos_glcm.csv', '22427682_d713ef5849f98b6c_MG_R_CC_ANON__fos_glcm.csv', '22427705_d713ef5849f98b6c_MG_L_CC_ANON__fos_glcm.csv', '22427728_d713ef5849f98b6c_MG_R_ML_ANON__fos_glcm.csv', '22427751_d713ef5849f98b6c_MG_L_ML_ANON__fos_glcm.csv', '22427840_bbd6a3a35438c11b_MG_R_CC_ANON__fos_glcm.csv', '22427864_bbd6a3a35438c11b_MG_L_CC_ANON__fos_glcm.csv', '22579730_bbd6a3a35438c11b_MG_R_ML_ANON__fos_glcm.csv', '22579754_bbd6a3a35438c11b_MG_L_ML_ANON__fos_glcm.csv', '22579847_301f1776aebbf5d2_MG_R_CC_ANON__fos_glcm.csv', '22579870_301f1776aebbf5d2_MG_L_CC_ANON__fos_glcm.csv', '22579893_301f1776aebbf5d2_MG_R_ML_ANON__fos_glcm.csv', '22579916_301f1776aebbf5d2_MG_L_ML_ANON__fos_glcm.csv', '22580015_6200187f3f1ccc18_MG_R_CC_ANON__fos_glcm.csv', '22580038_6200187f3f1ccc18_MG_L_CC_ANON__fos_glcm.csv', '22580068_6200187f3f1ccc18_MG_R_ML_ANON__fos_glcm.csv', '22580098_6200187f3f1ccc18_MG_L_ML_ANON__fos_glcm.csv', '22580192_5530d5782fc89dd7_MG_R_CC_ANON__fos_glcm.csv', '22580218_5530d5782fc89dd7_MG_L_CC_ANON__fos_glcm.csv', '22580244_5530d5782fc89dd7_MG_R_ML_ANON__fos_glcm.csv', '22580270_5530d5782fc89dd7_MG_L_ML_ANON__fos_glcm.csv', '22580341_5eae9beae14d26fd_MG_R_CC_ANON__fos_glcm.csv', '22580367_5eae9beae14d26fd_MG_L_CC_ANON__fos_glcm.csv', '22580393_5eae9beae14d26fd_MG_R_ML_ANON__fos_glcm.csv', '22580419_5eae9beae14d26fd_MG_L_ML_ANON__fos_glcm.csv', '22580492_2a5b932da4ce5ca1_MG_R_CC_ANON__fos_glcm.csv', '22580520_2a5b932da4ce5ca1_MG_L_CC_ANON__fos_glcm.csv', '22580548_2a5b932da4ce5ca1_MG_R_ML_ANON__fos_glcm.csv', '22580576_2a5b932da4ce5ca1_MG_L_ML_ANON__fos_glcm.csv', '22580654_fe7d005dcbbfb46d_MG_R_CC_ANON__fos_glcm.csv', '22580680_fe7d005dcbbfb46d_MG_L_CC_ANON__fos_glcm.csv', '22580706_fe7d005dcbbfb46d_MG_R_ML_ANON__fos_glcm.csv', '22580732_fe7d005dcbbfb46d_MG_L_ML_ANON__fos_glcm.csv', '22613624_dcafa6ba6374ec07_MG_R_CC_ANON__fos_glcm.csv', '22613650_dcafa6ba6374ec07_MG_L_CC_ANON__fos_glcm.csv', '22613676_dcafa6ba6374ec07_MG_R_ML_ANON__fos_glcm.csv', '22613702_dcafa6ba6374ec07_MG_L_ML_ANON__fos_glcm.csv', '22613770_45c7f44839fd9e68_MG_R_CC_ANON__fos_glcm.csv', '22613796_45c7f44839fd9e68_MG_L_CC_ANON__fos_glcm.csv', '22613822_45c7f44839fd9e68_MG_R_ML_ANON__fos_glcm.csv', '22613848_45c7f44839fd9e68_MG_L_ML_ANON__fos_glcm.csv', '22613918_f23fa352e7de3dc7_MG_R_CC_ANON__fos_glcm.csv', '22613944_f23fa352e7de3dc7_MG_L_CC_ANON__fos_glcm.csv', '22613970_f23fa352e7de3dc7_MG_R_ML_ANON__fos_glcm.csv', '22613996_f23fa352e7de3dc7_MG_L_ML_ANON__fos_glcm.csv', '22614074_6bd24a0a42c19ce1_MG_R_CC_ANON__fos_glcm.csv', '22614097_6bd24a0a42c19ce1_MG_L_CC_ANON__fos_glcm.csv', '22614127_6bd24a0a42c19ce1_MG_R_ML_ANON__fos_glcm.csv', '22614150_6bd24a0a42c19ce1_MG_L_ML_ANON__fos_glcm.csv', '22614236_1e5c3af078f74b05_MG_L_CC_ANON__fos_glcm.csv', '22614266_1e5c3af078f74b05_MG_L_ML_ANON__fos_glcm.csv', '22614353_d065adcb9905b973_MG_R_CC_ANON__fos_glcm.csv', '22614379_d065adcb9905b973_MG_L_CC_ANON__fos_glcm.csv', '22614405_d065adcb9905b973_MG_R_ML_ANON__fos_glcm.csv', '22614431_d065adcb9905b973_MG_L_ML_ANON__fos_glcm.csv', '22614499_2dec4948fbe6336d_MG_R_CC_ANON__fos_glcm.csv', '22614522_2dec4948fbe6336d_MG_L_CC_ANON__fos_glcm.csv', '22614545_2dec4948fbe6336d_MG_R_ML_ANON__fos_glcm.csv', '22614568_2dec4948fbe6336d_MG_L_ML_ANON__fos_glcm.csv', '22670094_e1f51192f7bf3f5f_MG_R_CC_ANON__fos_glcm.csv', '22670124_e1f51192f7bf3f5f_MG_L_CC_ANON__fos_glcm.csv', '22670147_e1f51192f7bf3f5f_MG_R_ML_ANON__fos_glcm.csv', '22670177_e1f51192f7bf3f5f_MG_L_ML_ANON__fos_glcm.csv', '22670278_98429c0bdf78c0c7_MG_R_CC_ANON__fos_glcm.csv', '22670301_98429c0bdf78c0c7_MG_L_CC_ANON__fos_glcm.csv', '22670324_98429c0bdf78c0c7_MG_R_ML_ANON__fos_glcm.csv', '22670347_98429c0bdf78c0c7_MG_L_ML_ANON__fos_glcm.csv', '22670442_7e677f3d530e41ed_MG_R_CC_ANON__fos_glcm.csv', '22670465_7e677f3d530e41ed_MG_L_CC_ANON__fos_glcm.csv', '22670488_7e677f3d530e41ed_MG_R_ML_ANON__fos_glcm.csv', '22670511_7e677f3d530e41ed_MG_L_ML_ANON__fos_glcm.csv', '22670620_e15a16f87b4f9782_MG_R_CC_ANON__fos_glcm.csv', '22670643_e15a16f87b4f9782_MG_L_CC_ANON__fos_glcm.csv', '22670673_e15a16f87b4f9782_MG_R_ML_ANON__fos_glcm.csv', '22670703_e15a16f87b4f9782_MG_L_ML_ANON__fos_glcm.csv', '22670809_0b7396cdccacca82_MG_R_CC_ANON__fos_glcm.csv', '22670832_0b7396cdccacca82_MG_L_CC_ANON__fos_glcm.csv', '22670855_0b7396cdccacca82_MG_R_ML_ANON__fos_glcm.csv', '22670878_0b7396cdccacca82_MG_L_ML_ANON__fos_glcm.csv', '22670978_f571fd4e63c718e3_MG_L_CC_ANON__fos_glcm.csv', '22671003_f571fd4e63c718e3_MG_L_ML_ANON__fos_glcm.csv', '22678449_60995d51033e24b8_MG_R_CC_ANON__fos_glcm.csv', '22678472_60995d51033e24b8_MG_L_CC_ANON__fos_glcm.csv', '22678495_60995d51033e24b8_MG_R_ML_ANON__fos_glcm.csv', '22678518_60995d51033e24b8_MG_L_ML_ANON__fos_glcm.csv', '22678622_61b13c59bcba149e_MG_R_CC_ANON__fos_glcm.csv', '22678646_61b13c59bcba149e_MG_L_CC_ANON__fos_glcm.csv', '22678670_61b13c59bcba149e_MG_R_ML_ANON__fos_glcm.csv', '22678694_61b13c59bcba149e_MG_L_ML_ANON__fos_glcm.csv', '22678787_64a22c47765f0c5c_MG_R_CC_ANON__fos_glcm.csv', '22678810_64a22c47765f0c5c_MG_L_CC_ANON__fos_glcm.csv', '22678833_64a22c47765f0c5c_MG_R_ML_ANON__fos_glcm.csv', '22678856_64a22c47765f0c5c_MG_L_ML_ANON__fos_glcm.csv', '22678953_b9a4da5f2dae63a9_MG_R_CC_ANON__fos_glcm.csv', '22678980_b9a4da5f2dae63a9_MG_L_CC_ANON__fos_glcm.csv', '22679008_b9a4da5f2dae63a9_MG_R_ML_ANON__fos_glcm.csv', '22679036_b9a4da5f2dae63a9_MG_L_ML_ANON__fos_glcm.csv', '24054997_2f1104b3cda7f145_MG_L_ML_ANON__fos_glcm.csv', '24055024_2f1104b3cda7f145_MG_R_ML_ANON__fos_glcm.csv', '24055051_2f1104b3cda7f145_MG_L_CC_ANON__fos_glcm.csv', '24055078_2f1104b3cda7f145_MG_R_CC_ANON__fos_glcm.csv', '24055149_606e9b184978a350_MG_L_ML_ANON__fos_glcm.csv', '24055176_606e9b184978a350_MG_R_ML_ANON__fos_glcm.csv', '24055203_606e9b184978a350_MG_L_CC_ANON__fos_glcm.csv', '24055274_1e10aef17c9fe149_MG_L_ML_ANON__fos_glcm.csv', '24055328_1e10aef17c9fe149_MG_R_ML_ANON__fos_glcm.csv', '24055355_1e10aef17c9fe149_MG_L_CC_ANON__fos_glcm.csv', '24055382_1e10aef17c9fe149_MG_R_CC_ANON__fos_glcm.csv', '24055445_ac3185e18ffdc7b6_MG_L_ML_ANON__fos_glcm.csv', '24055464_ac3185e18ffdc7b6_MG_R_ML_ANON__fos_glcm.csv', '24055483_ac3185e18ffdc7b6_MG_L_CC_ANON__fos_glcm.csv', '24055502_ac3185e18ffdc7b6_MG_R_CC_ANON__fos_glcm.csv', '24055573_6f1aef40b3775182_MG_L_ML_ANON__fos_glcm.csv', '24055600_6f1aef40b3775182_MG_R_ML_ANON__fos_glcm.csv', '24055627_6f1aef40b3775182_MG_L_CC_ANON__fos_glcm.csv', '24055654_6f1aef40b3775182_MG_R_CC_ANON__fos_glcm.csv', '24055725_f0f1a133837b5137_MG_L_ML_ANON__fos_glcm.csv', '24055752_f0f1a133837b5137_MG_R_ML_ANON__fos_glcm.csv', '24055779_f0f1a133837b5137_MG_L_CC_ANON__fos_glcm.csv', '24055806_f0f1a133837b5137_MG_R_CC_ANON__fos_glcm.csv', '24055877_839819f2eadaf325_MG_L_ML_ANON__fos_glcm.csv', '24055904_839819f2eadaf325_MG_R_ML_ANON__fos_glcm.csv', '24055931_839819f2eadaf325_MG_L_CC_ANON__fos_glcm.csv', '24055958_839819f2eadaf325_MG_R_CC_ANON__fos_glcm.csv', '24058660_9e8db9e34d5275ef_MG_R_CC_ANON__fos_glcm.csv', '24058686_9e8db9e34d5275ef_MG_L_CC_ANON__fos_glcm.csv', '24058712_9e8db9e34d5275ef_MG_R_ML_ANON__fos_glcm.csv', '24058738_9e8db9e34d5275ef_MG_L_ML_ANON__fos_glcm.csv', '24065251_c4b995eddb3c510c_MG_L_ML_ANON__fos_glcm.csv', '24065270_c4b995eddb3c510c_MG_R_ML_ANON__fos_glcm.csv', '24065289_c4b995eddb3c510c_MG_L_CC_ANON__fos_glcm.csv', '24065308_c4b995eddb3c510c_MG_R_CC_ANON__fos_glcm.csv', '24065380_83db89f57aea498a_MG_L_ML_ANON__fos_glcm.csv', '24065407_83db89f57aea498a_MG_R_ML_ANON__fos_glcm.csv', '24065434_83db89f57aea498a_MG_L_CC_ANON__fos_glcm.csv', '24065461_83db89f57aea498a_MG_R_CC_ANON__fos_glcm.csv', '24065530_d8205a09c8173f44_MG_L_ML_ANON__fos_glcm.csv', '24065557_d8205a09c8173f44_MG_R_ML_ANON__fos_glcm.csv', '24065584_d8205a09c8173f44_MG_L_CC_ANON__fos_glcm.csv', '24065611_d8205a09c8173f44_MG_R_CC_ANON__fos_glcm.csv', '24065680_5291e1aee2bbf5df_MG_L_ML_ANON__fos_glcm.csv', '24065707_5291e1aee2bbf5df_MG_R_ML_ANON__fos_glcm.csv', '24065734_5291e1aee2bbf5df_MG_L_CC_ANON__fos_glcm.csv', '24065761_5291e1aee2bbf5df_MG_R_CC_ANON__fos_glcm.csv', '24065833_c01f83a1eb283270_MG_L_ML_ANON__fos_glcm.csv', '24065860_c01f83a1eb283270_MG_R_ML_ANON__fos_glcm.csv', '24065887_c01f83a1eb283270_MG_L_CC_ANON__fos_glcm.csv', '24065914_c01f83a1eb283270_MG_R_CC_ANON__fos_glcm.csv', '26933772_f8bfddc28e8045c0_MG_R_CC_ANON__fos_glcm.csv', '26933801_f8bfddc28e8045c0_MG_L_CC_ANON__fos_glcm.csv', '26933830_f8bfddc28e8045c0_MG_R_ML_ANON__fos_glcm.csv', '26933859_f8bfddc28e8045c0_MG_L_ML_ANON__fos_glcm.csv', '27829134_fbb55bf7fff48540_MG_R_CC_ANON__fos_glcm.csv', '27829161_fbb55bf7fff48540_MG_L_CC_ANON__fos_glcm.csv', '27829188_fbb55bf7fff48540_MG_R_ML_ANON__fos_glcm.csv', '27829215_fbb55bf7fff48540_MG_L_ML_ANON__fos_glcm.csv', '30011484_349323117bf0fd93_MG_R_CC_ANON__fos_glcm.csv', '30011507_349323117bf0fd93_MG_L_CC_ANON__fos_glcm.csv', '30011530_349323117bf0fd93_MG_R_ML_ANON__fos_glcm.csv', '30011553_349323117bf0fd93_MG_L_ML_ANON__fos_glcm.csv', '30011647_6968748e66837bc7_MG_R_CC_ANON__fos_glcm.csv', '30011674_6968748e66837bc7_MG_L_CC_ANON__fos_glcm.csv', '30011700_6968748e66837bc7_MG_R_ML_ANON__fos_glcm.csv', '30011727_6968748e66837bc7_MG_L_ML_ANON__fos_glcm.csv', '30011798_4f20c1285d8f0b1f_MG_R_CC_ANON__fos_glcm.csv', '30011824_4f20c1285d8f0b1f_MG_L_CC_ANON__fos_glcm.csv', '30011850_4f20c1285d8f0b1f_MG_R_ML_ANON__fos_glcm.csv', '30318067_4f20c1285d8f0b1f_MG_L_ML_ANON__fos_glcm.csv', '50993399_5d85ecc9cf26b254_MG_L_ML_ANON__fos_glcm.csv', '50993426_5d85ecc9cf26b254_MG_L_CC_ANON__fos_glcm.csv', '50993616_b03f1dd34eb3c55f_MG_L_ML_ANON__fos_glcm.csv', '50993643_b03f1dd34eb3c55f_MG_L_CC_ANON__fos_glcm.csv', '50993670_b03f1dd34eb3c55f_MG_L_ML_ANON__fos_glcm.csv', '50993697_b03f1dd34eb3c55f_MG_L_CC_ANON__fos_glcm.csv', '50993787_de5e8d61e501a71b_MG_L_ML_ANON__fos_glcm.csv', '50993814_de5e8d61e501a71b_MG_R_ML_ANON__fos_glcm.csv', '50993841_de5e8d61e501a71b_MG_L_CC_ANON__fos_glcm.csv', '50993868_de5e8d61e501a71b_MG_R_CC_ANON__fos_glcm.csv', '50993895_de5e8d61e501a71b_MG_L_ML_ANON__fos_glcm.csv', '50993922_de5e8d61e501a71b_MG_R_ML_ANON__fos_glcm.csv', '50993949_de5e8d61e501a71b_MG_L_CC_ANON__fos_glcm.csv', '50993976_de5e8d61e501a71b_MG_R_CC_ANON__fos_glcm.csv', '50994110_cc9e66c5b31baab8_MG_L_ML_ANON__fos_glcm.csv', '50994137_cc9e66c5b31baab8_MG_R_ML_ANON__fos_glcm.csv', '50994164_cc9e66c5b31baab8_MG_L_CC_ANON__fos_glcm.csv', '50994191_cc9e66c5b31baab8_MG_R_CC_ANON__fos_glcm.csv', '50994273_cc9e66c5b31baab8_MG_R_CC_ANON__fos_glcm.csv', '50994300_cc9e66c5b31baab8_MG_R_FB_ANON__fos_glcm.csv', '50994327_cc9e66c5b31baab8_MG_L_ML_ANON__fos_glcm.csv', '50994354_cc9e66c5b31baab8_MG_R_ML_ANON__fos_glcm.csv', '50994381_cc9e66c5b31baab8_MG_L_CC_ANON__fos_glcm.csv', '50994408_cc9e66c5b31baab8_MG_R_CC_ANON__fos_glcm.csv', '50994535_de4c34099d6ef8de_MG_R_ML_ANON__fos_glcm.csv', '50994562_de4c34099d6ef8de_MG_R_CC_ANON__fos_glcm.csv', '50994589_de4c34099d6ef8de_MG_R_ML_ANON__fos_glcm.csv', '50994616_de4c34099d6ef8de_MG_R_CC_ANON__fos_glcm.csv', '50994706_069212ec65a94339_MG_R_CC_ANON__fos_glcm.csv', '50994733_069212ec65a94339_MG_L_ML_ANON__fos_glcm.csv', '50994760_069212ec65a94339_MG_R_ML_ANON__fos_glcm.csv', '50994787_069212ec65a94339_MG_L_CC_ANON__fos_glcm.csv', '50994814_069212ec65a94339_MG_L_ML_ANON__fos_glcm.csv', '50994841_069212ec65a94339_MG_R_ML_ANON__fos_glcm.csv', '50994868_069212ec65a94339_MG_L_CC_ANON__fos_glcm.csv', '50994895_069212ec65a94339_MG_R_CC_ANON__fos_glcm.csv', '50995762_0c735e8768d276b4_MG_R_ML_ANON__fos_glcm.csv', '50995789_0c735e8768d276b4_MG_R_CC_ANON__fos_glcm.csv', '50995872_c94d8a1ebd452afe_MG_L_ML_ANON__fos_glcm.csv', '50995899_c94d8a1ebd452afe_MG_L_CC_ANON__fos_glcm.csv', '50995963_d742ec2f9b90aa62_MG_L_ML_ANON__fos_glcm.csv', '50995990_d742ec2f9b90aa62_MG_L_CC_ANON__fos_glcm.csv', '50996056_71c1a60d57c5322f_MG_L_ML_ANON__fos_glcm.csv', '50996083_71c1a60d57c5322f_MG_R_ML_ANON__fos_glcm.csv', '50996110_71c1a60d57c5322f_MG_L_CC_ANON__fos_glcm.csv', '50996137_71c1a60d57c5322f_MG_R_CC_ANON__fos_glcm.csv', '50996201_8c1b2bd64ca4d778_MG_L_ML_ANON__fos_glcm.csv', '50996228_8c1b2bd64ca4d778_MG_L_CC_ANON__fos_glcm.csv', '50996325_6aba0b402889a16f_MG_L_ML_ANON__fos_glcm.csv', '50996352_6aba0b402889a16f_MG_R_ML_ANON__fos_glcm.csv', '50996379_6aba0b402889a16f_MG_L_CC_ANON__fos_glcm.csv', '50996406_6aba0b402889a16f_MG_R_CC_ANON__fos_glcm.csv', '50996709_330e5fe16929eed4_MG_R_ML_ANON__fos_glcm.csv', '50996736_330e5fe16929eed4_MG_R_CC_ANON__fos_glcm.csv', '50996800_fdf4a1516f88b280_MG_L_ML_ANON__fos_glcm.csv', '50996827_fdf4a1516f88b280_MG_R_ML_ANON__fos_glcm.csv', '50996854_fdf4a1516f88b280_MG_L_CC_ANON__fos_glcm.csv', '50996881_fdf4a1516f88b280_MG_R_CC_ANON__fos_glcm.csv', '50996945_ce5e5e18a261cd29_MG_L_ML_ANON__fos_glcm.csv', '50996972_ce5e5e18a261cd29_MG_R_ML_ANON__fos_glcm.csv', '50996999_ce5e5e18a261cd29_MG_L_CC_ANON__fos_glcm.csv', '50997026_ce5e5e18a261cd29_MG_R_CC_ANON__fos_glcm.csv', '50997053_ce5e5e18a261cd29_MG_L_CC_ANON__fos_glcm.csv', '50997080_ce5e5e18a261cd29_MG_L_ML_ANON__fos_glcm.csv', '50997107_ce5e5e18a261cd29_MG_R_ML_ANON__fos_glcm.csv', '50997134_ce5e5e18a261cd29_MG_R_CC_ANON__fos_glcm.csv', '50997223_9054942f7be52dd9_MG_L_ML_ANON__fos_glcm.csv', '50997250_9054942f7be52dd9_MG_R_ML_ANON__fos_glcm.csv', '50997277_9054942f7be52dd9_MG_L_CC_ANON__fos_glcm.csv', '50997304_9054942f7be52dd9_MG_R_CC_ANON__fos_glcm.csv', '50997434_97ec8cadfca70d32_MG_L_ML_ANON__fos_glcm.csv', '50997461_97ec8cadfca70d32_MG_R_ML_ANON__fos_glcm.csv', '50997488_97ec8cadfca70d32_MG_L_CC_ANON__fos_glcm.csv', '50997515_97ec8cadfca70d32_MG_R_CC_ANON__fos_glcm.csv', '50997597_67cc8c9939d74a9a_MG_L_ML_ANON__fos_glcm.csv', '50997624_67cc8c9939d74a9a_MG_R_ML_ANON__fos_glcm.csv', '50997651_67cc8c9939d74a9a_MG_L_CC_ANON__fos_glcm.csv', '50997678_67cc8c9939d74a9a_MG_R_CC_ANON__fos_glcm.csv', '50997742_cbb6c98a81e69eeb_MG_L_ML_ANON__fos_glcm.csv', '50997769_cbb6c98a81e69eeb_MG_R_ML_ANON__fos_glcm.csv', '50997796_cbb6c98a81e69eeb_MG_L_CC_ANON__fos_glcm.csv', '50997823_cbb6c98a81e69eeb_MG_R_CC_ANON__fos_glcm.csv', '50998032_66adfbb4f19c76d2_MG_R_CC_ANON__fos_glcm.csv', '50998059_66adfbb4f19c76d2_MG_L_ML_ANON__fos_glcm.csv', '50998086_66adfbb4f19c76d2_MG_R_ML_ANON__fos_glcm.csv', '50998113_66adfbb4f19c76d2_MG_L_CC_ANON__fos_glcm.csv', '50998177_f34ee0ab6591b792_MG_L_ML_ANON__fos_glcm.csv', '50998204_f34ee0ab6591b792_MG_R_ML_ANON__fos_glcm.csv', '50998231_f34ee0ab6591b792_MG_L_CC_ANON__fos_glcm.csv', '50998258_f34ee0ab6591b792_MG_R_CC_ANON__fos_glcm.csv', '50998322_1e4b534393d18753_MG_R_ML_ANON__fos_glcm.csv', '50998349_1e4b534393d18753_MG_R_CC_ANON__fos_glcm.csv', '50998413_1f139436acfc5467_MG_L_ML_ANON__fos_glcm.csv', '50998440_1f139436acfc5467_MG_R_ML_ANON__fos_glcm.csv', '50998467_1f139436acfc5467_MG_L_CC_ANON__fos_glcm.csv', '50998494_1f139436acfc5467_MG_R_CC_ANON__fos_glcm.csv', '50998580_cd12bc20b3d27d0b_MG_L_ML_ANON__fos_glcm.csv', '50998607_cd12bc20b3d27d0b_MG_R_ML_ANON__fos_glcm.csv', '50998634_cd12bc20b3d27d0b_MG_L_CC_ANON__fos_glcm.csv', '50998661_cd12bc20b3d27d0b_MG_R_CC_ANON__fos_glcm.csv', '50998981_a78eba834ef6ee88_MG_R_ML_ANON__fos_glcm.csv', '50999008_a78eba834ef6ee88_MG_R_CC_ANON__fos_glcm.csv', '50999094_cb65e8dac169f596_MG_L_ML_ANON__fos_glcm.csv', '50999121_cb65e8dac169f596_MG_R_ML_ANON__fos_glcm.csv', '50999148_cb65e8dac169f596_MG_L_CC_ANON__fos_glcm.csv', '50999175_cb65e8dac169f596_MG_R_CC_ANON__fos_glcm.csv', '50999246_cb65e8dac169f596_MG_L_ML_ANON__fos_glcm.csv', '50999273_cb65e8dac169f596_MG_R_ML_ANON__fos_glcm.csv', '50999300_cb65e8dac169f596_MG_L_CC_ANON__fos_glcm.csv', '50999327_cb65e8dac169f596_MG_R_CC_ANON__fos_glcm.csv', '50999432_f62fbf38fb208316_MG_L_ML_ANON__fos_glcm.csv', '50999459_f62fbf38fb208316_MG_L_CC_ANON__fos_glcm.csv', '51048738_3f22cdda8da215e3_MG_R_ML_ANON__fos_glcm.csv', '51048765_3f22cdda8da215e3_MG_R_CC_ANON__fos_glcm.csv', '51048891_f3e93e889a7746f0_MG_L_ML_ANON__fos_glcm.csv', '51048918_f3e93e889a7746f0_MG_R_ML_ANON__fos_glcm.csv', '51048945_f3e93e889a7746f0_MG_L_CC_ANON__fos_glcm.csv', '51048972_f3e93e889a7746f0_MG_R_CC_ANON__fos_glcm.csv', '51049053_8c105bb715bf1c3c_MG_L_ML_ANON__fos_glcm.csv', '51049080_8c105bb715bf1c3c_MG_R_ML_ANON__fos_glcm.csv', '51049107_8c105bb715bf1c3c_MG_L_CC_ANON__fos_glcm.csv', '51049134_8c105bb715bf1c3c_MG_R_CC_ANON__fos_glcm.csv', '51049249_832ebce700241036_MG_L_CC_ANON__fos_glcm.csv', '51049276_832ebce700241036_MG_L_ML_ANON__fos_glcm.csv', '51049462_6f64793857feb5d0_MG_L_ML_ANON__fos_glcm.csv', '51049489_6f64793857feb5d0_MG_R_ML_ANON__fos_glcm.csv', '51049516_6f64793857feb5d0_MG_L_CC_ANON__fos_glcm.csv', '51049543_6f64793857feb5d0_MG_R_CC_ANON__fos_glcm.csv', '51049628_6f64793857feb5d0_MG_L_ML_ANON__fos_glcm.csv', '51049655_6f64793857feb5d0_MG_R_ML_ANON__fos_glcm.csv', '51049682_6f64793857feb5d0_MG_L_CC_ANON__fos_glcm.csv', '51070197_6f64793857feb5d0_MG_R_CC_ANON__fos_glcm.csv', '53580611_40e22f2e3215b954_MG_L_ML_ANON__fos_glcm.csv', '53580638_40e22f2e3215b954_MG_R_ML_ANON__fos_glcm.csv', '53580665_40e22f2e3215b954_MG_L_CC_ANON__fos_glcm.csv', '53580692_40e22f2e3215b954_MG_R_CC_ANON__fos_glcm.csv', '53580804_51bec6477a7898b9_MG_L_ML_ANON__fos_glcm.csv', '53580831_51bec6477a7898b9_MG_R_ML_ANON__fos_glcm.csv', '53580858_51bec6477a7898b9_MG_L_CC_ANON__fos_glcm.csv', '53580885_51bec6477a7898b9_MG_R_CC_ANON__fos_glcm.csv', '53580979_4c341dad22471922_MG_L_ML_ANON__fos_glcm.csv', '53581006_4c341dad22471922_MG_R_ML_ANON__fos_glcm.csv', '53581033_4c341dad22471922_MG_L_CC_ANON__fos_glcm.csv', '53581060_4c341dad22471922_MG_R_CC_ANON__fos_glcm.csv', '53581124_3be876aecfaad4ca_MG_L_ML_ANON__fos_glcm.csv', '53581151_3be876aecfaad4ca_MG_L_CC_ANON__fos_glcm.csv', '53581237_80123a24997098dc_MG_R_ML_ANON__fos_glcm.csv', '53581264_80123a24997098dc_MG_R_CC_ANON__fos_glcm.csv', '53581379_b231a8ba4dd4214f_MG_L_ML_ANON__fos_glcm.csv', '53581406_b231a8ba4dd4214f_MG_R_ML_ANON__fos_glcm.csv', '53581433_b231a8ba4dd4214f_MG_L_CC_ANON__fos_glcm.csv', '53581460_b231a8ba4dd4214f_MG_R_CC_ANON__fos_glcm.csv', '53581769_573747ee33ef6e5a_MG_L_ML_ANON__fos_glcm.csv', '53581796_573747ee33ef6e5a_MG_L_CC_ANON__fos_glcm.csv', '53581860_21e6cc12630e5e9f_MG_L_ML_ANON__fos_glcm.csv', '53581887_21e6cc12630e5e9f_MG_R_ML_ANON__fos_glcm.csv', '53581914_21e6cc12630e5e9f_MG_L_CC_ANON__fos_glcm.csv', '53581941_21e6cc12630e5e9f_MG_R_CC_ANON__fos_glcm.csv', '53582304_8913a7e0cf3bd74e_MG_R_ML_ANON__fos_glcm.csv', '53582331_8913a7e0cf3bd74e_MG_R_CC_ANON__fos_glcm.csv', '53582395_3f0db31711fc9795_MG_L_ML_ANON__fos_glcm.csv', '53582422_3f0db31711fc9795_MG_R_ML_ANON__fos_glcm.csv', '53582449_3f0db31711fc9795_MG_L_CC_ANON__fos_glcm.csv', '53582476_3f0db31711fc9795_MG_R_CC_ANON__fos_glcm.csv', '53582540_3e73f1c0670cfb0a_MG_R_ML_ANON__fos_glcm.csv', '53582567_3e73f1c0670cfb0a_MG_R_CC_ANON__fos_glcm.csv', '53582656_465aa5ec1b59efc6_MG_L_ML_ANON__fos_glcm.csv', '53582683_465aa5ec1b59efc6_MG_L_CC_ANON__fos_glcm.csv', '53582710_465aa5ec1b59efc6_MG_R_ML_ANON__fos_glcm.csv', '53582737_465aa5ec1b59efc6_MG_L_ML_ANON__fos_glcm.csv', '53582764_465aa5ec1b59efc6_MG_R_ML_ANON__fos_glcm.csv', '53582791_465aa5ec1b59efc6_MG_L_CC_ANON__fos_glcm.csv', '53582818_465aa5ec1b59efc6_MG_R_CC_ANON__fos_glcm.csv', '53586361_dda3c6969a34ff8e_MG_L_ML_ANON__fos_glcm.csv', '53586388_dda3c6969a34ff8e_MG_R_ML_ANON__fos_glcm.csv', '53586415_dda3c6969a34ff8e_MG_L_CC_ANON__fos_glcm.csv', '53586442_dda3c6969a34ff8e_MG_R_CC_ANON__fos_glcm.csv', '53586724_e5f3f68b9ce31228_MG_L_ML_ANON__fos_glcm.csv', '53586751_e5f3f68b9ce31228_MG_R_ML_ANON__fos_glcm.csv', '53586778_e5f3f68b9ce31228_MG_L_CC_ANON__fos_glcm.csv', '53586805_e5f3f68b9ce31228_MG_R_CC_ANON__fos_glcm.csv', '53586869_6ac23356b912ee9b_MG_L_ML_ANON__fos_glcm.csv', '53586896_6ac23356b912ee9b_MG_L_CC_ANON__fos_glcm.csv', '53586960_809e3f43339f93c6_MG_L_ML_ANON__fos_glcm.csv', '53586987_809e3f43339f93c6_MG_R_ML_ANON__fos_glcm.csv', '53587014_809e3f43339f93c6_MG_L_CC_ANON__fos_glcm.csv', '53587041_809e3f43339f93c6_MG_R_CC_ANON__fos_glcm.csv', '53587104_7b71aa9928e6975e_MG_L_ML_ANON__fos_glcm.csv', '53587131_7b71aa9928e6975e_MG_L_CC_ANON__fos_glcm.csv', '53587427_d2befe622e188943_MG_L_ML_ANON__fos_glcm.csv', '53587454_d2befe622e188943_MG_R_ML_ANON__fos_glcm.csv', '53587481_d2befe622e188943_MG_L_CC_ANON__fos_glcm.csv', '53587508_d2befe622e188943_MG_R_CC_ANON__fos_glcm.csv', '53587572_11e6732579acf692_MG_L_ML_ANON__fos_glcm.csv', '53587599_11e6732579acf692_MG_L_CC_ANON__fos_glcm.csv', '53587663_5fb370d4c1c71974_MG_R_CC_ANON__fos_glcm.csv', '53587690_5fb370d4c1c71974_MG_L_ML_ANON__fos_glcm.csv', '53587717_5fb370d4c1c71974_MG_R_ML_ANON__fos_glcm.csv', '53587744_5fb370d4c1c71974_MG_L_CC_ANON__fos_glcm.csv']\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "\n",
        "#first put a shortcut in your drive to the image processing folder\n",
        "\n",
        "#Choose the image folder you want to try\n",
        "\n",
        "pipeFolder = '10.8Prepro+glcm+fixed+CorrectedLabels'\n",
        "\n",
        "RESULTS_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results', \n",
        "                        pipeFolder)\n",
        "\n",
        "\n",
        "RESULTS_DIR_MAIN = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results')\n",
        "\n",
        "\n",
        "\n",
        "MODELS_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results', \n",
        "                        'MLModelsFinal')\n",
        "\n",
        "DATA_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Image Processing and Analysis 2022',\n",
        "                        'projects',\n",
        "                        'Calcification Detection',\n",
        "                        'dataset')\n",
        "\n",
        "\n",
        "print(os.listdir(RESULTS_DIR))\n",
        "\n",
        "results_file = os.listdir(RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6-OnlgGfsSdp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5os_ORY98A4Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1-TQuxisz-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import tree\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import pickle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qMqDYgXyktxL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier # random forest\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eJbtKF-ylFf0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "#!pip install fastprogress\n",
        "from fastprogress import master_bar, progress_bar\n",
        "#!pip install tqdm\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from skimage import measure\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "neMctBoMuIMD"
      },
      "outputs": [],
      "source": [
        "def read_results_train_test(path, keys):\n",
        "  df_result = pd.DataFrame()\n",
        "  for result in os.listdir(path):\n",
        "    if result.split('.')[-1] == 'csv':\n",
        "      if int(result.split('_')[0]) in list(keys):\n",
        "        try:\n",
        "          df = pd.read_csv(path+'/'+result)\n",
        "          df_result = df_result.append(df)\n",
        "        except:\n",
        "          print(\"Empty file \", result)\n",
        "  return df_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZkqzRwsJlfUs"
      },
      "outputs": [],
      "source": [
        "def read_results(path):\n",
        "  dfs = pd.DataFrame()\n",
        "  for result in results_file:\n",
        "    try:\n",
        "      df = pd.read_csv(path+'/'+result)\n",
        "    except:\n",
        "      print(\"Empty file \", result)\n",
        "\n",
        "    dfs = dfs.append(df)\n",
        "  \n",
        "  return dfs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "huyN37RmC_uw"
      },
      "outputs": [],
      "source": [
        "def train_test_function():\n",
        "  df_train_test = pd.read_csv(os.path.join('/content',\n",
        "                                'drive',\n",
        "                                'MyDrive',\n",
        "                                'Results',\n",
        "                                'standard_partitions.csv'), index_col=0)\n",
        "  train_keys = df_train_test.loc[df_train_test.partition == 'train']['image_id'].values\n",
        "  test_keys = df_train_test.loc[df_train_test.partition == 'test']['image_id'].values\n",
        "\n",
        "\n",
        "  dfs_train = read_results_train_test(RESULTS_DIR, train_keys)\n",
        "  dfs_test = read_results_train_test(RESULTS_DIR, test_keys)\n",
        "  return dfs_train,dfs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4UKgbKXPDJA9"
      },
      "outputs": [],
      "source": [
        "# dfs_train, dfs_test = train_test_function()\n",
        "# dfs_train.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiwTtcPjrMRK"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "udO3hTL2c0W7"
      },
      "outputs": [],
      "source": [
        "# We will balance data randomly taking  the same amount of 0 and 1 label inputs. There are several options to manage unbalanced data,\n",
        "# this is just one approach, we should try different approaches.\n",
        "# https://towardsdatascience.com/comparing-different-classification-machine-learning-models-for-an-imbalanced-dataset-fdae1af3677f\n",
        "def rus(dfs, not_consider=0):\n",
        "  # df_negative = dfs.loc[dfs.label == dfs.label.value_counts().idxmax()] #This is for making it more general\n",
        "  df_negative = dfs.loc[dfs.label == 0] #Take all negative samples from dfs -> label = 0\n",
        "  print('rus negative 1: ', len(df_negative))\n",
        "  print('rus negative 1 without duplicates: ', len(df_negative.drop_duplicates()))\n",
        "  #Here we are taking a subset from the 'negative pool' called df_negative with length equal to the number of positive candidates I have\n",
        "  if not_consider >= len(dfs.loc[dfs.label == 1]):\n",
        "    df_0 = pd.DataFrame()\n",
        "  else:\n",
        "    df_0 = df_negative.sample(len(dfs.loc[dfs.label == 1])-not_consider, random_state = 1) #not consider is the number of negative samples that I already have and that I misclassified the first time\n",
        "  print('df_0: ', len(df_0))\n",
        "  print('append ', len(df_negative.append(df_0)))\n",
        "  print('df_negative drop no keeping', len(df_negative.drop_duplicates(keep=False)))\n",
        "  df_negative = df_negative.append(df_0).drop_duplicates(keep=False) #df_negative is the samples that have not been used, and have not been selected yet for the training\n",
        "  print('after drop duplicates with appended: ',len(df_negative))\n",
        "  dfs_eq = df_0.append(dfs.loc[dfs.label == 1]) #dfs.label.value_counts().idxmin() #Here we are taking the balanced tada\n",
        "  print(\"Length of balanced data: \", len(dfs_eq))\n",
        "  print(dfs_eq['label'].value_counts())\n",
        "  return dfs_eq, df_negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dLoDror0v6el"
      },
      "outputs": [],
      "source": [
        "def plotFeatures(data):\n",
        "  #plot the scatter matrix\n",
        "  pd.plotting.scatter_matrix(data,figsize=(25,25))\n",
        "  #correlation plot\n",
        "  corr = data.corr()\n",
        "  f, ax = plt.subplots(figsize=(25, 25))\n",
        "  sns.heatmap(corr,annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hNRW1sKxHoOu"
      },
      "outputs": [],
      "source": [
        "def standardScaler(X_train, X_test):\n",
        "  standard_scaler = preprocessing.StandardScaler()\n",
        "  X_train = standard_scaler.fit_transform(X_train)\n",
        "  X_test = standard_scaler.transform(X_test)\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXE13LCNrZcX"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z0Icd_DnzSdZ"
      },
      "outputs": [],
      "source": [
        "def featureSelectionTrees(estimators, X_train, y_train, X_test):\n",
        "  clf = ExtraTreesClassifier(n_estimators=estimators)\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "  model = SelectFromModel(clf, prefit=True)\n",
        "  X_train = model.transform(X_train)\n",
        "  X_test = model.transform(X_test)\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jdnQpMHUI-0T"
      },
      "outputs": [],
      "source": [
        "def pcaAnalysis(components, X_train, X_test):\n",
        "  pca = PCA(components)\n",
        "  X_train = pca.fit_transform(X_train)\n",
        "  X_test = pca.transform(X_test)\n",
        "  print(\"PCA variance ratio: \", pca.explained_variance_ratio_)\n",
        "  print(\"Total variance Explained by PCA: \", sum(pca.explained_variance_ratio_))\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uTqXxdqBLFYS"
      },
      "outputs": [],
      "source": [
        "def selectFeaturesChi(k, X_train, X_test, y_train):\n",
        "  sel = SelectKBest(chi2, k=k)\n",
        "  sel.fit(X_train, y_train)\n",
        "  X_train = sel.transform(X_train)\n",
        "  X_test = sel.transform(X_test)\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b3nb74MroMO"
      },
      "source": [
        "# **Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SGbJYG0szmJW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import matthews_corrcoef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kN7GRye0Fjb2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def RandomForest(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    print(\"Searching for best hyperparameters\")\n",
        "    params = {'criterion': ['gini'],\n",
        "              'n_estimators': [100], # , 500, 900\n",
        "              'max_features': ['auto', 'sqrt'],#, 'sqrt', 'log2'\n",
        "              'max_depth' : [10, 12]}\n",
        "    grid = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs = -1), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for rf are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = RandomForestClassifier(random_state=42, n_jobs = -1)\n",
        "  \n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vI8BeXyIJIqj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier # gradient boosting regressor\n",
        "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
        "def GradientBoosting(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'learning_rate': [0.1],#0.05, 0.2\n",
        "              #'min_samples_split': [0.5, 0.8],\n",
        "              #'min_samples_leaf': [0.1, 0.2, 0.5],\n",
        "              'max_depth':[8],\n",
        "              #'max_features':['sqrt'],#'log2'\n",
        "              #'criterion': ['friedman_mse',  'mae'],\n",
        "              #'subsample':[0.5, 1.0],\n",
        "              'n_estimators':[600]}\n",
        "    grid = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for gb are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "\n",
        "  classifier = GradientBoostingClassifier(random_state=42)\n",
        "  \n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oZcwaXUQK71l"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier # adaboost, adaboostRegressor for regression problems\n",
        "\n",
        "def AdaBoost(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'n_estimators': [600],\n",
        "              'learning_rate': [0.5]}\n",
        "    grid = GridSearchCV(AdaBoostClassifier(random_state=42), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for ab are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KiHOO-9tLlBK"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def LogRegre(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'multi_class':['multinomial'],\n",
        "              'solver': ['sag'],#,'saga'\n",
        "              'penalty': ['l2']}#'elasticnet',\n",
        "    grid = GridSearchCV(LogisticRegression(random_state=42), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for lrg are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = LogisticRegression(random_state=42)\n",
        "  \n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kkiwq_C0lD38"
      },
      "outputs": [],
      "source": [
        "def KNN(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'n_neighbors':[3,5]}\n",
        "    grid = GridSearchCV(KNeighborsClassifier(), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "  classifier = KNeighborsClassifier()\n",
        "\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5hohRhuWbs3W"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def DecisionTree(X_train, y_train, cv=5, best_params = dict()):\n",
        "  classifier = tree.DecisionTreeClassifier()\n",
        " \n",
        "\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pjVbyTcmwekE"
      },
      "outputs": [],
      "source": [
        "# This is an example of how to use a Pipe inside a function we are training, as done in Challenge 3 by the professor\n",
        "def SVC_linear(X_train, y_train, cv=5, best_params = dict()):\n",
        "\n",
        "  if len(best_params) == 0:\n",
        "    lower_value_C = 1\n",
        "    higher_value_C = 10\n",
        "    n_values = 10\n",
        "    base = 10\n",
        "    params = {'C': [1, 3, 5,9.11], #12\n",
        "              'kernel' : ['rbf'],\n",
        "              'gamma': [2.5, 5, 10]}\n",
        "\n",
        "    grid = GridSearchCV(SVC(random_state = 42, probability=True), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for svm are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = SVC(random_state = 42, probability=True)\n",
        "\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ox_3v0fW-Yh8"
      },
      "outputs": [],
      "source": [
        "def cv_classification(classifier, params, X_train, X_test, y_train, y_test, candidates_train, candidates_test, normals, clf_name, imageKeyLocation):\n",
        "  \n",
        "  trainDataFull = pd.DataFrame(X_train).copy()\n",
        "  ss_f1 = StandardScaler()\n",
        "  trainDataFull = pd.DataFrame(ss_f1.fit_transform(trainDataFull), columns=X_train.columns)\n",
        "  trainDataFull['label'] = y_train\n",
        "\n",
        "  dataBalanced, dfNegative = rus(trainDataFull)\n",
        "  print(\"Data Balanced: \", len(dataBalanced))\n",
        "  print(\"Negative pool: \",len(dfNegative))\n",
        "\n",
        "  X_train_f1 = dataBalanced.drop('label', axis=1)\n",
        "  y_train_f1 = dataBalanced['label']\n",
        "\n",
        "  clf1 = clone(classifier)\n",
        "  clf1.set_params(**params)\n",
        "  cvf1Score = []\n",
        "  cvMccScore = []\n",
        "  cvpAUCScore = []\n",
        "\n",
        "  print(\"******************** RESULTS FOLD 1 *****************************\")\n",
        "  selectedFold = 1\n",
        "  clf1.fit(X_train_f1, y_train_f1)\n",
        "  print('TRAIN')\n",
        "  y_pred_tr_1 = clf1.predict(X_train_f1)\n",
        "  print(confusion_matrix(y_train_f1, y_pred_tr_1))\n",
        "  print(f1_score(y_train_f1, y_pred_tr_1))\n",
        "\n",
        "  y_pred_proba_tr = clf1.predict_proba(X_train_f1)[:, 1]\n",
        "  print('pAUC train: ', roc_auc_score(y_train_f1, y_pred_proba_tr, max_fpr = 0.0001))\n",
        "\n",
        "  \n",
        "  print('TEST')\n",
        "  y_pred_1 = clf1.predict(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))\n",
        "  print(confusion_matrix(y_test, y_pred_1))\n",
        "  print(f1_score(y_test, y_pred_1))\n",
        "  y_pred_proba = clf1.predict_proba(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))[:, 1]\n",
        "  print('pAUC test: ', roc_auc_score(y_test, y_pred_proba, max_fpr = 0.0001))\n",
        "\n",
        "  print(classification_report(y_test, y_pred_1))  \n",
        "  print('F1 score test: ', f1_score(y_test, y_pred_1, average='binary'))\n",
        "\n",
        "  df_cm = confusion_matrix(y_test, y_pred_1)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(df_cm, annot=True, fmt='d') # font size\n",
        "  plt.show()\n",
        "\n",
        "  filename = f'best_model_{clf_name}_{selectedFold}_{pipeFolder}_rus_scaled.pkl'\n",
        "  pickle.dump(clf1, open(os.path.join(MODELS_DIR,'rus_scaled', filename), 'wb'))\n",
        "  print(f\"Fold {selectedFold} of {clf_name} was saved!\")  \n",
        "\n",
        "\n",
        "  cvf1Score.append(f1_score(y_test, y_pred_1, average='binary'))\n",
        "  cvMccScore.append(matthews_corrcoef(y_test, y_pred_1))\n",
        "  cvpAUCScore.append(roc_auc_score(y_test, clf1.predict_proba(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))[:, 1], max_fpr = 0.0001))\n",
        "\n",
        "\n",
        "  candidates_unique = [cand.split('_')[imageKeyLocation] for cand in candidates_test['name'].values.tolist()]\n",
        "  candidates_unique = list(set(candidates_unique))\n",
        "  groundTruthShort = [i for i in groundTruths if i.split('_')[0] in candidates_unique]\n",
        "  print(len(groundTruthShort))\n",
        "  normals_f1 = [i for i in normals if i in candidates_unique]\n",
        "\n",
        "\n",
        "  fn_1, dfROC_1, filename_key_1 = calculateFROC(groundTruthShort, normals_f1, candidates_test, y_pred_proba, classifier, pipeFolder)\n",
        "\n",
        "  print(\"*********************** RESULTS FOLD 2 *******************************\")\n",
        "  selectedFold = 2\n",
        "  trainDataFull = pd.DataFrame(X_test).copy()\n",
        "  ss_f2 = StandardScaler()\n",
        "  trainDataFull = pd.DataFrame(ss_f2.fit_transform(trainDataFull), columns=X_test.columns)\n",
        "  trainDataFull['label'] = y_test\n",
        "\n",
        "  dataBalanced, dfNegative = rus(trainDataFull)\n",
        "  print(\"Data Balanced: \", len(dataBalanced))\n",
        "  print(\"Negative pool: \",len(dfNegative))\n",
        "\n",
        "  X_test_f2 = dataBalanced.drop('label', axis=1)\n",
        "  y_test_f2 = dataBalanced['label']\n",
        "\n",
        "  clf2 = clone(classifier)\n",
        "  clf2.set_params(**params)\n",
        "  clf2.fit(X_test_f2, y_test_f2)\n",
        "\n",
        "  print('TRAIN')\n",
        "  y_pred_tr_2 = clf2.predict(X_test_f2)\n",
        "  print(confusion_matrix(y_test_f2, y_pred_tr_2))\n",
        "  print(f1_score(y_test_f2, y_pred_tr_2))\n",
        "  y_pred_proba_tr = clf2.predict_proba(X_test_f2)[:, 1]\n",
        "  print('pAUC train: ', roc_auc_score(y_test_f2, y_pred_proba_tr, max_fpr = 0.0001))\n",
        "\n",
        "  print('TEST')\n",
        "  y_pred_2 = clf2.predict(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))\n",
        "  print(confusion_matrix(y_train, y_pred_2))\n",
        "  print(f1_score(y_train, y_pred_2))\n",
        "  y_pred_proba_2 = clf2.predict_proba(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))[:, 1]\n",
        "  print('pAUC test: ', roc_auc_score(y_train, y_pred_proba_2, max_fpr = 0.0001))\n",
        "\n",
        "  print(classification_report(y_train, y_pred_2))  \n",
        "\n",
        "  print('F1 score: ', f1_score(y_train, y_pred_2, average='binary'))\n",
        "  df_cm = confusion_matrix(y_train, y_pred_2)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(df_cm, annot=True, fmt='d') # font size\n",
        "  plt.show()\n",
        "\n",
        "  filename = f'best_model_{clf_name}_{selectedFold}_{pipeFolder}_rus_scaled.pkl'\n",
        "  pickle.dump(clf2, open(os.path.join(MODELS_DIR,'rus_scaled', filename), 'wb'))\n",
        "  print(f\"Fold {selectedFold} of {clf_name} was saved!\")  \n",
        "\n",
        "  cvf1Score.append(f1_score(y_train, y_pred_2, average='binary'))\n",
        "  cvMccScore.append(matthews_corrcoef(y_train, y_pred_2))\n",
        "  cvpAUCScore.append(roc_auc_score(y_train, clf2.predict_proba(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))[:, 1], max_fpr = 0.0001))\n",
        "  print(\"Mean CV_score F1-score: \", np.mean(cvf1Score))\n",
        "\n",
        "  candidates_unique = [cand.split('_')[imageKeyLocation] for cand in candidates_train['name'].values.tolist()]\n",
        "  candidates_unique = list(set(candidates_unique))\n",
        "  groundTruthShort = [i for i in groundTruths if i.split('_')[0] in candidates_unique]\n",
        "  print(len(groundTruthShort))\n",
        "  normals_f2 = [i for i in normals if i in candidates_unique]\n",
        "\n",
        "  fn_2, dfROC_2, filename_key_2 = calculateFROC(groundTruthShort, normals_f2, candidates_train, y_pred_proba_2, clf_name, pipeFolder)\n",
        "  \n",
        "  \n",
        "  # Choose the best one\n",
        "  if np.array(cvf1Score).argmax() == 0:  \n",
        "    best_f1 = f1_score(y_test, y_pred_1)\n",
        "    best_mcc = matthews_corrcoef(y_test, y_pred_1)\n",
        "    best_pAUC = roc_auc_score(y_test, y_pred_proba, max_fpr = 0.0001)\n",
        "    \n",
        "  else:\n",
        "    best_f1 = f1_score(y_train, y_pred_2)\n",
        "    best_mcc = matthews_corrcoef(y_train, y_pred_2)\n",
        "    best_pAUC = roc_auc_score(y_train, y_pred_proba_2, max_fpr = 0.0001)    \n",
        "\n",
        "\n",
        "  print(\"cvf1Score: \", cvf1Score)\n",
        "  dfROC_1 = dfROC_1.append(dfROC_2)\n",
        "\n",
        "  return np.array(cvf1Score).argmax(),[np.mean(cvpAUCScore), np.mean(cvf1Score), np.mean(cvMccScore)], [best_pAUC, best_f1, best_mcc], dfROC_1, fn_1+fn_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXAhatZZ_Ap-"
      },
      "source": [
        "# **Scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgc5Kg2FLMMC",
        "outputId": "8e62d48a-2d6a-424a-b92a-d0a4461d7459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['normals.txt', 'images', 'groundtruths', 'masks']\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "\n",
        "#first put a shortcut in your drive to the image processing folder\n",
        "\n",
        "DATA_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Image Processing and Analysis 2022',\n",
        "                        'projects',\n",
        "                        'Calcification Detection',\n",
        "                        'dataset')\n",
        "\n",
        "\n",
        "print(os.listdir(DATA_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ao5mV9BCL4sa"
      },
      "outputs": [],
      "source": [
        "_, _, groundTruths = next(os.walk(os.path.join(DATA_DIR, 'groundtruths')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8U-oRZ_h_AJB"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def calculateFROC(groundTruthsList, normals, candidates, prediction, model, pipeline):\n",
        "\n",
        "  #List needed for counting\n",
        "  evaluationList = []\n",
        "\n",
        "  # evaluation froc curve #\n",
        "\n",
        "  fn = 0 # false negative, for the blobs that do not belong to any component\n",
        "  positive_candidates = 0\n",
        "  flag = True\n",
        "\n",
        "  rowListdfROC = []\n",
        "\n",
        "  candidates_copy= candidates.copy()\n",
        "  candidates_copy['prediction'] = prediction\n",
        "\n",
        "  for imageKey in tqdm_notebook(groundTruthsList):\n",
        "\n",
        "\n",
        "    evaluationList = []\n",
        " \n",
        "    # list of features found with y,x and sigma\n",
        "    candidatesImg = candidates_copy.loc[candidates_copy['name'].str.contains(imageKey.split('_')[0], regex=False)]\n",
        "    candidates_number = len(candidatesImg)\n",
        "#    print('cand number: ', candidates_number)\n",
        "\n",
        "    mask = cv2.imread(os.path.join(DATA_DIR, 'groundtruths', imageKey), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        " #   print('# IMAGEKEY: ', imageKey)\n",
        "    blobs = mask > 0.7 * mask.mean() #Thresholding the backgroudnd\n",
        "    blobs_labels, count = measure.label(blobs, background=0, return_num=True) #Getting labels of the connected components and the amount of them without considering the count\n",
        "\n",
        "    # dictionaries\n",
        "    dictCounting={}\n",
        "\n",
        "    for index in range(1, count+1):\n",
        "      dictCounting[index] = 0\n",
        "    \n",
        "    evaluationList = []\n",
        "    dictMean = {}\n",
        "\n",
        "\n",
        "    for index2, candidate in candidatesImg.iterrows():\n",
        "      sigma = 7\n",
        "\n",
        "      if (imageKey.split('_')[0] in normals):\n",
        "        if candidate['name'].split('_')[1] not in dictMean.keys():\n",
        "          dictMean[candidate['name'].split('_')[1]] = [candidate.prediction]\n",
        "        else:\n",
        "          dictMean[candidate['name'].split('_')[1]].append(candidate.prediction)\n",
        "        continue\n",
        "\n",
        "      # is the image a normal image?\n",
        "\n",
        "      if (((candidate.x - sigma) < 0) or \n",
        "        ((candidate.x + sigma) > mask.shape[0]) or \n",
        "        ((candidate.y - sigma) < 0) or\n",
        "        ((candidate.y + sigma) > mask.shape[1])):\n",
        "        continue\n",
        "\n",
        "\n",
        "      # n = 3\n",
        "      left = int(candidate.x - sigma)\n",
        "      right = int(candidate.x + sigma)\n",
        "      top = int(candidate.y - sigma)\n",
        "      bottom = int(candidate.y + sigma) \n",
        "\n",
        "      #    y : y + w , x : x +  h\n",
        "      nonzero = cv2.countNonZero(blobs_labels[top:bottom, left: right])\n",
        "\n",
        "      if nonzero > 0:\n",
        "        # Find all connected components (cc) that intersect with the candidate\n",
        "        foundCC = [i for i in np.unique(blobs_labels[top:bottom, left: right]) if i!= 0]\n",
        "\n",
        "        # Keep the maximum prediction of the candidates that intsersect with at least one component\n",
        "        for cc in foundCC:\n",
        "          dictCounting[cc] = max(candidate.prediction, dictCounting[cc])\n",
        "\n",
        "    if (imageKey.split('_')[0] not in normals):\n",
        "      for key,value in dictCounting.items():\n",
        "        if np.sum(blobs_labels == key) > np.floor(np.pi*(15/2.0)**2):\n",
        "#          print(\"Too big... discarded\")\n",
        "          continue\n",
        "        if value > 0:\n",
        "            rowListdfROC.append(['TP', value])\n",
        "        else:\n",
        "          fn = fn + 1\n",
        "    else:\n",
        "      if len(dictMean.keys()) > 0:\n",
        "        for key, value in dictMean.items():\n",
        "          rowListdfROC.append(['FP', np.max(np.array(value))])\n",
        "\n",
        "\n",
        "  filename_key = 'FROC_calculations_{}_pip{}_fn{}_normals{}_rus_scaled.csv'.format(model, pipeline, fn, len(normals))\n",
        "\n",
        "  dfROC = pd.DataFrame(rowListdfROC, columns=['type', 'prob'])\n",
        "\n",
        "  dfROC.to_csv(os.path.join(MODELS_DIR,\n",
        "                            'rus_scaled',\n",
        "                            filename_key))\n",
        "  \n",
        "  print(\"File saved as \", filename_key)\n",
        "\n",
        "\n",
        "\n",
        "  return fn, dfROC, filename_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "a-mMXd-6JYpT"
      },
      "outputs": [],
      "source": [
        "def writeFile(df, flag, name):\n",
        "  if(flag):\n",
        "    df.to_csv(os.path.join('/content',\n",
        "                                 'drive',\n",
        "                                 'MyDrive',\n",
        "                                 'Results',\n",
        "                                 name),\n",
        "                    mode='a',\n",
        "                    index=False)\n",
        "    flag = False\n",
        "  else:\n",
        "    df.to_csv(os.path.join('/content',\n",
        "                                 'drive',\n",
        "                                 'MyDrive',\n",
        "                                 'Results',\n",
        "                                 name),\n",
        "                  mode='a',\n",
        "                  header=False,\n",
        "                  index=False)\n",
        "  return flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "K2iVgV-pBsQA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from numpy import trapz\n",
        "\n",
        "def draw_curve(fn, normals, dfROC, name, clf_name):\n",
        "\n",
        "  experiments_results_path = os.path.join(MODELS_DIR, 'rus_scaled')\n",
        "  results_filename = f'results_AUC_rus_scaled_{pipeFolder}.csv'\n",
        "  confusion_matrix_filename = f'results_ConfMat_rus_scaled_{pipeFolder}.csv'\n",
        "\n",
        "\n",
        "  if results_filename not in os.listdir(experiments_results_path):\n",
        "    results_df = pd.DataFrame(columns=[\"model\", \"AUC_TOTAL\", \"AUC_final_50fPpi\", \"F1_final\"])\n",
        "    results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "  \n",
        "    results_confmat = pd.DataFrame(columns=[\"model\", \"TP\", \"FP\", \"FN\", \"Total True\"])\n",
        "    results_confmat.to_csv(os.path.join(experiments_results_path, confusion_matrix_filename), index=False)\n",
        "  \n",
        "  \n",
        "  else:\n",
        "    results_df = pd.read_csv(os.path.join(experiments_results_path, results_filename))\n",
        "    results_confmat = pd.read_csv(os.path.join(experiments_results_path, confusion_matrix_filename))\n",
        "\n",
        "  tpc = 0\n",
        "  fpc = 0\n",
        "  tpr = []\n",
        "  fppi = []\n",
        "\n",
        "  dfROC['prob'] = [round(i,4) for i in dfROC['prob'].values]\n",
        "  dfROC = dfROC.sort_values('prob', ascending=False)\n",
        "\n",
        "  thresholds = dfROC.prob.unique()\n",
        "  print('Number of thresholds: ', len(thresholds))\n",
        "  print(thresholds)\n",
        "\n",
        "  tp = len(dfROC.loc[dfROC.type == 'TP'])\n",
        "  print('true positives: ', tp)\n",
        "  fp = len(dfROC.loc[dfROC.type == 'FP'])\n",
        "  print('false positives: ',  fp)\n",
        "  print(\"Total number of positives: \", tp+fn)\n",
        "\n",
        "  for i in progress_bar(range(len(thresholds))):\n",
        "\n",
        "    tpc += len(dfROC.loc[(dfROC.prob > thresholds[i]) & (dfROC.type=='TP')])\n",
        "    fpc += len(dfROC.loc[(dfROC.prob > thresholds[i]) & (dfROC.type!='TP')])\n",
        "              \n",
        "    # print('TP amount {} in threshold {}'.format(tpc, thresholds[i]))\n",
        "    # print('FP amount {} in threshold {}'.format(fpc, thresholds[i]))\n",
        "    \n",
        "\n",
        "    tpr.append( tpc/(tp+fn) )\n",
        "    fppi.append( fpc/normals )\n",
        "    tpc = 0\n",
        "    fpc = 0\n",
        "  \n",
        "  gamma = [i for i in fppi if i <= 50 ]\n",
        "  print(max(fppi))\n",
        "  auc_total = trapz(tpr, x=fppi)/max(fppi)\n",
        "  print(max(gamma))\n",
        "  auc_final = trapz(tpr[0:len(gamma)], x=gamma)/max(gamma)\n",
        "\n",
        "  print('AUC TOTAL:', auc_total)\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(fppi, tpr)\n",
        "  ax.set_xlabel('fPpI', fontsize=15)\n",
        "  ax.set_ylabel('TPR', fontsize=15)\n",
        "  ax.grid(True)\n",
        "  plt.ylim(0,1)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  print('AUC final:', auc_final)\n",
        "  fig1, ax1 = plt.subplots()\n",
        "  ax1.plot(gamma, tpr[0:len(gamma)])\n",
        "  ax1.set_xlabel('fPpI', fontsize=12)\n",
        "  ax1.set_ylabel('TPR', fontsize=12)\n",
        "  ax1.grid(True)\n",
        "  plt.ylim(0,1)\n",
        "  plt.xlim(0,50)\n",
        "\n",
        "  plt.savefig(os.path.join(MODELS_DIR,\n",
        "                          'rus_scaled', \n",
        "                           name+'.eps'), format='eps')\n",
        "  plt.show()\n",
        "    \n",
        "  final_f1 = tp / (tp + (fp + fn)/2)\n",
        "\n",
        "  row_results = [clf_name, auc_total, auc_final, final_f1]\n",
        "  results_df = results_df.append(pd.DataFrame([row_results], columns=[\"model\", \"AUC_TOTAL\", \"AUC_final_50fPpi\", \"F1_final\"]))\n",
        "  results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "\n",
        "  confmat_results = [clf_name, tp, fp, fn, tp+fn]\n",
        "  results_confmat = results_confmat.append(pd.DataFrame([confmat_results], columns=[\"model\", \"TP\", \"FP\", \"FN\", \"Total True\"]))\n",
        "  results_confmat.to_csv(os.path.join(experiments_results_path, confusion_matrix_filename), index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cQyjMqP1ULaC"
      },
      "outputs": [],
      "source": [
        "def load_and_predict(selectedFold, classifier_name):\n",
        "\n",
        "  #Reading data\n",
        "  dfs_train, dfs_test = train_test_function()\n",
        "  print('Length of train set: ', len(dfs_train))\n",
        "  print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "  dfs_tr_hyperopt = dfs_train.sample(frac=0.1, random_state=42)\n",
        "  dfs_ts_hyperopt = dfs_test.sample(frac=0.1, random_state=42)\n",
        "\n",
        "  dfs_train = dfs_train.append(dfs_tr_hyperopt).drop_duplicates(keep=False)\n",
        "  dfs_test = dfs_test.append(dfs_ts_hyperopt).drop_duplicates(keep=False)\n",
        "  print('Length of train set: ', len(dfs_train))\n",
        "  print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "  try:  \n",
        "    if dfs_train.isnull().values.any():\n",
        "      colm = dfs_train.columns[dfs_train.isna().any()]\n",
        "      dfs_train = dfs_train[dfs_train[colm[0]].notna()]\n",
        "\n",
        "    if dfs_test.isnull().values.any():\n",
        "      colm = dfs_test.columns[dfs_test.isna().any()]    \n",
        "      dfs_test = dfs_test[dfs_test[colm[0]].notna()]\n",
        "\n",
        "    print(\"Train dfs without nans: \", len(dfs_train))\n",
        "    print(\"Test dfs without nans: \",len(dfs_test))\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"No nan values to drop, or not columns\")\n",
        "    print(e)\n",
        "\n",
        "  y_train = dfs_train['label'].reset_index(drop=True)\n",
        "  candidates_train = dfs_train[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "  X_train = dfs_train.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "  y_test = dfs_test['label'].reset_index(drop=True)\n",
        "  candidates_test = dfs_test[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "  X_test = dfs_test.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "  #Obtain train-test datasets\n",
        "  if selectedFold == 2:\n",
        "    chosenCandidates = candidates_train\n",
        "    trainDataFull = pd.DataFrame(X_test).copy()\n",
        "    ss_f2 = StandardScaler()\n",
        "    trainDataFull = pd.DataFrame(ss_f2.fit_transform(trainDataFull), columns=X_test.columns)\n",
        "    trainDataFull['label'] = y_test\n",
        "\n",
        "    model = pickle.load(open(os.path.join(MODELS_DIR,'rus_scaled', f'best_model_{classifier_name}_{selectedFold}_{pipeFolder}_rus_scaled.pkl'), 'rb'))\n",
        "    y_pred = model.predict(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))\n",
        "    y_pred_proba = model.predict_proba(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))[:,1]\n",
        "\n",
        "  else:\n",
        "    chosenCandidates = candidates_test\n",
        "    trainDataFull = pd.DataFrame(X_train).copy()\n",
        "    ss_f1 = StandardScaler()\n",
        "    trainDataFull = pd.DataFrame(ss_f1.fit_transform(trainDataFull), columns=X_train.columns)\n",
        "    trainDataFull['label'] = y_train\n",
        "\n",
        "    model = pickle.load(open(os.path.join(MODELS_DIR,'rus_scaled', f'best_model_{classifier_name}_{selectedFold}_{pipeFolder}_rus_scaled.pkl'), 'rb'))\n",
        "    y_pred = model.predict(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))\n",
        "    y_pred_proba = model.predict_proba(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))[:,1]\n",
        "\n",
        "\n",
        "  normals = []\n",
        "  with open(os.path.join(RESULTS_DIR_MAIN,'normals_final.txt')) as f:\n",
        "      for line in f:\n",
        "          normals.append(line[:-1])\n",
        "\n",
        "  if 'glcm' in pipeFolder:\n",
        "    imageKeyName = 2\n",
        "  else:\n",
        "    imageKeyName = 3\n",
        "\n",
        "  candidates_unique = [cand.split('_')[imageKeyName] for cand in chosenCandidates['name'].values.tolist()]\n",
        "\n",
        "  candidates_unique = list(set(candidates_unique))\n",
        "\n",
        "  groundTruthShort = [i for i in groundTruths if i.split('_')[0] in candidates_unique]\n",
        "\n",
        "  print(len(groundTruthShort))\n",
        "\n",
        "  normals = [i for i in normals if i in candidates_unique]\n",
        "\n",
        "  print(\"Calculating fROC file...\") \n",
        "  fn, dfROC, filename_key = calculateFROC(groundTruthShort, normals, chosenCandidates, y_pred_proba, classifier_name, pipeFolder+'_loaded_')\n",
        "  print(len(dfROC.prob.unique()))\n",
        "  draw_curve(fn, len(normals), dfROC, filename_key[:-4], classifier_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_liWLRjttKkK"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5Jw-NQaT7j0d",
        "outputId": "a691a590-6147-42b1-9de7-f61ddd5b462d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ConnectButton(){\n",
              "    console.log(\"Connect pushed\"); \n",
              "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
              "}\n",
              "setInterval(ConnectButton,60000);"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# https://medium.com/@robertbracco1/configuring-google-colab-like-a-pro-d61c253f7573#a642\n",
        "%%javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpPoui7q7XkO"
      },
      "source": [
        "**Cross validation classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GrJGAx_hEk3",
        "outputId": "76f5ddf3-ee75-4f50-c968-1dd31744cb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train set:  378938\n",
            "Length of test set:  394560\n",
            "Length of train set:  341044\n",
            "Length of test set:  355104\n",
            "Train dfs without nans:  341044\n",
            "Test dfs without nans:  355104\n",
            "rus negative 1:  334676\n",
            "rus negative 1 without duplicates:  334655\n",
            "df_0:  6368\n",
            "append  341044\n"
          ]
        }
      ],
      "source": [
        "# def main():\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "normals = []\n",
        "with open(os.path.join(RESULTS_DIR_MAIN,'normals_final.txt')) as f:\n",
        "    for line in f:\n",
        "        normals.append(line[:-1])\n",
        "\n",
        "if 'glcm' in pipeFolder:\n",
        "  imageKeyName = 2\n",
        "else:\n",
        "  imageKeyName = 3\n",
        "\n",
        "\n",
        "\n",
        "classifiers = ['gb', 'rf', 'adaboost', 'logreg', 'knn', 'svm']\n",
        "best_hyperparameters = dict()\n",
        "y_pred_proba = []\n",
        "f1_scores = []\n",
        "\n",
        "experiments_results_path = os.path.join(MODELS_DIR, 'rus_scaled')\n",
        "results_filename = f'results_rus_scaled_{pipeFolder}.csv'\n",
        "\n",
        "if results_filename not in os.listdir(experiments_results_path):\n",
        "  results_df = pd.DataFrame(columns=[\"model\", \"pAUC_mean\", \"f1_mean\", \"mcc_mean\", \"pAUC\", \"f1\", \"mcc\", \"selectedFold\", \"parameters\"])\n",
        "  results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "else:\n",
        "  results_df = pd.read_csv(os.path.join(experiments_results_path, results_filename))\n",
        "\n",
        "if len(results_df)>0:\n",
        "  loadedFold = results_df.iloc[len(results_df)-1]['selectedFold']\n",
        "else:\n",
        "  loadedFold = -1\n",
        "\n",
        "#Reading data\n",
        "dfs_train, dfs_test = train_test_function()\n",
        "print('Length of train set: ', len(dfs_train))\n",
        "print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "dfs_tr_hyperopt = dfs_train.sample(frac=0.1, random_state=42)\n",
        "dfs_ts_hyperopt = dfs_test.sample(frac=0.1, random_state=42)\n",
        "\n",
        "dfs_train = dfs_train.append(dfs_tr_hyperopt).drop_duplicates(keep=False)\n",
        "dfs_test = dfs_test.append(dfs_ts_hyperopt).drop_duplicates(keep=False)\n",
        "print('Length of train set: ', len(dfs_train))\n",
        "print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "\n",
        "dfs_hyperOpt = shuffle(dfs_tr_hyperopt.append(dfs_ts_hyperopt))\n",
        "\n",
        "\n",
        "#Preprocessing steps\n",
        "#Manage unbalanced data\n",
        "# dataBalanced, dfNegative = rus(dfs) #Random majority undersampling\n",
        "\n",
        "try:\n",
        "  if dfs_train.isnull().values.any():\n",
        "    colm = dfs_train.columns[dfs_train.isna().any()]\n",
        "    dfs_train = dfs_train[dfs_train[colm[0]].notna()]\n",
        "\n",
        "  if dfs_test.isnull().values.any():\n",
        "    colm = dfs_test.columns[dfs_test.isna().any()]    \n",
        "    dfs_test = dfs_test[dfs_test[colm[0]].notna()]\n",
        "\n",
        "  if dfs_hyperOpt.isnull().values.any():\n",
        "    colm = dfs_hyperOpt.columns[dfs_hyperOpt.isna().any()]\n",
        "    dfs_hyperOpt = dfs_hyperOpt[dfs_hyperOpt[colm[0]].notna()]\n",
        "\n",
        "\n",
        "  print(\"Train dfs without nans: \", len(dfs_train))\n",
        "  print(\"Test dfs without nans: \",len(dfs_test))\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"No nan values to drop, or not columns\")\n",
        "  print(e)\n",
        "\n",
        "y_train = dfs_train['label'].reset_index(drop=True)\n",
        "candidates_train = dfs_train[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "X_train = dfs_train.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "y_test = dfs_test['label'].reset_index(drop=True)\n",
        "candidates_test = dfs_test[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "X_test = dfs_test.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "#Plot features \n",
        "#  plotFeatures(data)\n",
        "\n",
        "#Obtain train-test datasets\n",
        "\n",
        "\n",
        "trainDataFull = pd.DataFrame(X_train).copy()\n",
        "trainDataFull['label'] = y_train\n",
        "\n",
        "dataBalanced, dfNegative = rus(trainDataFull)\n",
        "print(\"Data Balanced: \", len(dataBalanced))\n",
        "print(\"Negative pool: \",len(dfNegative))\n",
        "\n",
        "trainBalanceData = dataBalanced.drop('label', axis=1)\n",
        "trainBalanceLabel = dataBalanced['label']\n",
        "\n",
        "hyperOptTrainData = dfs_hyperOpt.drop(['name', 'label',  'x', 'y'], axis=1)\n",
        "ss = StandardScaler()\n",
        "hyperOptTrainData = pd.DataFrame(ss.fit_transform(hyperOptTrainData), columns=X_train.columns)\n",
        "hyperOptLabel = dfs_hyperOpt['label']\n",
        "\n",
        "\n",
        "for classifier in classifiers:\n",
        "\n",
        "    if classifier == 'svm':\n",
        "      clf, best_params = SVC_linear(hyperOptTrainData, hyperOptLabel)\n",
        "    elif classifier == 'rf':\n",
        "      clf, best_params = RandomForest(hyperOptTrainData, hyperOptLabel, best_params = {'criterion': 'gini', 'max_depth': 12, 'max_features': 'auto', 'n_estimators': 100})\n",
        "    elif classifier == 'gb':\n",
        "      clf, best_params = GradientBoosting(hyperOptTrainData, hyperOptLabel, best_params = {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 600})\n",
        "    elif classifier == 'adaboost':\n",
        "      clf, best_params = AdaBoost(hyperOptTrainData, hyperOptLabel, best_params = {'learning_rate': 0.5, 'n_estimators': 600})\n",
        "    elif classifier == 'logreg':\n",
        "      clf, best_params = LogRegre(hyperOptTrainData, hyperOptLabel)\n",
        "    elif classifier == 'knn':\n",
        "      clf, best_params = KNN(hyperOptTrainData, hyperOptLabel, best_params = {'n_neighbors': 5})\n",
        "    elif classifier == 'dt':\n",
        "      clf, best_params = DecisionTree(hyperOptTrainData, hyperOptLabel)\n",
        "\n",
        "\n",
        "    selectedFold, mean_metrics, best_metrics, dfROC, fn = cv_classification(clf, best_params, X_train, X_test, y_train, y_test, \n",
        "                                                                            candidates_train, candidates_test, normals, classifier, imageKeyName)\n",
        "  # Columns of results file are [\"model\", \"pAUC_mean\", \"f1_mean\", \"mcc_mean\", \"pAUC\", \"f1\", \"mcc\", \"selectedFold\", \"parameters\"]\n",
        "    results_to_save = [classifier]\n",
        "    results_to_save.extend(mean_metrics)\n",
        "    results_to_save.extend(best_metrics)\n",
        "    results_to_save.append(selectedFold)\n",
        "    results_to_save.append([best_params])\n",
        "    \n",
        "  #save Results file in already created file\n",
        "    results_df = results_df.append(pd.DataFrame([results_to_save], columns=[\"model\", \"pAUC_mean\", \"f1_mean\", \"mcc_mean\", \"pAUC\", \"f1\", \"mcc\", \"selectedFold\", \"parameters\"]))\n",
        "    results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "\n",
        "    best_hyperparameters[classifier] = best_params\n",
        "\n",
        "\n",
        "    draw_curve(fn, len(normals), dfROC, f'FROC_curve_{classifier}_pip{pipeFolder}_fn{fn}_normals{len(normals)}_rus_scaled', classifier)\n",
        "\n",
        "    del dfROC, fn, results_to_save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb6bOKt4hvt3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ML_structure_project_rus_ScaledData_Final.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}