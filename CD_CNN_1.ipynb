{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emyesme/CalcificationDetection/blob/Zarin/CD_CNN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5H3DkTAVz5n"
      },
      "source": [
        "## Import the required packages\n",
        "Insert here all the packages you require, so in case they are not found an error will be shown before any other operation is performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO2FJxaIVz5o"
      },
      "outputs": [],
      "source": [
        "# import the required packages\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glV1s22RVz5r"
      },
      "source": [
        "## Set hyperparameters and options\n",
        "Set here your hyperparameters (to be used later in the code), so that you can run and compare different experiments operating on these values. \n",
        "<br>_Note: a better alternative would be to use command-line arguments to set hyperparameters and other options (see argparse Python package)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_TgO-mWVz5r"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.02\n",
        "epochs = 30\n",
        "momentum = 0.1\n",
        "lr_step_size = 1000   # if < epochs, we are using decaying learning rate\n",
        "lr_gamma = 0.1\n",
        "data_augmentation = True\n",
        "dropout = 0.1\n",
        "activation = nn.LeakyReLU()\n",
        "\n",
        "# make visible only one GPU at the time\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # <-- should be the ID of the GPU you want to use\n",
        "\n",
        "# options\n",
        "# device = \"cuda:0\"           # put here \"cuda:0\" if you want to run on GPU\n",
        "monitor_display = True      # whether to display monitored performance plots\n",
        "display_first_n = 0         # how many samples/batches are displayed\n",
        "num_workers = 2             # how many workers (=threads) for fetching data\n",
        "pretrained = False          # whether to test a pretrained model (to be loaded) or train a new one\n",
        "display_errors = True       # whether to display errors (only in pretrained mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txuQL7n1Vz5u"
      },
      "source": [
        "## Define the model architecture\n",
        "Define here your network.\n",
        "<br>_Note: a better alternative would be to have a pool of network architectures defined in a python file (module) that one could import_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92rh4FVMVz5u"
      },
      "outputs": [],
      "source": [
        "# define CNN\n",
        "\n",
        "###\n",
        "# Haq, I.U., Ali, H., Yu, W.H., Lei, C., Ali, H., Feature fusion and ensemble learning\u0002based CNN model for mammographic image classification, Journal of King Saud University - Computer and\n",
        "# Information Sciences (2022), doi: https://doi.org/10.1016/j.jksuci.2022.03.023\n",
        "###\n",
        "\n",
        "class CD_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CD_CNN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=7, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.dropout1=nn.Dropout(p=0.2)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.dwconv1 = nn.DepthwiseConv2D(in_channels=64, out_channels=128, kernel_size=5, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=5, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.dropout2=nn.Dropout(p=0.2)\n",
        "        self.bn6 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.dwconv2 = nn.DepthwiseConv2D(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn9 = nn.BatchNorm2d(256)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.dropout3=nn.Dropout(p=0.2)\n",
        "        self.bn10 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.fc1 = nn.Linear(256,512)\n",
        "        self.fc2 = nn.Linear(512,512)\n",
        "        self.fc3 = nn.Linear(512,2)\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "        nn.init.xavier_normal_(self.conv1.weight)\n",
        "        nn.init.xavier_normal_(self.conv2.weight)\n",
        "        nn.init.xavier_normal_(self.conv3.weight)\n",
        "        nn.init.xavier_normal_(self.conv4.weight)\n",
        "        nn.init.xavier_normal_(self.conv5.weight)\n",
        "        nn.init.xavier_normal_(self.dwconv1.weight)\n",
        "        nn.init.xavier_normal_(self.dwconv2.weight)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # to complete\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKLlmOf2Vz5x"
      },
      "source": [
        "## Create the building blocks for training\n",
        "Create an instance of the network, the loss function, the optimizer, and learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZorYOv0Vz5y"
      },
      "outputs": [],
      "source": [
        "# net = CD_CNN()\n",
        "\n",
        "# create loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# create SGD optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "# create learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n",
        "\n",
        "# experiment ID\n",
        "experiment_ID = \"%s_%s_%s_bs(%d)lr(%.4f_%d_%.1f)m(%.1f)e(%d)act(%s)xavier(yes)da(%s)do(%.1f)BN\" % (type(net).__name__, type(criterion).__name__, type(optimizer).__name__,\n",
        "                batch_size, learning_rate, lr_step_size, lr_gamma, momentum, epochs, type(activation).__name__, data_augmentation, dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sicz_zPTVz50"
      },
      "source": [
        "##Â Create datasets\n",
        "This includes training/validation split, where possible. In our example, MNIST does not have a validation set, so we use the test set as validation set (warning: see comments in the code).\n",
        "<br>_Note: in general, you might need to implement your own Dataset in a separate Python file, and then import it in this file in order to create the dataset. The training/validation/test data split is also on your own, you may consider to embed it in your Dataset class_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDZ7FEL6Vz51"
      },
      "outputs": [],
      "source": [
        "# create datasets, transforms will be set after (or can be set here, if we had all we need)\n",
        "# NOTE: torchvision MNIST has no validation set, we will use the test set as validation set\n",
        "#       but in a real case scenario we MUST use a train / validation / test split to avoid\n",
        "#       introducing biases in our results (final model performance SHOULD NOT be evaluated\n",
        "#       on the validation set)\n",
        "dataset_train = torchvision.datasets.MNIST(\"./mnist\", train=True, download=False)\n",
        "dataset_valid = torchvision.datasets.MNIST(\"./mnist\", train=False, download=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rUXr7ClVz54"
      },
      "source": [
        "## Check your data _before_ transforms are applied\n",
        "This may sound naive, but the most recurring problem in Machine Learning and Deep Learning is that the model is fed with _wrong data_. This can be caused by incorrect data loading, processing, etc. Even if you are 100% sure your data are correct, you should _always_ check your data _before_ and _after_ transforms are applied. A good practice is to calculate and print statistics, or even displaying data where feasible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvDfTbDZVz54"
      },
      "outputs": [],
      "source": [
        "# check your original data, before applying any transform\n",
        "# NOTE: we also calculate data mean and standard deviation\n",
        "print (\"\\nTrain data are %d, with shape %s\" % (len(dataset_train), dataset_train.data.shape))\n",
        "mu = dataset_train.data.float().mean()\n",
        "std = dataset_train.data.float().std()\n",
        "print (\"...with mean %.1f and standard deviation %.1f\" % (mu, std))\n",
        "print (\"...with labels %s, %s, %s, ...\" % (dataset_train.targets[0], dataset_train.targets[1], dataset_train.targets[2]))\n",
        "mu_valid = dataset_valid.data.float().mean()\n",
        "std_valid = dataset_valid.data.float().std()\n",
        "print (\"\\nValidation data are %d, with shape %s\" % (len(dataset_valid), dataset_valid.data.shape))\n",
        "print (\"...with mean %.1f and standard deviation %.1f\" % (mu_valid, std_valid))\n",
        "print (\"...with labels %s, %s, %s, ...\" % (dataset_valid.targets[0], dataset_valid.targets[1], dataset_valid.targets[2]))\n",
        "# visual check\n",
        "for i in range(0, display_first_n):\n",
        "    plt.imshow(dataset_train.data[i], cmap='gray')\n",
        "    plt.title('Training Sample %d' % i)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKfOvnlcVz58"
      },
      "source": [
        "## Define data transforms\n",
        "Data transforms are applied sample-wise at _batch generation_ time: they are _not_ applied until you use a Dataloader and fetch data from it. In general, they serve to transform your data into what the neural network expects. Data should be _at least_ converted to tensors whose shape corresponds to network input, and possibly normalized so as to be 0-centered roughly in the [-1,1] range. \n",
        "\n",
        "In this example, we also apply a transform to the targets (labels), so as to have one-hot tensor that can be compared with network outputs using the loss function.\n",
        "\n",
        "Optionally, we may also apply data augmentation (on the training set, only)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2SR9L8JVz58"
      },
      "outputs": [],
      "source": [
        "# define Convert transform to convert MNIST images to torch float tensors\n",
        "# the operations are (in sequence):\n",
        "# - 'np.array' to convert the image to a numpy array\n",
        "# - 'torch.from_numpy' to convert the numpy array to torch tensor\n",
        "# - 'torch.unsqueeze' to add a singleton channel dimension so as to have 1x28x28 instead of 28x28\n",
        "#    since torchvision.transforms want CxHxW tensors\n",
        "# - '.float()' to convert to float tensors, since deep learning builds on float numbers\n",
        "class Convert(object):\n",
        "    def __call__(self, img):\n",
        "        return torch.unsqueeze(torch.from_numpy(np.array(img)), 0).float()\n",
        "\n",
        "# define data transform as a composition of Data Augmentation (training only), Convert, Normalize, and Reshape\n",
        "# here, Normalize implements standardization using the previously computed mu and std\n",
        "DataAugmentation = transforms.RandomApply(\n",
        "        [transforms.RandomRotation(20, fill=(0,))], p=0.5)  # fill=(0,) is a workaround for the torchvision bug tracked at https://github.com/pytorch/vision/issues/1759#issuecomment-575307516\n",
        "transform_train = transforms.Compose(\n",
        "    [DataAugmentation,\n",
        "     Convert(),\n",
        "     transforms.Normalize(mean=[mu], std=[std])])\n",
        "transform_test = transforms.Compose(\n",
        "    [Convert(),\n",
        "     transforms.Normalize(mean=[mu], std=[std])])\n",
        "\n",
        "# set data and target transforms on both datasets\n",
        "# NOTE: always use the SAME transforms (except for data augmentation) on both training and validation/test sets\n",
        "#       to avoid introducing biases\n",
        "if data_augmentation:\n",
        "    dataset_train.transform = transform_train\n",
        "else:\n",
        "    dataset_train.transform = transform_test\n",
        "dataset_valid.transform = transform_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F55AH4hEVz5_"
      },
      "source": [
        "## Create data loaders\n",
        "Dataloaders are in-built PyTorch objects that serve to sample batches from datasets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVj-FUwxVz5_"
      },
      "outputs": [],
      "source": [
        "# create data loaders\n",
        "# NOTE 1: shuffle helps training\n",
        "# NOTE 2: in test mode, batch size can be as high as the GPU can handle (faster, but requires more GPU RAM)\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True) \n",
        "dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=512, num_workers=num_workers, pin_memory=True)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b82rYDUbVz6C"
      },
      "source": [
        "## Check your data _after_ transforms are applied\n",
        "To check what the network will see at train/test time, you have to use dataloaders which will apply the data transforms previously defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d66M8LFVz6C"
      },
      "outputs": [],
      "source": [
        "# define batch_show function to show MNIST data\n",
        "# assume 'img' is a standardised tensor\n",
        "# remember: tensor images are stored in channel-width-height (CWH) order\n",
        "# remember: pyplot expects images to be in width-height-channel (WHC) order\n",
        "def batch_show(img, batch_i):\n",
        "    img = img*std + mu  # un-normalize\n",
        "    img = img/255       # move data to [0,1] since pyplot expects float images to be in [0,1]\n",
        "    npimg = img.numpy() # convert to numpy, since pyplot expects numpy images\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # CHW to WHC reshape\n",
        "    plt.title('Training Batch %d' % batch_i)\n",
        "    plt.show()\n",
        "\n",
        "# visual check\n",
        "for i, minibatch in enumerate(dataloader_train):\n",
        "    if i >= display_first_n:\n",
        "        break\n",
        "    data, labels = minibatch\n",
        "    # data have size batch_size x 784, with .view we reshape\n",
        "    # data so as to have batch_size x 1 (channel) x 28 x 28\n",
        "    batch_show(torchvision.utils.make_grid(data.view(-1, 1, 28, 28)), i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taA4YIRdVz6F"
      },
      "source": [
        "## Define train function\n",
        "It is preferable (but not mandatory) to embed training (1 epoch) code into a function, and call that function later during the training phase, at each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzDenAZaVz6G"
      },
      "outputs": [],
      "source": [
        "# define train function (1 epoch)\n",
        "# returns average loss and accuracy\n",
        "def train(dataset, dataloader):\n",
        "\n",
        "    # switch to train mode\n",
        "    net.train()\n",
        "\n",
        "    # reset performance measures\n",
        "    loss_sum = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    # 1 epoch = 1 complete loop over the dataset\n",
        "    for batch in dataloader:\n",
        "\n",
        "        # get data from dataloader\n",
        "        inputs, targets = batch\n",
        "\n",
        "        # move data to device\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # loss gradient backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # net parameters update\n",
        "        optimizer.step()\n",
        "\n",
        "        # accumulate loss\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        # accumulate correct outputs (for accuracy calculation)\n",
        "        outputs_max = torch.argmax(outputs, dim=1)\n",
        "        targets_max = targets #torch.argmax(targets, dim=1)\n",
        "        correct += outputs_max.eq(targets_max).sum().float()\n",
        "\n",
        "    # step learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # return average loss and accuracy\n",
        "    return loss_sum / len(dataloader), 100. * correct / len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xrv9oS5Vz6I"
      },
      "source": [
        "## Define test function\n",
        "It is preferable (but not mandatory) to embed the test code into a function, and call that function whenever needed. For instance, during training for validation at each epoch, or after training for testing, or for deploying the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3kTDWVuVz6K"
      },
      "outputs": [],
      "source": [
        "# define test function\n",
        "# returns predictions\n",
        "def test(dataset, dataloader):\n",
        "\n",
        "    # switch to test mode\n",
        "    net.eval()  \n",
        "\n",
        "    # initialize predictions\n",
        "    predictions = torch.zeros(len(dataset), dtype=torch.int64)\n",
        "    sample_counter = 0\n",
        "\n",
        "    # do not accumulate gradients (faster)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # test all batches\n",
        "        for batch in dataloader:\n",
        "\n",
        "            # get data from dataloader [ignore labels/targets as they are not used in test mode]\n",
        "            inputs = batch[0]\n",
        "\n",
        "            # move data to device\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            # store predictions\n",
        "            outputs_max = torch.argmax(outputs, dim=1)\n",
        "            for output in outputs_max:\n",
        "                predictions[sample_counter] = output\n",
        "                sample_counter += 1\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg9J1zMDVz6M"
      },
      "source": [
        "## Train a new model or test a pretrained one\n",
        "The code below also includes visual loss/accuracy monitoring during training, both on training and validation sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUumAQckVz6N"
      },
      "outputs": [],
      "source": [
        "# pretrained model not available --> TRAIN a new one and save it\n",
        "if not pretrained:\n",
        "    \n",
        "    # reset performance monitors\n",
        "    losses = []\n",
        "    train_accuracies = []\n",
        "    valid_accuracies = []\n",
        "    ticks = []\n",
        "    \n",
        "    # move net to device\n",
        "    net.to(device)\n",
        "    \n",
        "    # start training\n",
        "    for epoch in range(1, epochs+1):\n",
        "\n",
        "        # measure time elapsed\n",
        "        t0 = time.time()\n",
        "        \n",
        "        # train\n",
        "        avg_loss, accuracy_train = train(dataset_train, dataloader_train)\n",
        "\n",
        "        # test on validation\n",
        "        predictions = test(dataset_valid, dataloader_valid)\n",
        "        accuracy_valid = 100. * predictions.eq(dataset_valid.targets).sum().float() / len(dataset_valid)\n",
        "                    \n",
        "        # update performance history\n",
        "        losses.append(avg_loss)\n",
        "        train_accuracies.append(accuracy_train.cpu())\n",
        "        valid_accuracies.append(accuracy_valid.cpu())\n",
        "        ticks.append(epoch)\n",
        "\n",
        "        # print or display performance\n",
        "        if not monitor_display:\n",
        "            print (\"\\nEpoch %d\\n\"\n",
        "                \"...TIME: %.1f seconds\\n\"\n",
        "                \"...loss: %g (best %g at epoch %d)\\n\"\n",
        "                \"...training accuracy: %.2f%% (best %.2f%% at epoch %d)\\n\"\n",
        "                \"...validation accuracy: %.2f%% (best %.2f%% at epoch %d)\" % (\n",
        "                epoch,\n",
        "                time.time()-t0,\n",
        "                avg_loss, min(losses), ticks[np.argmin(losses)],\n",
        "                accuracy_train, max(train_accuracies), ticks[np.argmax(train_accuracies)],\n",
        "                accuracy_valid, max(valid_accuracies), ticks[np.argmax(valid_accuracies)]))\n",
        "        else:\n",
        "            fig, ax1 = plt.subplots(figsize=(12, 8), num=1)\n",
        "            ax1.set_xticks(np.arange(0, epochs+1, step=epochs/10.0))\n",
        "            ax1.set_xlabel('Epochs')\n",
        "            ax1.set_ylabel(type(criterion).__name__, color='blue')\n",
        "            ax1.set_ylim(0.0001, 1)\n",
        "            ax1.tick_params(axis='y', labelcolor='blue')\n",
        "            ax1.set_yscale('log')\n",
        "            ax1.plot(ticks, losses, 'b-', linewidth=1.0, aa=True, \n",
        "                label='Training (best at ep. %d)' % ticks[np.argmin(losses)])\n",
        "            ax1.legend(loc=\"lower left\")\n",
        "            ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "            ax2.set_ylabel('Accuracy %', color='red')\n",
        "            ax2.set_ylim(90, 100)\n",
        "            ax2.set_yticks(np.arange(90, 100, step=1))\n",
        "            ax2.tick_params(axis='y', labelcolor='red')\n",
        "            ax2.plot(ticks, train_accuracies, 'r-', linewidth=1.0, aa=True, \n",
        "                label='Training (%.2f%%, best %.2f%% at ep. %d)' % (accuracy_train, max(train_accuracies), ticks[np.argmax(train_accuracies)]))\n",
        "            ax2.plot(ticks, valid_accuracies, 'r--', linewidth=1.0, aa=True, \n",
        "                label='Validation (%.2f%%, best %.2f%% at ep. %d)' % (accuracy_valid, max(valid_accuracies), ticks[np.argmax(valid_accuracies)]))\n",
        "            ax2.legend(loc=\"lower right\")\n",
        "            plt.xlim(0, epochs+1)\n",
        "            # this works if running from notebooks\n",
        "            if run_from_notebook:\n",
        "                fig.show()\n",
        "                fig.canvas.draw()\n",
        "            # this works if running from console\n",
        "            else:\n",
        "                plt.draw()\n",
        "                #plt.pause(0.001)\n",
        "                plt.show()\n",
        "           # plt.savefig(experiment_ID + \".png\", dpi=300)\n",
        "            fig.clear()\n",
        "\n",
        "        # save model if validation performance has improved\n",
        "        if (epoch-1) == np.argmax(valid_accuracies):\n",
        "            torch.save({\n",
        "                'net': net,\n",
        "                'accuracy': max(valid_accuracies),\n",
        "                'epoch': epoch\n",
        "            }, experiment_ID + \".tar\")\n",
        "\n",
        "# pretrained model available -> load it and test\n",
        "else:\n",
        "\n",
        "    # load pretrained model\n",
        "    checkpoint = torch.load(experiment_ID + \".tar\", map_location=lambda storage, loc: storage)\n",
        "    net = checkpoint['net']\n",
        "    print (\"Loaded pretrained model\\n...trained for %d epochs\\n...reached accuracy %.2f\" % (checkpoint['epoch'], checkpoint['accuracy']))\n",
        "\n",
        "    # move net to device\n",
        "    net.to(device)\n",
        "\n",
        "    # test\n",
        "    predictions = test(dataset_valid, dataloader_valid)\n",
        "    accuracy = 100. * predictions.eq(dataset_valid.targets).sum().float() / len(dataset_valid)\n",
        "    print (\"Accuracy on test set is %.2f\" % accuracy)\n",
        "\n",
        "    # display errors\n",
        "    if display_errors:\n",
        "\n",
        "        # predictions / target comparisons = 1 for match, 0 for mismatch\n",
        "        # we subtract 1, so we have 0 for match, -1 for mismatch\n",
        "        # nonzero elements are thus all mismatches\n",
        "        errors = torch.nonzero(~predictions.eq(dataset_valid.targets))\n",
        "\n",
        "        # get errors samples and convert them to torch tensors\n",
        "        error_samples = torch.zeros(len(errors), 1, 28, 28)\n",
        "        conversion = Convert()\n",
        "        for i, e in enumerate(errors):\n",
        "            error_samples[i] = conversion(dataset_valid.data[e.item()])\n",
        "\n",
        "        # make a grid of images and show\n",
        "        img = torchvision.utils.make_grid(error_samples, nrow=20)\n",
        "        img = img/255       # move data to [0,1] since pyplot expects float images to be in [0,1]\n",
        "        npimg = img.numpy() # convert to numpy, since pyplot expects numpy images\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))  # CHW to WHC reshape\n",
        "        plt.title('Errors')\n",
        "        plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CD_CNN_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}