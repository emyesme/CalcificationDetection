{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wthu5HssWsF3"
      },
      "source": [
        "\n",
        "# Load the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dd477p2erqy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDRr9M44sYxK"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "\n",
        "#first put a shortcut in your drive to the image processing folder\n",
        "\n",
        "#Choose the image folder you want to try\n",
        "\n",
        "pipeFolder = '10.8Prepro+glcm+fixed+CorrectedLabels'\n",
        "\n",
        "RESULTS_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results', \n",
        "                        pipeFolder)\n",
        "\n",
        "\n",
        "RESULTS_DIR_MAIN = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results')\n",
        "\n",
        "\n",
        "\n",
        "MODELS_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Results', \n",
        "                        'MLModelsFinal')\n",
        "\n",
        "DATA_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Image Processing and Analysis 2022',\n",
        "                        'projects',\n",
        "                        'Calcification Detection',\n",
        "                        'dataset')\n",
        "\n",
        "\n",
        "print(os.listdir(RESULTS_DIR))\n",
        "\n",
        "results_file = os.listdir(RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-OnlgGfsSdp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5os_ORY98A4Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-TQuxisz-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import tree\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import pickle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMqDYgXyktxL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier # random forest\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJbtKF-ylFf0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "#!pip install fastprogress\n",
        "from fastprogress import master_bar, progress_bar\n",
        "#!pip install tqdm\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from skimage import measure\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neMctBoMuIMD"
      },
      "outputs": [],
      "source": [
        "def read_results_train_test(path, keys):\n",
        "  df_result = pd.DataFrame()\n",
        "  for result in os.listdir(path):\n",
        "    if result.split('.')[-1] == 'csv':\n",
        "      if int(result.split('_')[0]) in list(keys):\n",
        "        try:\n",
        "          df = pd.read_csv(path+'/'+result)\n",
        "          df_result = df_result.append(df)\n",
        "        except:\n",
        "          print(\"Empty file \", result)\n",
        "  return df_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkqzRwsJlfUs"
      },
      "outputs": [],
      "source": [
        "def read_results(path):\n",
        "  dfs = pd.DataFrame()\n",
        "  for result in results_file:\n",
        "    try:\n",
        "      df = pd.read_csv(path+'/'+result)\n",
        "    except:\n",
        "      print(\"Empty file \", result)\n",
        "\n",
        "    dfs = dfs.append(df)\n",
        "  \n",
        "  return dfs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huyN37RmC_uw"
      },
      "outputs": [],
      "source": [
        "def train_test_function():\n",
        "  df_train_test = pd.read_csv(os.path.join('/content',\n",
        "                                'drive',\n",
        "                                'MyDrive',\n",
        "                                'Results',\n",
        "                                'standard_partitions.csv'), index_col=0)\n",
        "  train_keys = df_train_test.loc[df_train_test.partition == 'train']['image_id'].values\n",
        "  test_keys = df_train_test.loc[df_train_test.partition == 'test']['image_id'].values\n",
        "\n",
        "\n",
        "  dfs_train = read_results_train_test(RESULTS_DIR, train_keys)\n",
        "  dfs_test = read_results_train_test(RESULTS_DIR, test_keys)\n",
        "  return dfs_train,dfs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UKgbKXPDJA9"
      },
      "outputs": [],
      "source": [
        "# dfs_train, dfs_test = train_test_function()\n",
        "# dfs_train.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiwTtcPjrMRK"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udO3hTL2c0W7"
      },
      "outputs": [],
      "source": [
        "# We will balance data randomly taking  the same amount of 0 and 1 label inputs. There are several options to manage unbalanced data,\n",
        "# this is just one approach, we should try different approaches.\n",
        "# https://towardsdatascience.com/comparing-different-classification-machine-learning-models-for-an-imbalanced-dataset-fdae1af3677f\n",
        "def rus(dfs, not_consider=0):\n",
        "  # df_negative = dfs.loc[dfs.label == dfs.label.value_counts().idxmax()] #This is for making it more general\n",
        "  df_negative = dfs.loc[dfs.label == 0] #Take all negative samples from dfs -> label = 0\n",
        "  print('rus negative 1: ', len(df_negative))\n",
        "  print('rus negative 1 without duplicates: ', len(df_negative.drop_duplicates()))\n",
        "  #Here we are taking a subset from the 'negative pool' called df_negative with length equal to the number of positive candidates I have\n",
        "  if not_consider >= len(dfs.loc[dfs.label == 1]):\n",
        "    df_0 = pd.DataFrame()\n",
        "  else:\n",
        "    df_0 = df_negative.sample(len(dfs.loc[dfs.label == 1])-not_consider, random_state = 1) #not consider is the number of negative samples that I already have and that I misclassified the first time\n",
        "  print('df_0: ', len(df_0))\n",
        "  print('append ', len(df_negative.append(df_0)))\n",
        "  print('df_negative drop no keeping', len(df_negative.drop_duplicates(keep=False)))\n",
        "  df_negative = df_negative.append(df_0).drop_duplicates(keep=False) #df_negative is the samples that have not been used, and have not been selected yet for the training\n",
        "  print('after drop duplicates with appended: ',len(df_negative))\n",
        "  dfs_eq = df_0.append(dfs.loc[dfs.label == 1]) #dfs.label.value_counts().idxmin() #Here we are taking the balanced tada\n",
        "  print(\"Length of balanced data: \", len(dfs_eq))\n",
        "  print(dfs_eq['label'].value_counts())\n",
        "  return dfs_eq, df_negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLoDror0v6el"
      },
      "outputs": [],
      "source": [
        "def plotFeatures(data):\n",
        "  #plot the scatter matrix\n",
        "  pd.plotting.scatter_matrix(data,figsize=(25,25))\n",
        "  #correlation plot\n",
        "  corr = data.corr()\n",
        "  f, ax = plt.subplots(figsize=(25, 25))\n",
        "  sns.heatmap(corr,annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNRW1sKxHoOu"
      },
      "outputs": [],
      "source": [
        "def standardScaler(X_train, X_test):\n",
        "  standard_scaler = preprocessing.StandardScaler()\n",
        "  X_train = standard_scaler.fit_transform(X_train)\n",
        "  X_test = standard_scaler.transform(X_test)\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXE13LCNrZcX"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0Icd_DnzSdZ"
      },
      "outputs": [],
      "source": [
        "def featureSelectionTrees(estimators, X_train, y_train, X_test):\n",
        "  clf = ExtraTreesClassifier(n_estimators=estimators)\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "  model = SelectFromModel(clf, prefit=True)\n",
        "  X_train = model.transform(X_train)\n",
        "  X_test = model.transform(X_test)\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnQpMHUI-0T"
      },
      "outputs": [],
      "source": [
        "def pcaAnalysis(components, X_train, X_test):\n",
        "  pca = PCA(components)\n",
        "  X_train = pca.fit_transform(X_train)\n",
        "  X_test = pca.transform(X_test)\n",
        "  print(\"PCA variance ratio: \", pca.explained_variance_ratio_)\n",
        "  print(\"Total variance Explained by PCA: \", sum(pca.explained_variance_ratio_))\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTqXxdqBLFYS"
      },
      "outputs": [],
      "source": [
        "def selectFeaturesChi(k, X_train, X_test, y_train):\n",
        "  sel = SelectKBest(chi2, k=k)\n",
        "  sel.fit(X_train, y_train)\n",
        "  X_train = sel.transform(X_train)\n",
        "  X_test = sel.transform(X_test)\n",
        "  return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b3nb74MroMO"
      },
      "source": [
        "# **Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGbJYG0szmJW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import matthews_corrcoef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN7GRye0Fjb2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def RandomForest(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    print(\"Searching for best hyperparameters\")\n",
        "    params = {'criterion': ['gini'],\n",
        "              'n_estimators': [100], # , 500, 900\n",
        "              'max_features': ['auto', 'sqrt'],#, 'sqrt', 'log2'\n",
        "              'max_depth' : [10, 12]}\n",
        "    grid = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs = -1), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for rf are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = RandomForestClassifier(random_state=42, n_jobs = -1)\n",
        "  \n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI8BeXyIJIqj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier # gradient boosting regressor\n",
        "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
        "def GradientBoosting(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'learning_rate': [0.1],#0.05, 0.2\n",
        "              #'min_samples_split': [0.5, 0.8],\n",
        "              #'min_samples_leaf': [0.1, 0.2, 0.5],\n",
        "              'max_depth':[8],\n",
        "              #'max_features':['sqrt'],#'log2'\n",
        "              #'criterion': ['friedman_mse',  'mae'],\n",
        "              #'subsample':[0.5, 1.0],\n",
        "              'n_estimators':[600]}\n",
        "    grid = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for gb are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "\n",
        "  classifier = GradientBoostingClassifier(random_state=42)\n",
        "  \n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZcwaXUQK71l"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier # adaboost, adaboostRegressor for regression problems\n",
        "\n",
        "def AdaBoost(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'n_estimators': [600],\n",
        "              'learning_rate': [0.5]}\n",
        "    grid = GridSearchCV(AdaBoostClassifier(random_state=42), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for ab are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiHOO-9tLlBK"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def LogRegre(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'multi_class':['multinomial'],\n",
        "              'solver': ['sag'],#,'saga'\n",
        "              'penalty': ['l2']}#'elasticnet',\n",
        "    grid = GridSearchCV(LogisticRegression(random_state=42), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for lrg are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = LogisticRegression(random_state=42)\n",
        "  \n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkiwq_C0lD38"
      },
      "outputs": [],
      "source": [
        "def KNN(X_train, y_train, cv=5, best_params = dict()):\n",
        "  if len(best_params) == 0:\n",
        "    params = {'n_neighbors':[3,5]}\n",
        "    grid = GridSearchCV(KNeighborsClassifier(), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "  classifier = KNeighborsClassifier()\n",
        "\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hohRhuWbs3W"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def DecisionTree(X_train, y_train, cv=5, best_params = dict()):\n",
        "  classifier = tree.DecisionTreeClassifier()\n",
        " \n",
        "\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjVbyTcmwekE"
      },
      "outputs": [],
      "source": [
        "# This is an example of how to use a Pipe inside a function we are training, as done in Challenge 3 by the professor\n",
        "def SVC_linear(X_train, y_train, cv=5, best_params = dict()):\n",
        "\n",
        "  if len(best_params) == 0:\n",
        "    lower_value_C = 1\n",
        "    higher_value_C = 10\n",
        "    n_values = 10\n",
        "    base = 10\n",
        "    params = {'C': [1, 3, 5,9.11], #12\n",
        "              'kernel' : ['rbf'],\n",
        "              'gamma': [2.5, 5, 10]}\n",
        "\n",
        "    grid = GridSearchCV(SVC(random_state = 42, probability=True), param_grid=params, cv=cv, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    print(\"The best parameters for svm are %s with an accuracy of %0.4f\"%(best_params, grid.best_score_))\n",
        "  \n",
        "  classifier = SVC(random_state = 42, probability=True)\n",
        "\n",
        "  return classifier, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox_3v0fW-Yh8"
      },
      "outputs": [],
      "source": [
        "def cv_classification(classifier, params, X_train, X_test, y_train, y_test, candidates_train, candidates_test, normals, clf_name, imageKeyLocation):\n",
        "  \n",
        "  trainDataFull = pd.DataFrame(X_train).copy()\n",
        "  ss_f1 = StandardScaler()\n",
        "  trainDataFull = pd.DataFrame(ss_f1.fit_transform(trainDataFull), columns=X_train.columns)\n",
        "  trainDataFull['label'] = y_train\n",
        "\n",
        "  dataBalanced, dfNegative = rus(trainDataFull)\n",
        "  print(\"Data Balanced: \", len(dataBalanced))\n",
        "  print(\"Negative pool: \",len(dfNegative))\n",
        "\n",
        "  X_train_f1 = dataBalanced.drop('label', axis=1)\n",
        "  y_train_f1 = dataBalanced['label']\n",
        "\n",
        "  clf1 = clone(classifier)\n",
        "  clf1.set_params(**params)\n",
        "  cvf1Score = []\n",
        "  cvMccScore = []\n",
        "  cvpAUCScore = []\n",
        "\n",
        "  print(\"******************** RESULTS FOLD 1 *****************************\")\n",
        "  selectedFold = 1\n",
        "  clf1.fit(X_train_f1, y_train_f1)\n",
        "  print('TRAIN')\n",
        "  y_pred_tr_1 = clf1.predict(X_train_f1)\n",
        "  print(confusion_matrix(y_train_f1, y_pred_tr_1))\n",
        "  print(f1_score(y_train_f1, y_pred_tr_1))\n",
        "\n",
        "  y_pred_proba_tr = clf1.predict_proba(X_train_f1)[:, 1]\n",
        "  print('pAUC train: ', roc_auc_score(y_train_f1, y_pred_proba_tr, max_fpr = 0.0001))\n",
        "\n",
        "  \n",
        "  print('TEST')\n",
        "  y_pred_1 = clf1.predict(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))\n",
        "  print(confusion_matrix(y_test, y_pred_1))\n",
        "  print(f1_score(y_test, y_pred_1))\n",
        "  y_pred_proba = clf1.predict_proba(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))[:, 1]\n",
        "  print('pAUC test: ', roc_auc_score(y_test, y_pred_proba, max_fpr = 0.0001))\n",
        "\n",
        "  print(classification_report(y_test, y_pred_1))  \n",
        "  print('F1 score test: ', f1_score(y_test, y_pred_1, average='binary'))\n",
        "\n",
        "  df_cm = confusion_matrix(y_test, y_pred_1)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(df_cm, annot=True, fmt='d') # font size\n",
        "  plt.show()\n",
        "\n",
        "  filename = f'best_model_{clf_name}_{selectedFold}_{pipeFolder}_rus_scaled.pkl'\n",
        "  pickle.dump(clf1, open(os.path.join(MODELS_DIR,'rus_scaled', filename), 'wb'))\n",
        "  print(f\"Fold {selectedFold} of {clf_name} was saved!\")  \n",
        "\n",
        "\n",
        "  cvf1Score.append(f1_score(y_test, y_pred_1, average='binary'))\n",
        "  cvMccScore.append(matthews_corrcoef(y_test, y_pred_1))\n",
        "  cvpAUCScore.append(roc_auc_score(y_test, clf1.predict_proba(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))[:, 1], max_fpr = 0.0001))\n",
        "\n",
        "\n",
        "  candidates_unique = [cand.split('_')[imageKeyLocation] for cand in candidates_test['name'].values.tolist()]\n",
        "  candidates_unique = list(set(candidates_unique))\n",
        "  groundTruthShort = [i for i in groundTruths if i.split('_')[0] in candidates_unique]\n",
        "  print(len(groundTruthShort))\n",
        "  normals_f1 = [i for i in normals if i in candidates_unique]\n",
        "\n",
        "\n",
        "  fn_1, dfROC_1, filename_key_1 = calculateFROC(groundTruthShort, normals_f1, candidates_test, y_pred_proba, clf_name, pipeFolder)\n",
        "\n",
        "  print(\"*********************** RESULTS FOLD 2 *******************************\")\n",
        "  selectedFold = 2\n",
        "  trainDataFull = pd.DataFrame(X_test).copy()\n",
        "  ss_f2 = StandardScaler()\n",
        "  trainDataFull = pd.DataFrame(ss_f2.fit_transform(trainDataFull), columns=X_test.columns)\n",
        "  trainDataFull['label'] = y_test\n",
        "\n",
        "  dataBalanced, dfNegative = rus(trainDataFull)\n",
        "  print(\"Data Balanced: \", len(dataBalanced))\n",
        "  print(\"Negative pool: \",len(dfNegative))\n",
        "\n",
        "  X_test_f2 = dataBalanced.drop('label', axis=1)\n",
        "  y_test_f2 = dataBalanced['label']\n",
        "\n",
        "  clf2 = clone(classifier)\n",
        "  clf2.set_params(**params)\n",
        "  clf2.fit(X_test_f2, y_test_f2)\n",
        "\n",
        "  print('TRAIN')\n",
        "  y_pred_tr_2 = clf2.predict(X_test_f2)\n",
        "  print(confusion_matrix(y_test_f2, y_pred_tr_2))\n",
        "  print(f1_score(y_test_f2, y_pred_tr_2))\n",
        "  y_pred_proba_tr = clf2.predict_proba(X_test_f2)[:, 1]\n",
        "  print('pAUC train: ', roc_auc_score(y_test_f2, y_pred_proba_tr, max_fpr = 0.0001))\n",
        "\n",
        "  print('TEST')\n",
        "  y_pred_2 = clf2.predict(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))\n",
        "  print(confusion_matrix(y_train, y_pred_2))\n",
        "  print(f1_score(y_train, y_pred_2))\n",
        "  y_pred_proba_2 = clf2.predict_proba(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))[:, 1]\n",
        "  print('pAUC test: ', roc_auc_score(y_train, y_pred_proba_2, max_fpr = 0.0001))\n",
        "\n",
        "  print(classification_report(y_train, y_pred_2))  \n",
        "\n",
        "  print('F1 score: ', f1_score(y_train, y_pred_2, average='binary'))\n",
        "  df_cm = confusion_matrix(y_train, y_pred_2)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(df_cm, annot=True, fmt='d') # font size\n",
        "  plt.show()\n",
        "\n",
        "  filename = f'best_model_{clf_name}_{selectedFold}_{pipeFolder}_rus_scaled.pkl'\n",
        "  pickle.dump(clf2, open(os.path.join(MODELS_DIR,'rus_scaled', filename), 'wb'))\n",
        "  print(f\"Fold {selectedFold} of {clf_name} was saved!\")  \n",
        "\n",
        "  cvf1Score.append(f1_score(y_train, y_pred_2, average='binary'))\n",
        "  cvMccScore.append(matthews_corrcoef(y_train, y_pred_2))\n",
        "  cvpAUCScore.append(roc_auc_score(y_train, clf2.predict_proba(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))[:, 1], max_fpr = 0.0001))\n",
        "  print(\"Mean CV_score F1-score: \", np.mean(cvf1Score))\n",
        "\n",
        "  candidates_unique = [cand.split('_')[imageKeyLocation] for cand in candidates_train['name'].values.tolist()]\n",
        "  candidates_unique = list(set(candidates_unique))\n",
        "  groundTruthShort = [i for i in groundTruths if i.split('_')[0] in candidates_unique]\n",
        "  print(len(groundTruthShort))\n",
        "  normals_f2 = [i for i in normals if i in candidates_unique]\n",
        "\n",
        "  fn_2, dfROC_2, filename_key_2 = calculateFROC(groundTruthShort, normals_f2, candidates_train, y_pred_proba_2, clf_name, pipeFolder)\n",
        "  \n",
        "  \n",
        "  # Choose the best one\n",
        "  if np.array(cvf1Score).argmax() == 0:  \n",
        "    best_f1 = f1_score(y_test, y_pred_1)\n",
        "    best_mcc = matthews_corrcoef(y_test, y_pred_1)\n",
        "    best_pAUC = roc_auc_score(y_test, y_pred_proba, max_fpr = 0.0001)\n",
        "    \n",
        "  else:\n",
        "    best_f1 = f1_score(y_train, y_pred_2)\n",
        "    best_mcc = matthews_corrcoef(y_train, y_pred_2)\n",
        "    best_pAUC = roc_auc_score(y_train, y_pred_proba_2, max_fpr = 0.0001)    \n",
        "\n",
        "\n",
        "  print(\"cvf1Score: \", cvf1Score)\n",
        "  dfROC_1 = dfROC_1.append(dfROC_2)\n",
        "\n",
        "  return np.array(cvf1Score).argmax(),[np.mean(cvpAUCScore), np.mean(cvf1Score), np.mean(cvMccScore)], [best_pAUC, best_f1, best_mcc], dfROC_1, fn_1+fn_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXAhatZZ_Ap-"
      },
      "source": [
        "# **Scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgc5Kg2FLMMC"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "\n",
        "#first put a shortcut in your drive to the image processing folder\n",
        "\n",
        "DATA_DIR = os.path.join('/content',\n",
        "                        'drive',\n",
        "                        'MyDrive',\n",
        "                        'Image Processing and Analysis 2022',\n",
        "                        'projects',\n",
        "                        'Calcification Detection',\n",
        "                        'dataset')\n",
        "\n",
        "\n",
        "print(os.listdir(DATA_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao5mV9BCL4sa"
      },
      "outputs": [],
      "source": [
        "_, _, groundTruths = next(os.walk(os.path.join(DATA_DIR, 'groundtruths')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U-oRZ_h_AJB"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def calculateFROC(groundTruthsList, normals, candidates, prediction, model, pipeline):\n",
        "\n",
        "  #List needed for counting\n",
        "  evaluationList = []\n",
        "\n",
        "  # evaluation froc curve #\n",
        "\n",
        "  fn = 0 # false negative, for the blobs that do not belong to any component\n",
        "  positive_candidates = 0\n",
        "  flag = True\n",
        "\n",
        "  rowListdfROC = []\n",
        "\n",
        "  candidates_copy= candidates.copy()\n",
        "  candidates_copy['prediction'] = prediction\n",
        "\n",
        "  for imageKey in tqdm_notebook(groundTruthsList):\n",
        "\n",
        "\n",
        "    evaluationList = []\n",
        " \n",
        "    # list of features found with y,x and sigma\n",
        "    candidatesImg = candidates_copy.loc[candidates_copy['name'].str.contains(imageKey.split('_')[0], regex=False)]\n",
        "    candidates_number = len(candidatesImg)\n",
        "#    print('cand number: ', candidates_number)\n",
        "\n",
        "    mask = cv2.imread(os.path.join(DATA_DIR, 'groundtruths', imageKey), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        " #   print('# IMAGEKEY: ', imageKey)\n",
        "    blobs = mask > 0.7 * mask.mean() #Thresholding the backgroudnd\n",
        "    blobs_labels, count = measure.label(blobs, background=0, return_num=True) #Getting labels of the connected components and the amount of them without considering the count\n",
        "\n",
        "    # dictionaries\n",
        "    dictCounting={}\n",
        "\n",
        "    for index in range(1, count+1):\n",
        "      dictCounting[index] = 0\n",
        "    \n",
        "    evaluationList = []\n",
        "    dictMean = {}\n",
        "\n",
        "\n",
        "    for index2, candidate in candidatesImg.iterrows():\n",
        "      sigma = 7\n",
        "\n",
        "      if (imageKey.split('_')[0] in normals):\n",
        "        if candidate['name'].split('_')[1] not in dictMean.keys():\n",
        "          dictMean[candidate['name'].split('_')[1]] = [candidate.prediction]\n",
        "        else:\n",
        "          dictMean[candidate['name'].split('_')[1]].append(candidate.prediction)\n",
        "        continue\n",
        "\n",
        "      # is the image a normal image?\n",
        "\n",
        "      if (((candidate.x - sigma) < 0) or \n",
        "        ((candidate.x + sigma) > mask.shape[0]) or \n",
        "        ((candidate.y - sigma) < 0) or\n",
        "        ((candidate.y + sigma) > mask.shape[1])):\n",
        "        continue\n",
        "\n",
        "\n",
        "      # n = 3\n",
        "      left = int(candidate.x - sigma)\n",
        "      right = int(candidate.x + sigma)\n",
        "      top = int(candidate.y - sigma)\n",
        "      bottom = int(candidate.y + sigma) \n",
        "\n",
        "      #    y : y + w , x : x +  h\n",
        "      nonzero = cv2.countNonZero(blobs_labels[top:bottom, left: right])\n",
        "\n",
        "      if nonzero > 0:\n",
        "        # Find all connected components (cc) that intersect with the candidate\n",
        "        foundCC = [i for i in np.unique(blobs_labels[top:bottom, left: right]) if i!= 0]\n",
        "\n",
        "        # Keep the maximum prediction of the candidates that intsersect with at least one component\n",
        "        for cc in foundCC:\n",
        "          dictCounting[cc] = max(candidate.prediction, dictCounting[cc])\n",
        "\n",
        "    if (imageKey.split('_')[0] not in normals):\n",
        "      for key,value in dictCounting.items():\n",
        "        if np.sum(blobs_labels == key) > np.floor(np.pi*(15/2.0)**2):\n",
        "#          print(\"Too big... discarded\")\n",
        "          continue\n",
        "        if value > 0:\n",
        "            rowListdfROC.append(['TP', value])\n",
        "        else:\n",
        "          fn = fn + 1\n",
        "    else:\n",
        "      if len(dictMean.keys()) > 0:\n",
        "        for key, value in dictMean.items():\n",
        "          rowListdfROC.append(['FP', np.max(np.array(value))])\n",
        "\n",
        "\n",
        "  filename_key = 'FROC_calculations_{}_pip{}_fn{}_normals{}_rus_scaled.csv'.format(model, pipeline, fn, len(normals))\n",
        "\n",
        "  dfROC = pd.DataFrame(rowListdfROC, columns=['type', 'prob'])\n",
        "\n",
        "  dfROC.to_csv(os.path.join(MODELS_DIR,\n",
        "                            'rus_scaled',\n",
        "                            filename_key))\n",
        "  \n",
        "  print(\"File saved as \", filename_key)\n",
        "\n",
        "\n",
        "\n",
        "  return fn, dfROC, filename_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-mMXd-6JYpT"
      },
      "outputs": [],
      "source": [
        "def writeFile(df, flag, name):\n",
        "  if(flag):\n",
        "    df.to_csv(os.path.join('/content',\n",
        "                                 'drive',\n",
        "                                 'MyDrive',\n",
        "                                 'Results',\n",
        "                                 name),\n",
        "                    mode='a',\n",
        "                    index=False)\n",
        "    flag = False\n",
        "  else:\n",
        "    df.to_csv(os.path.join('/content',\n",
        "                                 'drive',\n",
        "                                 'MyDrive',\n",
        "                                 'Results',\n",
        "                                 name),\n",
        "                  mode='a',\n",
        "                  header=False,\n",
        "                  index=False)\n",
        "  return flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2iVgV-pBsQA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from numpy import trapz\n",
        "\n",
        "def draw_curve(fn, normals, dfROC, name, clf_name):\n",
        "\n",
        "  experiments_results_path = os.path.join(MODELS_DIR, 'rus_scaled')\n",
        "  results_filename = f'results_AUC_rus_scaled_{pipeFolder}.csv'\n",
        "  confusion_matrix_filename = f'results_ConfMat_rus_scaled_{pipeFolder}.csv'\n",
        "\n",
        "\n",
        "  if results_filename not in os.listdir(experiments_results_path):\n",
        "    results_df = pd.DataFrame(columns=[\"model\", \"AUC_TOTAL\", \"AUC_final_50fPpi\", \"F1_final\"])\n",
        "    results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "  \n",
        "    results_confmat = pd.DataFrame(columns=[\"model\", \"TP\", \"FP\", \"FN\", \"Total True\"])\n",
        "    results_confmat.to_csv(os.path.join(experiments_results_path, confusion_matrix_filename), index=False)\n",
        "  \n",
        "  \n",
        "  else:\n",
        "    results_df = pd.read_csv(os.path.join(experiments_results_path, results_filename))\n",
        "    results_confmat = pd.read_csv(os.path.join(experiments_results_path, confusion_matrix_filename))\n",
        "\n",
        "  tpc = 0\n",
        "  fpc = 0\n",
        "  tpr = []\n",
        "  fppi = []\n",
        "\n",
        "  dfROC['prob'] = [round(i,4) for i in dfROC['prob'].values]\n",
        "  dfROC = dfROC.sort_values('prob', ascending=False)\n",
        "\n",
        "  thresholds = dfROC.prob.unique()\n",
        "  print('Number of thresholds: ', len(thresholds))\n",
        "  print(thresholds)\n",
        "\n",
        "  tp = len(dfROC.loc[dfROC.type == 'TP'])\n",
        "  print('true positives: ', tp)\n",
        "  fp = len(dfROC.loc[dfROC.type == 'FP'])\n",
        "  print('false positives: ',  fp)\n",
        "  print(\"Total number of positives: \", tp+fn)\n",
        "\n",
        "  for i in progress_bar(range(len(thresholds))):\n",
        "\n",
        "    tpc += len(dfROC.loc[(dfROC.prob > thresholds[i]) & (dfROC.type=='TP')])\n",
        "    fpc += len(dfROC.loc[(dfROC.prob > thresholds[i]) & (dfROC.type!='TP')])\n",
        "              \n",
        "    # print('TP amount {} in threshold {}'.format(tpc, thresholds[i]))\n",
        "    # print('FP amount {} in threshold {}'.format(fpc, thresholds[i]))\n",
        "    \n",
        "\n",
        "    tpr.append( tpc/(tp+fn) )\n",
        "    fppi.append( fpc/normals )\n",
        "    tpc = 0\n",
        "    fpc = 0\n",
        "  \n",
        "  gamma = [i for i in fppi if i <= 50 ]\n",
        "  print(max(fppi))\n",
        "  auc_total = trapz(tpr, x=fppi)/max(fppi)\n",
        "  print(max(gamma))\n",
        "  auc_final = trapz(tpr[0:len(gamma)], x=gamma)/max(gamma)\n",
        "\n",
        "  print('AUC TOTAL:', auc_total)\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(fppi, tpr)\n",
        "  ax.set_xlabel('fPpI', fontsize=15)\n",
        "  ax.set_ylabel('TPR', fontsize=15)\n",
        "  ax.grid(True)\n",
        "  plt.ylim(0,1)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  print('AUC final:', auc_final)\n",
        "  fig1, ax1 = plt.subplots()\n",
        "  ax1.plot(gamma, tpr[0:len(gamma)])\n",
        "  ax1.set_xlabel('fPpI', fontsize=12)\n",
        "  ax1.set_ylabel('TPR', fontsize=12)\n",
        "  ax1.grid(True)\n",
        "  plt.ylim(0,1)\n",
        "  plt.xlim(0,50)\n",
        "\n",
        "  plt.savefig(os.path.join(MODELS_DIR,\n",
        "                          'rus_scaled', \n",
        "                           name+'.eps'), format='eps')\n",
        "  plt.show()\n",
        "    \n",
        "  final_f1 = tp / (tp + (fp + fn)/2)\n",
        "\n",
        "  row_results = [clf_name, auc_total, auc_final, final_f1]\n",
        "  results_df = results_df.append(pd.DataFrame([row_results], columns=[\"model\", \"AUC_TOTAL\", \"AUC_final_50fPpi\", \"F1_final\"]))\n",
        "  results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "\n",
        "  confmat_results = [clf_name, tp, fp, fn, tp+fn]\n",
        "  results_confmat = results_confmat.append(pd.DataFrame([confmat_results], columns=[\"model\", \"TP\", \"FP\", \"FN\", \"Total True\"]))\n",
        "  results_confmat.to_csv(os.path.join(experiments_results_path, confusion_matrix_filename), index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQyjMqP1ULaC"
      },
      "outputs": [],
      "source": [
        "def load_and_predict(selectedFold, classifier_name):\n",
        "\n",
        "  #Reading data\n",
        "  dfs_train, dfs_test = train_test_function()\n",
        "  print('Length of train set: ', len(dfs_train))\n",
        "  print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "  dfs_tr_hyperopt = dfs_train.sample(frac=0.1, random_state=42)\n",
        "  dfs_ts_hyperopt = dfs_test.sample(frac=0.1, random_state=42)\n",
        "\n",
        "  dfs_train = dfs_train.append(dfs_tr_hyperopt).drop_duplicates(keep=False)\n",
        "  dfs_test = dfs_test.append(dfs_ts_hyperopt).drop_duplicates(keep=False)\n",
        "  print('Length of train set: ', len(dfs_train))\n",
        "  print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "  try:  \n",
        "    if dfs_train.isnull().values.any():\n",
        "      colm = dfs_train.columns[dfs_train.isna().any()]\n",
        "      dfs_train = dfs_train[dfs_train[colm[0]].notna()]\n",
        "\n",
        "    if dfs_test.isnull().values.any():\n",
        "      colm = dfs_test.columns[dfs_test.isna().any()]    \n",
        "      dfs_test = dfs_test[dfs_test[colm[0]].notna()]\n",
        "\n",
        "    print(\"Train dfs without nans: \", len(dfs_train))\n",
        "    print(\"Test dfs without nans: \",len(dfs_test))\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"No nan values to drop, or not columns\")\n",
        "    print(e)\n",
        "\n",
        "  y_train = dfs_train['label'].reset_index(drop=True)\n",
        "  candidates_train = dfs_train[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "  X_train = dfs_train.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "  y_test = dfs_test['label'].reset_index(drop=True)\n",
        "  candidates_test = dfs_test[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "  X_test = dfs_test.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "  #Obtain train-test datasets\n",
        "  if selectedFold == 2:\n",
        "    chosenCandidates = candidates_train\n",
        "    trainDataFull = pd.DataFrame(X_test).copy()\n",
        "    ss_f2 = StandardScaler()\n",
        "    trainDataFull = pd.DataFrame(ss_f2.fit_transform(trainDataFull), columns=X_test.columns)\n",
        "    trainDataFull['label'] = y_test\n",
        "\n",
        "    model = pickle.load(open(os.path.join(MODELS_DIR,'rus_scaled', f'best_model_{classifier_name}_{selectedFold}_{pipeFolder}_rus_scaled.pkl'), 'rb'))\n",
        "    y_pred = model.predict(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))\n",
        "    y_pred_proba = model.predict_proba(pd.DataFrame(ss_f2.transform(X_train.copy()), columns=X_train.columns))[:,1]\n",
        "\n",
        "  else:\n",
        "    chosenCandidates = candidates_test\n",
        "    trainDataFull = pd.DataFrame(X_train).copy()\n",
        "    ss_f1 = StandardScaler()\n",
        "    trainDataFull = pd.DataFrame(ss_f1.fit_transform(trainDataFull), columns=X_train.columns)\n",
        "    trainDataFull['label'] = y_train\n",
        "\n",
        "    model = pickle.load(open(os.path.join(MODELS_DIR,'rus_scaled', f'best_model_{classifier_name}_{selectedFold}_{pipeFolder}_rus_scaled.pkl'), 'rb'))\n",
        "    y_pred = model.predict(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))\n",
        "    y_pred_proba = model.predict_proba(pd.DataFrame(ss_f1.transform(X_test.copy()), columns=X_test.columns))[:,1]\n",
        "\n",
        "\n",
        "  normals = []\n",
        "  with open(os.path.join(RESULTS_DIR_MAIN,'normals_final.txt')) as f:\n",
        "      for line in f:\n",
        "          normals.append(line[:-1])\n",
        "\n",
        "  if 'glcm' in pipeFolder:\n",
        "    imageKeyName = 2\n",
        "  else:\n",
        "    imageKeyName = 3\n",
        "\n",
        "  candidates_unique = [cand.split('_')[imageKeyName] for cand in chosenCandidates['name'].values.tolist()]\n",
        "\n",
        "  candidates_unique = list(set(candidates_unique))\n",
        "\n",
        "  groundTruthShort = [i for i in groundTruths if i.split('_')[0] in candidates_unique]\n",
        "\n",
        "  print(len(groundTruthShort))\n",
        "\n",
        "  normals = [i for i in normals if i in candidates_unique]\n",
        "\n",
        "  print(\"Calculating fROC file...\") \n",
        "  fn, dfROC, filename_key = calculateFROC(groundTruthShort, normals, chosenCandidates, y_pred_proba, classifier_name, pipeFolder+'_loaded_')\n",
        "  print(len(dfROC.prob.unique()))\n",
        "  draw_curve(fn, len(normals), dfROC, filename_key[:-4], classifier_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_liWLRjttKkK"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jw-NQaT7j0d"
      },
      "outputs": [],
      "source": [
        "# https://medium.com/@robertbracco1/configuring-google-colab-like-a-pro-d61c253f7573#a642\n",
        "%%javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpPoui7q7XkO"
      },
      "source": [
        "**Cross validation classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8GrJGAx_hEk3"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "normals = []\n",
        "with open(os.path.join(RESULTS_DIR_MAIN,'normals_final.txt')) as f:\n",
        "    for line in f:\n",
        "        normals.append(line[:-1])\n",
        "\n",
        "if 'glcm' in pipeFolder:\n",
        "  imageKeyName = 2\n",
        "else:\n",
        "  imageKeyName = 3\n",
        "\n",
        "\n",
        "\n",
        "classifiers = ['gb', 'rf', 'adaboost', 'logreg', 'knn', 'svm']\n",
        "best_hyperparameters = dict()\n",
        "y_pred_proba = []\n",
        "f1_scores = []\n",
        "\n",
        "experiments_results_path = os.path.join(MODELS_DIR, 'rus_scaled')\n",
        "results_filename = f'results_rus_scaled_{pipeFolder}.csv'\n",
        "\n",
        "if results_filename not in os.listdir(experiments_results_path):\n",
        "  results_df = pd.DataFrame(columns=[\"model\", \"pAUC_mean\", \"f1_mean\", \"mcc_mean\", \"pAUC\", \"f1\", \"mcc\", \"selectedFold\", \"parameters\"])\n",
        "  results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "else:\n",
        "  results_df = pd.read_csv(os.path.join(experiments_results_path, results_filename))\n",
        "\n",
        "if len(results_df)>0:\n",
        "  loadedFold = results_df.iloc[len(results_df)-1]['selectedFold']\n",
        "else:\n",
        "  loadedFold = -1\n",
        "\n",
        "#Reading data\n",
        "dfs_train, dfs_test = train_test_function()\n",
        "print('Length of train set: ', len(dfs_train))\n",
        "print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "dfs_tr_hyperopt = dfs_train.sample(frac=0.1, random_state=42)\n",
        "dfs_ts_hyperopt = dfs_test.sample(frac=0.1, random_state=42)\n",
        "\n",
        "dfs_train = dfs_train.append(dfs_tr_hyperopt).drop_duplicates(keep=False)\n",
        "dfs_test = dfs_test.append(dfs_ts_hyperopt).drop_duplicates(keep=False)\n",
        "print('Length of train set: ', len(dfs_train))\n",
        "print('Length of test set: ', len(dfs_test))\n",
        "\n",
        "\n",
        "dfs_hyperOpt = shuffle(dfs_tr_hyperopt.append(dfs_ts_hyperopt))\n",
        "\n",
        "\n",
        "#Preprocessing steps\n",
        "#Manage unbalanced data\n",
        "# dataBalanced, dfNegative = rus(dfs) #Random majority undersampling\n",
        "\n",
        "try:\n",
        "  if dfs_train.isnull().values.any():\n",
        "    colm = dfs_train.columns[dfs_train.isna().any()]\n",
        "    dfs_train = dfs_train[dfs_train[colm[0]].notna()]\n",
        "\n",
        "  if dfs_test.isnull().values.any():\n",
        "    colm = dfs_test.columns[dfs_test.isna().any()]    \n",
        "    dfs_test = dfs_test[dfs_test[colm[0]].notna()]\n",
        "\n",
        "  if dfs_hyperOpt.isnull().values.any():\n",
        "    colm = dfs_hyperOpt.columns[dfs_hyperOpt.isna().any()]\n",
        "    dfs_hyperOpt = dfs_hyperOpt[dfs_hyperOpt[colm[0]].notna()]\n",
        "\n",
        "\n",
        "  print(\"Train dfs without nans: \", len(dfs_train))\n",
        "  print(\"Test dfs without nans: \",len(dfs_test))\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"No nan values to drop, or not columns\")\n",
        "  print(e)\n",
        "\n",
        "y_train = dfs_train['label'].reset_index(drop=True)\n",
        "candidates_train = dfs_train[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "X_train = dfs_train.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "y_test = dfs_test['label'].reset_index(drop=True)\n",
        "candidates_test = dfs_test[['name', 'label', 'x', 'y']].reset_index(drop=True)\n",
        "X_test = dfs_test.drop(['name', 'label',  'x', 'y'], axis=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "#Plot features \n",
        "#  plotFeatures(data)\n",
        "\n",
        "#Obtain train-test datasets\n",
        "\n",
        "\n",
        "trainDataFull = pd.DataFrame(X_train).copy()\n",
        "trainDataFull['label'] = y_train\n",
        "\n",
        "dataBalanced, dfNegative = rus(trainDataFull)\n",
        "print(\"Data Balanced: \", len(dataBalanced))\n",
        "print(\"Negative pool: \",len(dfNegative))\n",
        "\n",
        "trainBalanceData = dataBalanced.drop('label', axis=1)\n",
        "trainBalanceLabel = dataBalanced['label']\n",
        "\n",
        "hyperOptTrainData = dfs_hyperOpt.drop(['name', 'label',  'x', 'y'], axis=1)\n",
        "ss = StandardScaler()\n",
        "hyperOptTrainData = pd.DataFrame(ss.fit_transform(hyperOptTrainData), columns=X_train.columns)\n",
        "hyperOptLabel = dfs_hyperOpt['label']\n",
        "\n",
        "\n",
        "for classifier in classifiers:\n",
        "\n",
        "    if classifier == 'svm':\n",
        "      clf, best_params = SVC_linear(hyperOptTrainData, hyperOptLabel)\n",
        "    elif classifier == 'rf':\n",
        "      clf, best_params = RandomForest(hyperOptTrainData, hyperOptLabel, best_params = {'criterion': 'gini', 'max_depth': 12, 'max_features': 'auto', 'n_estimators': 100})\n",
        "    elif classifier == 'gb':\n",
        "      clf, best_params = GradientBoosting(hyperOptTrainData, hyperOptLabel, best_params = {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 600})\n",
        "    elif classifier == 'adaboost':\n",
        "      clf, best_params = AdaBoost(hyperOptTrainData, hyperOptLabel, best_params = {'learning_rate': 0.5, 'n_estimators': 600})\n",
        "    elif classifier == 'logreg':\n",
        "      clf, best_params = LogRegre(hyperOptTrainData, hyperOptLabel)\n",
        "    elif classifier == 'knn':\n",
        "      clf, best_params = KNN(hyperOptTrainData, hyperOptLabel, best_params = {'n_neighbors': 5})\n",
        "    elif classifier == 'dt':\n",
        "      clf, best_params = DecisionTree(hyperOptTrainData, hyperOptLabel)\n",
        "\n",
        "\n",
        "    selectedFold, mean_metrics, best_metrics, dfROC, fn = cv_classification(clf, best_params, X_train, X_test, y_train, y_test, \n",
        "                                                                            candidates_train, candidates_test, normals, classifier, imageKeyName)\n",
        "  # Columns of results file are [\"model\", \"pAUC_mean\", \"f1_mean\", \"mcc_mean\", \"pAUC\", \"f1\", \"mcc\", \"selectedFold\", \"parameters\"]\n",
        "    results_to_save = [classifier]\n",
        "    results_to_save.extend(mean_metrics)\n",
        "    results_to_save.extend(best_metrics)\n",
        "    results_to_save.append(selectedFold)\n",
        "    results_to_save.append([best_params])\n",
        "    \n",
        "  #save Results file in already created file\n",
        "    results_df = results_df.append(pd.DataFrame([results_to_save], columns=[\"model\", \"pAUC_mean\", \"f1_mean\", \"mcc_mean\", \"pAUC\", \"f1\", \"mcc\", \"selectedFold\", \"parameters\"]))\n",
        "    results_df.to_csv(os.path.join(experiments_results_path, results_filename), index=False)\n",
        "\n",
        "    best_hyperparameters[classifier] = best_params\n",
        "\n",
        "\n",
        "    draw_curve(fn, len(normals), dfROC, f'FROC_curve_{classifier}_pip{pipeFolder}_fn{fn}_normals{len(normals)}_rus_scaled', classifier)\n",
        "\n",
        "    del dfROC, fn, results_to_save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb6bOKt4hvt3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ML_structure_project_rus_ScaledData_Final.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}